Title ,Text
0_B1,"['A Benchmark for Interpretability Methods in Deep  Neural Networks  Sara Hooker, Dumitru Erhan, Pieter-Jan Kindermans, Been Kim  shooker,dumitru,pikinder,beenkim@google.com  Google Brain  9 1 0 2     v o N 5         ]  G L .',
 's c [      3 v 8 5 7 0 1  .',
 '6 0 8 1 : v i X r a  Abstract  We propose an empirical measure of the approximate accuracy of feature impor- tance estimates in deep neural networks.',
 'Our results across several large-scale image classi?cation datasets show that many popular interpretability methods pro- duce estimates of feature importance that are not better than a random designation of feature importance.',
 'Only certain ensemble based approaches—VarGrad and SmoothGrad-Squared—outperform such a random assignment of importance.',
 'The manner of ensembling remains critical, we show that some approaches do no better then the underlying method but carry a far higher computational burden.',
 '1  Introduction  In a machine learning setting, a question of great interest is estimating the in?uence of a given input feature to the prediction made by a model.',
 'Understanding what features are important helps improve our models, builds trust in the model prediction and isolates undesirable behavior.',
 'Unfortunately, it is challenging to evaluate whether an explanation of model behavior is reliable.',
 'First, there is no ground truth.',
 'If we knew what was important to the model, we would not need to estimate feature importance in the ?rst place.',
 'Second, it is unclear which of the numerous proposed interpretability methods that estimate feature importance one should select [7, 6, 44, 31, 38, 34, 40, 37, 20, 23, 12, 10, 41, 32, 42, 28, 35, 3].',
 'Many feature importance estimators have interesting theoretical properties e.g.',
 'preservation of relevance [6] or implementation invariance [38].',
 'However even these methods need to be con?gured correctly [23, 38] and it has been shown that using the wrong con?guration can easily render them ineffective [19].',
 'For this reason, it is important that we build a framework to empirically validate the relative merits and reliability of these methods.',
 'A commonly used strategy is to remove the supposedly informative features from the input and look at how the classi?er degrades [30].',
 'This method is cheap to evaluate but comes at a signi?cant drawback.',
 'Samples where a subset of the features are removed come from a different distribution (as can be seen in Fig.',
 '1).',
 'Therefore, this approach clearly violates one of the key assumptions in machine learning: the training and evaluation data come from the same distribution.',
 'Without re-training,it is unclear whether the degradation in model performance comes from the distribution shift or because the features that were removed are truly informative [10, 12].',
 'For this reason we decided to verify how much information can be removed in a typical dataset before accuracy of a retrained model breaks down completely.',
 'In this experiment, we applied ResNet-50 [17], one of the most commonly used models, to ImageNet.',
 'It turns out that removing information is quite hard.',
 'With 90% of the inputs removed the network still achieves 63.53% accuracy compared to 76.68% on clean data.',
 'This implies that a strong performance degradation without re-training might be caused by a shift in distribution instead of removal of information.',
 'Instead, in this work we evaluate interpretability methods by verifying how the accuracy of a retrained model degrades as features estimated to be important are removed.',
 'We term this approach ROAR, RemOve And Retrain.',
 'For each feature importance estimator, ROAR replaces the fraction of the  33rd Conference on Neural Information Processing Systems (NeurIPS 2019), Vancouver, Canada.',
 'pixels estimated to be most important with a ?xed uninformative value.',
 'This modi?cation (shown in Fig.',
 '1) is repeated for each image in both the training and test set.',
 'To measure the change to model behavior after the removal of these input features, we separately train new models on the modi?ed dataset such that train and test data comes from a similar distribution.',
 'More accurate estimators will identify as important input pixels whose subsequent removal causes the sharpest degradation in accuracy.',
 'We also compare each method performance to a random assignment of importance and a sobel edge ?lter [36].',
 'Both of these control variants produce rankings that are independent of the properties of the model we aim to interpret.',
 'Given that these methods do not depend upon the model, the performance of these variants respresent a lower bound of accuracy that a interpretability method could be expected to achieve.',
 'In particular, a random baseline allows us to answer the question: is the interpretability method more accurate than a random guess as to which features are important?',
 'In Section 3 we will elaborate on the motivation and the limitations of ROAR.',
 'We applied ROAR in a broad set of experiments across three large scale, open source image datasets: ImageNet [11], Food 101 [9] and Birdsnap [8].',
 'In our experiments we show the following.',
 '• Training performance is quite robust to removing input features.',
 'For example, after randomly replacing 90% of all ImageNet input features, we can still train a model that achieves 63.53 ± 0.13 (average across 5 independent runs).',
 'This implies that a small subset of features are suf?cient for the actual decision making.',
 'Our observation is consistent across datasets.',
 '• The base methods we evaluate are no better or on par with a random estimate at ?nding the core set of informative features.',
 'However, we show that SmoothGrad-Squared (an unpublished variant of Classic SmoothGrad [35]) and Vargrad [3], methods which ensemble a set of estimates produced by basic methods, far outperform both the underlying method and a random guess.',
 'These results are consistent across datasets and methods.',
 '• Not all ensemble estimators improve performance.',
 'Classic SmoothGrad [35] is worse than  a single estimate despite being more computationally intensive.',
 '2 Related Work  Interpretability research is diverse, and many different approaches are used to gain intuition about the function implemented by a neural network.',
 'For example, one can distill or constrain a model into a functional form that is considered more interpretable [5, 13, 39, 29].',
 'Other methods explore the role of neurons or activations in hidden layers of the network [25, 27, 24, 43], while others use high level concepts to explain prediction results [18].',
 'Finally there are also the input feature importance estimators that we evaluate in this work.',
 'These methods estimate the importance of an input feature for a speci?ed output activation.',
 'While there is no clear way to measure “correctness”, comparing the relative merit of different estimators is often based upon human studies [31, 28, 22] which interrogate whether the ranking is meaningful to a human.',
 'Recently, there have been efforts to evaluate whether interpretability methods are both reliable and meaningful to human.',
 'For example, in [19] a unit test for interpretability methods is proposed which detects whether the explanation can be manipulated by factors that are not affecting the decision making process.',
 'Another approach considers a set of sanity checks that measure the change to an estimate as parameters in a model or dataset labels are randomized [3].',
 'Closely related to this manuscript are the modi?cation-based evaluation measures proposed originally by [30] with subsequent variations [20, 26].',
 'In this line of work, one replaces the inputs estimated to be most important with a value considered meaningless to the task.',
 'These methods measure the subsequent degradation to the trained model at inference time.',
 'Recursive feature elimination methods [16] are a greedy search where the algorithm is trained on an iteratively altered subset of features.',
 'Recursive feature elimination does not scale to high dimensional datasets (one would have to retrain removing one pixel at a time) and unlike our work is a method to estimate feature importance (rather than evaluate different existing interpretability methods).',
 'To the best of our knowledge, unlike prior modi?cation based evaluation measures, our benchmark requires retraining the model from random initialization on the modi?ed dataset rather than re-scoring the modi?ed image at inference time.',
 'Without this step, we argue that one cannot decouple whether the model’s degradation in performance is due to artifacts introduced by the value used to replace  2  \x0cFigure 1: A single ImageNet image modi?ed according to the ROAR framework.',
 'The fraction of pixels estimated to be most important by each interpretability method is replaced with the mean.',
 'Above each image, we include the average test-set accuracy for 5 ResNet-50 models independently trained on the modi?ed dataset.',
 'From left to right: base estimators (gradient heatmap (GRAD), Integrated Gradients (IG), Guided Backprop (GB)), derivative approaches that ensemble a set of estimates (SmoothGrad Integrated Gradients (SG-SQ-IG), SmoothGrad-Squared Integrated Gradients (SG-SQ-IG), VarGrad Integrated Gradients (Var-IG)) and control variants (random modi?cation (Random) and a sobel edge ?lter (Sobel)).',
 'This image is best visualized in digital format.',
 'the pixels that are removed or due to the approximate accuracy of the estimator.',
 'Our work considers several large scale datasets, whereas all previous evaluations have involved a far smaller subset of data [4, 30].',
 '3 ROAR: Remove And Retrain  To evaluate a feature importance estimate using ROAR, we sort the input dimensions according to the estimated importance.',
 'We compute an estimate e of feature importance for every input in the training and test set.',
 'We rank each e into an ordered set {eo i}N i=1.',
 'For the top t fraction of this ordered set, we replace the corresponding pixels in the raw image with the per channel mean.',
 'We generate new train and test datasets at different degradation levels t = [0., 10, .',
 '.',
 '.',
 ', 100] (where t is a percentage of all features modi?ed).',
 'Afterwards the model is re-trained from random initialization on the new dataset and evaluated on the new test data.',
 'Of course, because re-training can result in slightly different models, it is essential to repeat the training process multiple times to ensure that the variance in accuracy is low.',
 'To control for this, we repeat training 5 times for each interpretabiity method e and level of degradation t. We introduce the methodology and motivation for ROAR in the context of linear models and deep neural networks.',
 'However, we note that the properties of ROAR differ given an algorithm that explicitly uses feature selection (e.g.',
 'L1 regularization or any mechanism which limits the features available to the model at inference time).',
 'In this case one should of course mask the inputs that are known to be ignored by the model, before re-training.',
 'This will prevent them from being used after re-training, which could otherwise corrupt the ROAR metric.',
 'For the remainder of this paper, we focus on the performance of ROAR given deep neural networks and linear models which do not present this limitation  3  \x0cWhat would happen without re-training?',
 'The re-training is the most computationally expensive aspect of ROAR.',
 'One should question whether it is actually needed.',
 'We argue that re-training is needed because machine learning models typically assume that the train and the test data comes from a similar distribution.',
 'The replacement value c can only be considered uninformative if the model is trained to learn it as such.',
 'Without retraining, it is unclear whether degradation in performance is due to the introduction of artifacts outside of the original training distribution or because we actually removed information.This is made explicit in our experiment in Section 4.3.1, we show that without retraining the degradation is far higher than the modest decrease in performance observed with re-training.',
 'This suggests retraining has better controlled for artefacts introduced by the modi?cation.',
 'Are we evaluating the right aspects?',
 'Re-training does have limitations.',
 'For one, while the architecture is the same, the model used during evaluation is not the same as the model on which the feature importance estimates were originally obtained.',
 'To understand why ROAR is still meaningful we have to think about what happens when the accuracy degrades, especially when we compare it to a random baseline.',
 'The possibilities are:  1.',
 'We remove input dimensions and the accuracy drops.',
 'In this case, it is very likely that the removed inputs were informative to the original model.',
 'ROAR thus gives a good indication that the importance estimate is of high quality.',
 '2.',
 'We remove inputs and the accuracy does not drop.',
 'This can be explained as either:  (a) It could be caused by removal of an input that was uninformative to the model.',
 'This includes the case where the input might have been informative but not in a way that is useful to the model, for example, when a linear model is used and the relation between the feature and the output is non-linear.',
 'Since in such a case the information was not used by the model and it does not show in ROAR we can assume ROAR behaves as intended.',
 '(b) There might be redundancy in the inputs.',
 'The same information could represented in another feature.',
 'This behavior can be detected with ROAR as we will show in our toy data experiment.',
 'Validating the behavior of ROAR on arti?cial data.',
 'To demonstrate the difference between ROAR and an approach without re-training in a controlled environment we generate a 16 dimensional dataset with 4 informative features.',
 'Each datapoint x and its label y was generated as follows:  x =  az 10  + d? +  \x01 10  ,  y = (z > 0).',
 'All random variables were sampled from a standard normal distribution.',
 'The vectors a and d are 16 dimensional vectors that were sampled once to generate the dataset.',
 'In a only the ?rst 4 values have nonzero values to ensure that there are exactly 4 informative features.',
 'The values ?, \x01 were sampled independently for each example.',
 'We use a least squares model as this problem can be solved linearly.',
 'We compare three rankings: the ground truth importance ranking, random ranking and the inverted ground truth ranking (the worst possible estimate of importance).',
 'In the left plot of Fig.',
 '2 we can observe that without re-training the worst case estimator is shown to degrade performance relatively quickly.',
 'In contrast, ROAR shows no degradation until informative features begin to be removed at 75%.',
 'This correctly shows that this estimator has ranked feature importance poorly (ranked uninformative features as most important).',
 'Finally, we consider ROAR performance given a set of variables that are completely redundant.',
 'We note that ROAR might not decrease until all of them are removed.',
 'To account for this we measure ROAR at different levels of degradation, with the expectation that across this interval we would be able to detect in?ection points in performance that would indicate a set of redundant features.',
 'If this happens, we believe that it could be detected easily by the sharp decrease as shown in Fig.',
 '2.',
 'Now that we have validated ROAR in a controlled setup, we can move on to our large scale experiments.',
 '4  \x0cFigure 2: A comparison between not retraining and ROAR on arti?cial data.',
 'In the case where the model is not retrained, test-set accuracy quickly erodes despite the worst case ranking of redundant features as most important.',
 'This incorrectly evaluates a completely incorrect feature ranking as being informative.',
 'ROAR is far better at identifying this worst case estimator, showing no degradation until the features which are informative are removed at 75%.',
 'This plot also shows the limitation of ROAR, an accuracy decrease might not happen until a complete set of fully redundant features is removed.',
 'To account for this we measure ROAR at different levels of degradation, with the expectation that across this interval we would be able to control for performance given a set of redundant features.',
 '4 Large scale experiments  4.1 Estimators under consideration  Our evaluation is constrained to a subset of estimators of feature importance.',
 'We selected these based on the availability of open source code, consistent guidelines on how to apply them and the ease of implementation given a ResNet-50 architecture [17].',
 'Due to the breadth of the experimental setup it was not possible to include additional methods.',
 'However, we welcome the opportunity to consider additional estimators in the future, and in order to make it easy to apply ROAR to additional estimators we have open sourced our code https://bit.ly/2ttLLZB.',
 'Below, we brie?y introduce each of the methods we evaluate.',
 'Base estimators are estimators that compute a single estimate of importance (as opposed to ensemble methods).',
 'While we note that guided backprop and integrated gradients are examples of signal and attribution methods respectively, the performance of these estimators should not be considered representative of other methods, which should be evaluated separately.',
 '• Gradients or Sensitivity heatmaps [34, 7] (GRAD) are the gradient of the output activa-  tion of interest Al  n with respect to xi:  e =  ?Al n ?xi  • Guided Backprop [37] (GB) is an example of a signal method that aim to visualize the input patterns that cause the neuron activation Al n in higher layers [37, 40, 20].',
 'GB computes this by using a modi?ed backpropagation step that stops the ?ow of gradients when less than zero at a ReLu gate.',
 '• Integrated Gradients [38] (IG)is an example of an attribution method which assign impor- tance to input features by decomposing the output activation Al n into contributions from the individual input features [6, 38, 23, 33, 20].',
 'Integrated gradients interpolate a set of estimates for values between a non-informative reference point x0 to the actual input x.',
 'This integral can be approximated by summing a set of k points at small intervals between x0 and x:  k (x ? x0)) ?fw(x0 + i ?xi  × 1 k  e = (xi ? x0  i ) × k(cid:88)  i=1  The ?nal estimate e will depend upon both the choice of k and the reference point x0.',
 'As suggested by [38], we use a black image as the reference point and set k to be 25.',
 '5  \x0cEnsembling methods In addition to the base approaches we also evaluate three ensembling methods for feature importance.',
 'For all the ensemble approaches that we describe below (SG, SG-SQ, Var), we average over a set of 15 estimates as suggested by in the original SmoothGrad publication [35].',
 '• Classic SmoothGrad (SG) [35] SG averages a set J noisy estimates of feature importance  (constructed by injecting a single input with Gaussian noise ? independently J times):  e =  (gi(x + ?, Al  n))  J(cid:88)  i=1  J(cid:88)  • SmoothGrad2(SG-SQ) is an unpublished variant of classic SmoothGrad SG which squares  each estimate e before averaging the estimates:  e =  (gi(x + ?, Al  n)2)  i=1  Although SG-SQ is not described in the original publication, it is the default open-source implementation of the open source code for SG: https://bit.ly/2Hpx5ob.',
 '• VarGrad (Var) [3] employs the same methodology as classic SmoothGrad (SG) to construct a set of t J noisy estimates.',
 'However, VarGrad aggregates the estimates by computing the variance of the noisy set rather than the mean.',
 'e = Var(gi(x + ?, Al  n))  Control Variants As a control, we compare each estimator to two rankings (a random assignment of importance and a sobel edge ?lter) that do not depend at all on the model parameters.',
 'These controls represent a lower bound in performance that we would expect all interpretability methods to outperform.',
 '• Random A random estimator gR assigns a random binary importance probability e (cid:55)? 0, 1.',
 'This amounts to a binary vector e ? Bernoulli(1 ? t) where (1 ? t) is the probability of ei = 1.',
 'The formulation of gR does not depend on either the model parameters or the input image (beyond the number of pixels in the image).',
 '• Sobel Edge Filter convolves a hard-coded, separable, integer ?lter over an image to produce a mask of derivatives that emphasizes the edges in an image.',
 'A sobel mask treated as a ranking e will assign a high score to areas of the image with a high gradient (likely edges).',
 '4.2 Experimental setup  We use a ResNet-50 model for both generating the feature importance estimates and subsequent training on the modi?ed inputs.',
 'ResNet-50 was chosen because of the public code implementations (in both PyTorch [15] and Tensor?ow [1]) and because it can be trained to give near to state of art performance in a reasonable amount of time [14].',
 'For all train and validation images in the dataset we ?rst apply test time pre-processing as used by Goyal et al.',
 '[14].',
 'We evaluate ROAR on three open source image datasets: ImageNet, Birdsnap and Food 101.',
 'For each dataset and estimator, we generate new train and test sets that each correspond to a different fraction of feature modi?cation t = [0, 10, 30, 50, 70, 90].',
 'We evaluate 18 estimators in total (this includes the base estimators, a set of ensemble approaches wrapped around each base and ?nally a set of squared estimates).',
 'In total, we generate 540 large-scale modi?ed image datasets in order to consider all experiment variants (180 new test/train for each original dataset).',
 'We independently train 5 ResNet-50 models from random initialization on each of these modi?ed dataset and report test accuracy as the average of these 5 runs.',
 'In the base implementation, the ResNet-50 trained on an unmodi?ed ImageNet dataset achieves a mean accuracy of 76.68%.',
 'This is comparable to the performance reported by [14].',
 'On Birdsnap and Food 101, our unmodi?ed datasets achieve 66.65% and 84.54% respectively (average of 10 independent runs).',
 'This baseline performance is comparable to that reported by Kornblith et al.',
 '[21].',
 '6  \x0c4.3 Experimental results  4.3.1 Evaluating the random ranking  Comparing estimators to the random ranking allows us to answer the question: is the estimate of importance more accurate than a random guess?',
 'It is ?rstly worthwhile noting that model performance is remarkably robust to random modi?cation.',
 'After replacing a large portion of all inputs with a constant value, the model not only trains but still retains most of the original predictive power.',
 'For example, on ImageNet, when only 10% of all features are retained, the trained model still attains 63.53% accuracy (relative to unmodi?ed baseline of 76.68%).',
 'The ability of the model to extract a meaningful representation from a small random fraction of inputs suggests a case where many inputs are likely redundant.',
 'The nature of our input–an image where correlations between pixels are expected – provides one possible readons for redundancy.',
 'The results for our random baseline provides additional support for the need to re-train.',
 'We can compare random ranking on ROAR vs. a traditional deletion metric [30], i.e.',
 'the setting where we do not retain.',
 'These results are given in Fig.',
 '3.',
 'Without retraining, a random modi?cation of 90% degrades accuracy to 0.5% for the model that was not retrained.',
 'Keep in mind that on clean data we achieve 76.68% accuracy.',
 'This large discrepancy illustrates that without retraining the model, it is not possible to decouple the performance of the ranking from the degradation caused by the modi?cation itself.',
 '4.3.2 Evaluating Base Estimators  Now that we have established the baselines, we can start evaluating the base estimators: GB, IG, GRAD.',
 'Surprisingly, the left inset of Fig.',
 '4 shows that these estimators consistently perform worse than the random assignment of feature importance across all datasets and for all thresholds t = [0.1, 0.3, 0.5, 0.7, 0.9].',
 'Furthermore, our estimators fall further behind the accuracy of random guess as a larger fraction t of inputs is modi?ed.',
 'The gap is widest when t = 0.9.',
 'Our base estimators also do not compare favorably to the performance of a sobel edge ?lter SOBEL.',
 'Both the sobel ?lter and the random ranking have formulations that are entirely independent of the model parameters.',
 'All the base estimators that we consider have formulations that depend upon the trained model weights, and thus we would expect them to have a clear advantage in outperforming the control variants.',
 'However, across all datasets and thresholds t, the base estimators GB, IG, GRAD perform on par or worse than SOBEL.',
 'Base estimators perform within a very narrow range.',
 'Despite the very different formulations of base estimators that we consider, the difference between the performance of the base estimators is in a strikingly narrow range.',
 'For example, as can be seen in the left column of Fig.',
 '4, for Birdsnap, the difference in accuracy between the best and worst base estimator at t = 90% is only 4.22%.',
 'This range remains narrow for both Food101 and ImageNet, with a gap of 5.17% and 3.62 respectively.',
 'Our base estimator results are remarkably consistent results across datasets, methods and for all fractions of t considered.',
 'The variance is very low across independent runs for all datasets and estimators.',
 'The maximum variance observed for ImageNet was a variance of 1.32% using SG-SQ- GRAD at 70% of inputs removed.',
 'On Birdsnap the highest variance was 0.12% using VAR-GRAD at 90% removed.',
 'For food101 it was 1.52% using SG-SQ-GRAD at 70% removed.',
 'Finally, we compare performance of the base estimators using ROAR re-training vs. a traditional deletion metric [30], again the setting where we do not retain.',
 'In Fig.',
 '3 we see a behavior for the base estimators on all datasets that is similar to the behavior of the inverse (worst possible) ranking on the toy data in Fig.',
 '2.',
 'The base estimators appear to be working when we do not retrain, but they are clearly not better than the random baseline when evaluated using ROAR.',
 'This provides additional support for the need to re-train.',
 '4.3.3 Evaluating Ensemble Approaches  Since the base estimators do not appear to perform well, we move on to ensemble estimators.',
 'Ensemble approaches inevitably carry a higher computational approach, as the methodology requires the aggregation of a set of individual estimates.',
 'However, these methods are often preferred by humans because they appear to produce “less noisy” explanations.',
 'However, there is limited theoretical understanding of what these methods are actually doing or how this is related to the accuracy of the  7  \x0cFigure 3: On the left we evaluate three base estimators and the random baseline without retraining.',
 'All of the methods appear to reduce accuracy at quite a high rate.',
 'On the right, we see, using ROAR, that after re-training most of the information is actually still present.',
 'It is also striking that in this case the base estimators perform worse than the random baseline.',
 'Figure 4: Left: Grad (GRAD), Integrated Gradients (IG) and Guided Backprop (GB) perform worse than a random assignment of feature importance.',
 'Middle: SmoothGrad (SG) is less accurate than a random assignment of importance and often worse than a single estimate (in the case of raw gradients SG-Grad and Integrated Gradients SG-IG).',
 'Right: SmoothGrad Squared (SG-SQ) and VarGrad (VAR) produce a dramatic improvement in approximate accuracy and far outperform the other methods in all datasets considered, regardless of the underlying estimator.',
 'explanation.',
 'We evaluate ensemble estimators and produce results that are remarkably consistent results across datasets, methods and for all fractions of t considered.',
 'Classic Smoothgrad is less accurate or on par with a single estimate.',
 'In the middle column in Fig.',
 '4 we evaluate Classic SmoothGrad (SG).',
 'It average 15 estimates computed according to an underlying base method (GRAD, IG or GB).',
 'However, despite averaging SG degrades test-set accuracy still less than a random guess.',
 'In addition, for GRAD and IG SmoothGrad performs worse than a single estimate.',
 '8  \x0cSmoothGrad-Squared and VarGrad produce large gains in accuracy.',
 'In the right inset of Fig.',
 '4, we show that both VarGrad (VAR) and SmoothGrad-Squared (SG-SQ) far outperform the two control variants.',
 'In addition, for all the interpretability methods we consider, VAR or SG-SQ far outperform the approximate accuracy of a single estimate.',
 'However, while VAR and SG-SQ bene?t the accuracy of all base estimators, the overall ranking of estimator performance differs by dataset.',
 'For ImageNet and Food101, the best performing estimators are VAR or SG-SQ when wrapped around GRAD.',
 'However, for the Birdsnap dataset, the most approximately accurate estimates are these ensemble approaches wrapped around GB.',
 'This suggests that while the VAR and SG-SQ consistently improve performance, the choice of the best underlying estimator may vary by task.',
 'Now, why do both of these methods work so well?',
 'First, these methods are highly similar.',
 'If the average (squared) gradient over the noisy samples is zero then VAR and SG-SQ reduce to the same method.',
 'For many images it appears that the mean gradient is much smaller than the mean squared gradient.',
 'This implies that the ?nal output should be similar.',
 'Qualitatively this seems to be the case as well.',
 'In Fig.',
 '1 we observe that both methods appear to remove whole objects.',
 'The other methods removed inputs that are less concentrated but spread more widely over the image.',
 'It is important to note that these methods were not forced to behave as such.',
 'It is emergent behavior.',
 'Understanding why this happens and why this is bene?cial should be the focus of future work.',
 'Squaring estimates The ?nal question we consider is why SmoothGrad-Squared SG-SQ dramatically improves upon the performance of SmoothGrad SG despite little difference in formulation.',
 'The only difference between the two estimates is that SG-SQ squares the estimates before averaging.',
 'We consider the effect of only squaring estimates (no ensembling).',
 'We ?nd that while squaring improves the accuracy of all estimators, the transformation does not adequately explain the large gains that we observe when applying VAR or SG-SQ.',
 'When base estimators are squared they slightly outperform the random baseline (all results included in the supplementary materials).',
 '5 Conclusion and Future Work  In this work, we propose ROAR to evaluate the quality of input feature importance estimators.',
 'Surprisingly, we ?nd that the commonly used base estimators, Gradients, Integrated Gradients and Guided BackProp are worse or on par with a random assignment of importance.',
 'Furthermore, certain ensemble approaches such as SmoothGrad are far more computationally intensive but do not improve upon a single estimate (and in some cases are worse).',
 'However, we do ?nd that VarGrad and SmoothGrad-Squared strongly improve the quality of these methods and far outperform a random guess.',
 'While the low effectiveness of many methods could be seen as a negative result, we view the remarkable effectiveness of SmoothGrad-Squared and VarGrad as important progress within the community.',
 'Our ?ndings are particularly pertinent for sensitive domains where the accuracy of a explanation of model behavior is paramount.',
 'While we venture some initial consideration of why certain ensemble methods far outperform other estimator, the divergence in performance between the ensemble estimators is an important direction of future research.',
 'Acknowledgments  We thank Gabriel Bender, Kevin Swersky, Andrew Ross, Douglas Eck, Jonas Kemp, Melissa Fabros, Julius Adebayo, Simon Kornblith, Prajit Ramachandran, Niru Maheswaranathan, Gamaleldin Elsayed, Hugo Larochelle, Varun Vasudevan for their thoughtful feedback on earlier iterations of this work.',
 'In addition, thanks to Sally Jesmonth"
1_B1,"['DRAGNN: A Transition-based Framework for Dynamically Connected  Neural Networks  Lingpeng Kong† Chris Alberti(cid:63) Daniel Andor(cid:63) lingpengk@cs.cmu.edu, {chrisalberti,danielandor,bogatyy,djweiss}@google.com  Ivan Bogatyy(cid:63) David Weiss(cid:63)  †Carnegie Mellon University, Pittsburgh, PA.  (cid:63)Google, New York, NY.',
 '7 1 0 2    r a     M 3 1      ] L C .',
 's c [      1 v 4 7 4 4 0  .',
 '3 0 7 1 : v i X r a  Abstract  In this work, we present a compact, mod- ular framework for constructing novel re- current neural architectures.',
 'Our basic module is a new generic unit, the Transi- tion Based Recurrent Unit (TBRU).',
 'In ad- dition to hidden layer activations, TBRUs have discrete state dynamics that allow network connections to be built dynam- ically as a function of intermediate acti- vations.',
 'By connecting multiple TBRUs, we can extend and combine commonly used architectures such as sequence-to- sequence, attention mechanisms, and re- cursive tree-structured models.',
 'A TBRU can also serve as both an encoder for downstream tasks and as a decoder for its own task simultaneously, resulting in more accurate multi-task learning.',
 'We call our approach Dynamic Recurrent Acyclic Graphical Neural Networks, or DRAGNN.',
 'We show that DRAGNN is signi?cantly more accurate and ef?cient than seq2seq with attention for syntactic dependency parsing and yields more accurate multi- task learning for extractive summarization tasks.',
 '1  Introduction  To apply deep learning models to structured pre- diction, machine learning practitioners must ad- dress two primary issues: (1) how to represent the input, and (2) how to represent the output.',
 'The seq2seq encoder/decoder framework (Kalch- brenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014) proposes solving these generically.',
 'In its simplest form, the encoder net- work produces a ?xed-length vector representa- tion of an input, while the decoder network pro-  duces a linearization of the target output structure as a sequence of output symbols.',
 'Encoder/decoder is state of the art for several key tasks in natural language processing, such as machine translation (Wu et al., 2016).',
 'However, ?xed-size encodings become less competitive when the input structure can be ex- plicitly mapped to the output.',
 'In the simple case of predicting tags for individual tokens in a sen- tence, state-of-the-art taggers learn vector repre- sentations for each input token and predict output tags from those (Ling et al., 2015; Huang et al., 2015; Andor et al., 2016).',
 'When the input or out- put is a syntactic parse tree, networks that explic- itly operate over the compositional structure of the network typically outperform generic representa- tions (Dyer et al., 2015; Li et al., 2015; Bowman et al., 2016).',
 'Implictly learned mappings via at- tention mechanisms can signi?cantly improve the performance of sequence-to-sequence (Bahdanau et al., 2015; Vinyals et al., 2015), but require run- time that’s quadratic in the input size.',
 'In this work, we propose a modular neural architecture that generalizes the encoder/decoder concept to include explicit structure.',
 'Our frame- work can represent sequence-to-sequence learn- ing as well as models with explicit structure like bi-directional tagging models and compositional, tree-structured models.',
 'Our core idea is to de?ne any given architecture as a series of modular units, where connections between modules are unfolded dynamically as a function of the intermediate acti- vations produced by the network.',
 'These dynamic connections represent the explicit input and output structure produced by the network for a given task.',
 'We build on the idea of transition systems from the parsing literature (Nivre, 2006), which lin- earize structured outputs as a sequence of (state, decision) pairs.',
 'Transition-based neural networks have recently been applied to a wide variety of  \x0cFigure 1: High level schematic of a Transition-Based Recurrent Unit (TBRU), and common network architectures that can be implemented with multiple TBRUs.',
 'The discrete state is used to compute recurrences and ?xed input embeddings, which are then fed through a network cell.',
 'The network predicts an action which is used to update the discrete state (dashed output) and provides activations that can be consumed through recurrences (solid output).',
 'Note that we present a slightly simpli?ed version of Stack- LSTM (Dyer et al., 2015) for clarity.',
 'NLP problems; Dyer et al.',
 '(2015); Lample et al.',
 '(2016); Kiperwasser and Goldberg (2016); Zhang et al.',
 '(2016); Andor et al.',
 '(2016), among others.',
 'We generalize these approaches with a new ba- sic module, the Transition-Based Recurrent Unit (TBRU), which produces a vector representation for every transition state in the output lineariza- tion (Figure 1).',
 'These representations also serve as the encoding of the explicit structure de?ned by the states.',
 'For example, a TBRU that attaches two sub-trees while building a syntactic parse tree will also produce the hidden layer activations to serve as an encoding for the newly constructed phrase.',
 'Multiple TBRUs can be connected and learned jointly to add explicit structure to multi-task learn- ing setups and share representations between tasks with different input or output spaces (Figure 2).',
 'This inference procedure will construct an acyclic compute graph representing the network architecture, where recurrent connections are dy- namically added as the network unfolds.',
 'We there- fore call our approach Dynamic Recurrent Acyclic Graphical Neural Networks, or DRAGNN.',
 'DRAGNN has several distinct modeling advan- tages over traditional ?xed neural architectures.',
 'Unlike generic seq2seq, DRAGNN supports vari- able sized input representations that may contain explicit structure.',
 'Unlike purely sequential RNNs, the dynamic connections in a DRAGNN can span arbitrary distances in the input space.',
 'Crucially, inference remains linear in the size of the input, in contrast to quadratic-time attention mechanisms.',
 'Dynamic connections thus establish a compromise between pure seq2seq and pure attention architec-  tures by providing a ?nite set of long-range in- puts that ‘attend’ to relevant portions of the input space.',
 'Unlike recursive neural networks (Socher et al., 2010, 2011) DRAGNN can both predict in- termediate structures (such as parse trees) and uti- lize those structures in a single deep model, back- propagating downstream task errors through the intermediate structures.',
 'Compared to models such as Stack-LSTM (Dyer et al., 2015) and SPINN (Bowman et al., 2016), TBRUs are a more general formulation that allows incorporating dynamically structured multi-task learning (Zhang and Weiss, 2016) and more varied network architectures.',
 'In sum, DRAGNN is not a particular neural ar- chitecture, but rather a formulation for describing neural architectures compactly.',
 'The key to this compact description is a new recurrent unit—the TBRU—which allows connections between nodes in an unrolled compute graph to be speci?ed dy- namically in a generic fashion.',
 'We utilize tran- sition systems to provide succinct, discrete repre- sentations via linearizations of both the input and the output for structured prediction.',
 'We provide a straightforward way of re-using representations across NLP tasks that operate on different struc- tures.',
 'We demonstrate the effectiveness of DRAGNN on two NLP tasks that bene?t from explicit struc- ture: dependency parsing and extractive sentence summarization (Filippova and Altun, 2013).',
 'First, we show how to use TBRUs to incrementally add structure to the input and output of a “vanilla” seq2seq dependency parsing model, dramatically boosting accuracy over seq2seq with no additional  Encoder/Decoder (2 TBRU)Bi-LSTM Tagging (3 TBRU)Y1Y2Y3Y4Y5Y1Y2Y3Y4Y5Stack-LSTM (2 TBRU)Y1Y2Y3Y4Y5Transition Based Recurrent Unit (TBRU)Network CellDiscrete stateRecurrence fcnInput embeddingsnetwork activations\x0cFigure 2: Using TBRUs to share ?ne-grained, structured representations.',
 'Top left: A high level view of multi-task learning with DRAGNN in the style of multi-task seq2seq (Luong et al., 2016).',
 'Bottom left: Extending the “stack-propagation” (Zhang and Weiss, 2016) idea to included dependency parse trees as intermediate representations.',
 'Right: Unrolled TBRUs for each setup for a input fragment “Uniformed man laughed”, utilizing the transition systems described in Section 4.  computational cost.',
 'Second, we demonstrate how the same TBRUs can be used to provide structured intermediate syntactic representations for extrac- tive sentence summarization.',
 'This yields better ac- curacy than is possible with the generic multi-task seq2seq (Dong et al., 2015; Luong et al., 2016) ap- proach.',
 'Finally, we show how multiple TBRUs for the same dependency parsing task can be stacked together to produce a single state-of-the-art depen- dency parsing model.',
 '2 Transition Systems  We use transition systems to map inputs x into a sequence of output symbols, d1 .',
 '.',
 '.',
 'dn.',
 'For the purposes of implementing DRAGNN, transi- tion systems make explicit two desirable proper- ties.',
 'First, we stipulate that the output symbols represent modi?cations of a persistent, discrete state, which makes book-keeping to construct the dynamic recurrent connections easier to express.',
 'Second, transition systems make it easy to enforce arbitrary constraints on the output, e.g.',
 'the output should produce a valid tree.',
 'Formally, we use the same setup as Andor et al.',
 '(2016), and de?ne a transition system T = {S,A, t} as:  • A set of states S(x).',
 '• A special start state s†  ? S(x).',
 '• A set of allowed decisions A(s, x) for all s ? S. • A transition function t(s, d, x) returning a new state s(cid:48) for any decision d ? A(s, x).',
 'For brevity, we will drop the dependence on x in the functions given above.',
 'Throughout this work we will use transition systems in which all com- plete structures for the same input x have the same number of decisions n(x) (or n for brevity), al- though this is not necessary.',
 'A complete structure is then a sequence of deci- sion/state pairs (s1, d1) .',
 '.',
 '.',
 '(sn, dn) such that s1 =  s†, di ? A(si) for i = 1 .',
 '.',
 '.',
 'n, and si+1 =  t(si, di).',
 'We will now de?ne recurrent network architectures that operate over these linearizations of input and output structure.',
 '3 Transition Based Recurrent Networks  We now formally de?ne how to combine transi- tion systems with recurrent networks into what we call a transition based recurrent unit (TBRU).',
 'A TBRU consists of the following:  • A transition system T , • An input function m(s) that maps states to ?xed-size vector representations, for exam- ple, an embedding lookup operation for fea- tures from the discrete state, m(s) : S (cid:55)? RK  Right-to-left LSTMSummarizationMulti-task Encoder/DecoderDependency TreesRight-to-left LSTMSummarizationDRAGNN w/ Intermediate representationsDependency TreesIntermediate representationDynamic unrolled linkslaughedmanUniformedUniformedSHIFTmanSHIFTlaughedLA(nn)laughedSHIFT<eos>LA(nsubj)<eos>RA(root)UniformedDROPmanKEEPlaughedKEEPlaughedmanUniformedUniformedSHIFTmanSHIFTlaughedLA(nn)laughedSHIFT<eos>LA(nsubj)<eos>RA(root)UniformedDROPmanKEEPlaughedKEEPRight-to-left LSTM TBRUExtractive summarization TBRUDynamic links as a function of transition stateDependency parsing TBRU\x0cFigure 3: Left: TBRU schematic.',
 'Right: Dependency parsing example.',
 'A gold parse tree and an arc- standard transition state with two sub-trees on the stack are shown.',
 'From this state, two possible actions are also shown (Shift and Right arc).',
 'To agree with the gold tree, the Shift action should be taken.',
 '• A recurrence function r(s) that maps states  to a set of previous time steps:  r(s) : S (cid:55)? P{1, .',
 '.',
 '.',
 ', i ? 1},  where P is the power set.',
 'Note that in general |r(s)| is not necessarily ?xed and can vary with s. We use r to specify state-dependent recurrent links in the unrolled computation graph.',
 '• A RNN cell that computes a new hidden rep- resentation from the ?xed and recurrent in- puts:  hs ? RNN(m(s),{hi | i ? r(s)}).',
 'Example 1.',
 'Sequential tagging RNN.',
 'Let the input x = {x1, .',
 '.',
 '.',
 ', xn} be a sequence of word embeddings, and the output be a sequence of tags d1, .',
 '.',
 '.',
 ', dn.',
 'Then we can model a simple LSTM tagger as follows:  • T sequentially tags each input token, where si = {1, .',
 '.',
 '.',
 ', di?1}, and A is the set of pos- sible tags.',
 'We call this the tagger transition system.',
 'previous state.',
 'token to be tagged.',
 '• m(si) = xi, the word embedding for the next • r(si) = {i? 1} to connect the network to the • RNN is a single instance of the LSTM cell.',
 'Example 2.',
 'Parsey McParseface.',
 'The open- source syntactic parsing model of Andor et al.',
 '(2016) can be de?ned in our framework as fol- lows:  • T is the arc-standard transition system (Fig- ure 3), so the state contains all words and par- tially built trees on the stack as well as unseen words on the buffer.',
 '• m(si) is the concatenation of 52 feature embeddings extracted from tokens based on their positions in the stack and the buffer.',
 'network.',
 '• r(si) = {} is empty, as this is a feed-forward • RNN is a feed-forward multi-layer percep-  tron (MLP).',
 'Inference with TBRUs.',
 'Given the above, infer- ence in the TBRU proceeds as follows: 1.',
 'Initialize s1 = s†.',
 '2.',
 'For i = 1, .',
 '.',
 '.',
 ', n:  (a) Update the hidden state:  (b) Update the transition state:  hi ? RNN(m(si),{hj | j ? r(si)}).',
 'di ? argmaxd?A(si) w(cid:62)  si+1 ?  t(si, di).',
 'd hi,  A schematic overview of a single TBRU is pre- sented in Figure 3.',
 'By adjusting RNN, r, and T , TBRUs can represent a wide variety of neural architectures.',
 '3.1 Connecting multiple TBRUs to learn  shared representations  While TBRUs are a useful abstraction for describ- ing recurrent models, the primary motivation for this framework is to allow new architectures by combining representations across tasks and com- positional structures.',
 'We do this by connecting multiple TBRUs with different transition systems via the recurrence function r(s).',
 'We formally aug- ment the above de?nition as follows:  1.',
 'We execute a list of T TBRU components, one at a time, so that each TBRU advances a global step counter.',
 'Note that for simplicity, we assume an earlier TBRU ?nishes all of its steps before the next one starts execution.',
 'LSTM / MLPCellLSTM / MLPCellm(si)LSTM / MLPCellhisi······s1s2m(s1)m(s2)h1h2LSTM / MLPCellh3s3m(s3)r(si)={1,3}d1 argmaxd2A(s1)w>dh1d2 argmaxd2A(s2)w>dh2······BobgaveAliceapretty?oweronMonday.nsubjiobjpunctdobjdetamodpreppobjgave?oweronMonday.BobAlicensubjiobjaprettydetamodStackBuffergave?oweronMonday.BobAlicensubjiobjaprettydetamodgave?owerMonday.BobAlicensubjiobjaprettydetamodondobjTransition state:Dependency Parse:Bufferd = Shift (correct)d = Right arc (incorrect)\x0cFigure 4: Detailed schematic for the compositional dependency parser used in our experiments.',
 'TBRU 1 consumes each input word right-to-left.',
 'TBRU 2 uses the arc-standard transition system.',
 'Note how each Shift action causes the TBRU 1?TBRU 2 link to advance.',
 'The dynamic recurrent inputs to each state are highlighted; the stack representations are obtained from the last Reduce action to modify each sub-tree.',
 '2.',
 'Each transition state from the ?’th compo- nent s? has access to the terminal states from every prior transition system, and the recur- rence function r(s? ) for any given compo- nent can pull hidden activations from every prior one as well.',
 'Example 3.',
 '“Input” transducer TBRUs via no- op decisions.',
 'We ?nd it useful to de?ne TBRUs even when the transition system decisions don’t correspond to any output.',
 'These TBRUs, which we call no-op TBRUs, transduce the input accord- ing to some linearization.',
 'The simplest is the shift- only transition system, in which the state is just an input pointer si = {i}, and there is only one tran- sition which advances it: t(si,·) = {i + 1}.',
 'Exe- cuting this transition system will produce a hidden representation hi for every input token.',
 'Example 4.',
 'Encoder/decoder networks with TBRUs.',
 'We can reproduce the encoder/decoder framework for sequence tagging by using two TBRUs: one using the shift-only transition system to encode the input, and the other using the tagger transition system.',
 'For input x = {x1, .',
 '.',
 '.',
 ', xn}, we connect them as follows: • For shift-only TBRU: m(si) = xi, r(si) = {i ? 1}.',
 '• For tagger TBRU: m(sn+i) = ydn+i?1, r(si) = {n, n + i ? 1}.',
 'We observe that the tagger TBRU starts at step n after the shift-only TBRU ?nishes, that yj is a ?xed embedding vector for the output tag j, and that the tagger TBRU has access to both the ?- nal encoding vector hn as well as its own previous time step hn+i?1.',
 'Example 4.',
 'Bi-directional LSTM tagger.',
 'With three TBRUs, we can implement a simple bi-  directional tagger.',
 'The ?rst two run the shift-only transition system, but in opposite directions.',
 'The ?nal TBRU runs the tagger transition system and concatenates the two representations:  • Left to right: T = shift-only, m(si) = xi, r(si) = {i ? 1}.',
 '• Right to left: T = shift-only, m(sn+i) = xn?i, r(sn+i) = {n + i ? 1}.',
 '• Tagger: T = tagger, m(s2n+i) = {}, r(s2n+i) = {i, 2n ? i}.',
 'We observe that the network cell in the tag- ger TBRU takes recurrences only from the bi- directional representations, and so is not recurrent in the traditional sense.',
 'See Fig.',
 '1 for an unrolled example.',
 'Example 5.',
 'Multi-task bi-directional tagging.',
 'Here we observe that it’s possible to add addi- tional annotation tasks to the bi-directional TBRU stack from Example 4 simply by adding more in- stances of the tagger TBRUs that produce outputs from different tag sets, e.g.',
 'parts-of-speech vs. morphological tags.',
 'Most important, however, is that any additional TBRUs have access to all three earlier TBRUs.',
 'This means that we can support the “stack-propagation” (Zhang and Weiss, 2016) style of multi-task learning simply by changing r for the last TBRU:  • Traditional multi-task:  r(s3n+i) = {i, 2n ? i}  • Stack-prop: r(s3n+i) = {  i(cid:124)(cid:123)(cid:122)(cid:125)  , 2n + i Tagger TBRU Remark: the raison d’ˆetre of DRAGNN.',
 'This example highlights the primary advantage of our  , 2n ? i Right-to-left  Left-to-right  (cid:124) (cid:123)(cid:122) (cid:125)  (cid:124) (cid:123)(cid:122) (cid:125)  }  BobgaveAliceapretty?oweronMonday.BobgaveAliceapretty?oweronMondayStackBufferINPUT(s)===..ShShLShRShShShLLUnrolled graph (incomplete):Recurrent inputs:TBRU 1TBRU 2Subtree(s,S0)Subtree(s,S1)\x0cParsing TBRU recurrence, r(si) ? {1, .',
 '.',
 '.',
 ', n + i} Input links  {n} {n}  Attention Attention INPUT(si) INPUT(si)  Recurrent edges {n + i ? 1}  {SUBTREE(si, S0), SUBTREE(si, S1)}  {n + i ? 1}  {SUBTREE(si, S0), SUBTREE(si, S1)}  {n + i ? 1}  {SUBTREE(si, S0), SUBTREE(si, S1)}  Questions  Parsing Accuracy (%) News 27.3 36.0 76.1 89.0 87.1 90.9  70.1 75.6 84.8 91.9 89.7 92.1  Runtime  O(n) O(n) O(n2) O(n2)  O(n) O(n)  Table 1: Dynamic links enable much more accurate, ef?cient linear-time parsing models on the Treebank Union dev set.',
 'We vary the recurrences r to explore utilizing explicit structure in the parsing TBRU.',
 'Utilizing the explicit INPUT(si) pointer is more effective and more ef?cient than a quadratic attention mechanism.',
 'Incorporating the explicit stack structure via recurrent links further improves performance.',
 'formulation: a TBRU can serve as both an en- coder for downstream tasks and as a decoder for its own task simultaneously.',
 'This idea will prove particularly powerful when we consider syntac- tic parsing, which involves compositional struc- ture over the input.',
 'For example, consider a no-op TBRU that traverses an input sequence x1, .',
 '.',
 '.',
 ', xn in the order determined by a binary parse tree: this transducer can implement a recursive tree- structured network in the style of Tai et al.',
 '(2015), which computes representations for sub-phrases in the tree.',
 'In contrast, with DRAGNN, we can use the arc-standard parser directly to produce the parse tree as well as encode sub-phrases into rep- resentations.',
 'Example 6.',
 'Compositional representations from arc-standard dependency parsing.',
 'We use the arc-standard transition system (Nivre, 2006) to model dependency trees.',
 'The system maintains two data structures as part of the state s: an input pointer and a stack (Figure 3).',
 'Trees are built bottom up via three possible attachment decisions.',
 'Assume that the stack consists of S = {A, B}, with the next token being C. We use S0 and S1 to refer to the top two tokens on the stack.',
 'Then the decisions are de?ned as:  • Shift: Push the next token on to the stack: S = {A, B, C}, and advance the input pointer.',
 '• Left arc + label: Add an arc A ?label B, and remove A from the stack: S = {B}.',
 '• Right arc + label: Add an arc A ?label B, and remove B from the stack: S = {A}.',
 'For a given parser state si, we compute two types of recurrences:  • rINPUT(si) = {INPUT(si)}, where INPUT re-  turns the index of the next input token.',
 '• rSTACK(si) =  {SUBTREE(si, S0), SUBTREE(s, S1)}, where SUBTREE(S,I) is a function returning the index of the last decision that modi?ed the i’th token:  SUBTREE(s, i) = argmax  {dj s.t.',
 'dj shifts or adds a new  j  child to token i}  We show an example of the links constructed by these recurrences in Figure 4, and we investigate variants of this model in Section 4.',
 'This model is recursively compositional according to the de- cision taken by the network: when the TBRU at step si decides to add an arc A ? B for state, the activations hi will be used to represent that new subtree in future decisions.1  Example 7.',
 'Extractive summarization pipeline with parse representations.',
 'To model extrac- tive summarization, we follow Andor et al.',
 '(2016) and use a tagger transition system with two tags: Keep and Drop.',
 'However, whereas Andor et al.',
 '(2016) use discrete features of the parse tree, we can utilize the SUBTREE recurrence function to pull compositional, phrase-based representations of tokens as constructed by the dependency parser.',
 'This model is outlined in Fig.',
 '2.',
 'A full speci?ca- tion is given in the Appendix.',
 '1This composition function is similar to that in the con- stituent parsing SPINN model (Bowman et al., 2016), but with several key differences.',
 'Since we use TBRUs, we com- pose new representations for “Shift” actions as well as reduc- tions, we take inputs from other recurrent models, and we can utilize subtree representations in downstream tasks.',
 'Input representation  Multi-task style  Single LSTM  Bi-LSTM  Multi-task LSTM  – –  Luong et al.',
 '(2016)  Parse sub-trees (Figure 2) Zhang and Weiss (2016)  A (%) F1 (%) LAS (%) 28.93 29.51 30.07 30.56  79.75 80.03 80.31 80.74  89.42 89.13  – –  Table 2: Single- vs. multi-task learning with DRAGNN on extractive summarization.',
 '“A” is full-sentence accuracy of the extraction model, “F1” is per-token F1 score, and “LAS” is labeled parsing accuracy on the Treebank Union News dev set.',
 'Both multi-task models that use the parsing data outperform the single-task approach, but the model that uses parses as an intermediate representation via our extension of Zhang and Weiss (2016) (Fig.',
 '2) is more effective.',
 'The locally normalized model in Andor et al.',
 '(2016) obtains 30.50% accuracy and 78.72% F1.',
 '3.2 How to train a DRAGNN Given a list of TBRUs, we propose the following learning procedure.',
 'We assume training data con- sists of examples x along with gold decision se- quences for one of the TBRUs in the DRAGNN.',
 'At a minimum, we need such data for the ?nal TBRU.',
 'Given decisions d1 .',
 '.',
 '.',
 'dN from prior com- ponents 1 .',
 '.',
 '.',
 'T ? 1, we de?ne a log-likelihood ob- jective to train the T ’th TBRU along its gold deci- sion sequence d(cid:63)  N +1, .',
 '.',
 '.',
 ', d(cid:63)  N +n:  L(x, d(cid:63)  (cid:88)  i  N +1:N +n; ?) = log P (d(cid:63)  N +i | d1:N , d(cid:63)  N +1:N +i?1; ?),  (1)  where ? are the combined parameters across all TBRUs.',
 'Eq.',
 '(1) is locally normalized (Andor et al., 2016), since we optimize the probabilities of the individual decisions in the gold sequence.',
 'The remaining question is where the decisions d1 .',
 '.',
 '.',
 'dN come from.',
 'There are two options here: either 1) they come as part of the gold annotation (e.g.',
 'if we have joint tagging and parsing data), or 2) they are predicted by unrolling the previous components.',
 'When training the stacked extractive summarization model, the parse trees will be pre- dicted by the previously trained parser TBRU.',
 'When training a given TBRU, we unroll an en- tire input sequence and then use backpropagation through structure (Goller and Kuchler, 1996) to optimize (1).',
 'To train the whole system on a set of C datasets, we use a strategy similar to (Dong et al., 2015; Luong et al., 2016).',
 'We sample a tar- get task c, 1 ? c ? C, based on a pre-de?ned dis- tribution, and take a stochastic optimization step on the objective of task c’s TBRU.',
 'In practice, task sampling is usually preceded by a deterministic number of pre-training steps, allowing, for exam-  ple, to run a certain number of tagger training steps before running any parser training steps.',
 '4 Experiments  In this section, we evaluate three aspects of our approach on two NLP tasks: English dependency parsing and extractive sentence summarization.',
 'For English dependency parsing, we primarily use the Union Treebank setup from Andor et al.',
 '(2016).',
 'By evaluating on both news and ques- tions domains, we can separately evaluate how the model handles naturally longer and shorter form text.',
 'On the Union Treebank setup there are 93 possible actions considering all arc-label combi- nations.',
 'For extractive sentence summarization, we use the dataset of Filippova and Altun (2013), where a large news collection is used to heuris- tically generate compression instances.',
 'The ?- nal corpus contains about 2.3M compression in- stances, but since we evaluated multiple tasks us- ing this data, we sub-sampled the training set to be comparably sized to the parsing data (?60K training sentences).',
 'The test set contains 160K examples.',
 'We implement our method in Tensor- Flow, using mini-batches of size 4 and following the averaged momentum training and hyperparam- eter tuning procedure of Weiss et al.',
 '(2015).',
 'explicit  improves the  structure explore  en- Using coder/decoder We impact of types of recurrences on dependency different In this setup, we used rela- parsing in Table 1. single-layer LSTMs with tively small models: 256 hidden units, taking 32-dimensional word or output symbol embeddings as input to each cell.',
 'In each case, the parsing TBRU takes input from a right-to-left shift-only TBRU.',
 'Under these settings, the pure encoder/decoder seq2seq  \x0cModel  Andor et al.',
 '(2016) Left-to-right parsing Deep stacked parsing  Union-News  Union-Web  Union-QTB  UAS 94.44 94.60 94.66  LAS 92.93 93.17 93.23  POS 97.77 97.88 98.09  UAS 90.17 90.09 90.22  LAS 87.54 87.50 87.67  POS 94.80 94.75 95.06  UAS 95.40 95.62 96.05  LAS 93.64 94.06 94.51  POS 96.86 96.76 97.25  Table 3: Deep stacked parsing compared to state-of-the-art on Treebank Union for parsing and POS.',
 'model simply does not have the capacity to parse newswire text with any degree of accuracy, but the TBRU-based approach is nearly state-of-the-art at the same exact computational cost.',
 'As a point of comparison and an alternative to using input pointers, we also implemented an attention mech- anism within DRAGNN.',
 'We used the dot-product formulation from Parikh et al.',
 '(2016), where r(si) in the parser takes in all of the shift-only TBRU’s hidden states and RNN aggregates over them.',
 'Utilizing parse representations improves sum- marization We evaluate our approach on the summarization task in Table 2.',
 'We compare two single-task LSTM tagging baselines against two multi-task approaches: an adaptation of Luong et al.',
 '(2016) and the stack-propagation idea of Zhang and Weiss (2016).',
 'In both multi-task se- tups, we use a right-to-left shift-only TBRU to en- code the input, and connect it to both our com- positional arc-standard dependency parser and the Keep/Drop summarization tagging model.',
 'In both setups we do not follow seq2seq, but utilize the INPUT function to connect output de- cisions directly to input token representations.',
 'in the stack-prop case, we use the However, SUBTREE function to connect the tagging TBRU to the parser TBRU’s phrase representations di- rectly (Figure 2).',
 'We ?nd that allowing the com- pressor to directly use the parser’s phrase repre- sentations signi?cantly improves the outcome of the multi-task learning setup.',
 'In both setups, we pretrained the parsing model for 400K steps and tuned the subsequent ratio of parser/tagger update steps using a development set.',
 'Deep stacked bi-directional parsing Here we propose a continuous version of the bi-directional parsing model of Attardi and Dell’Orletta (2009): ?rst, the sentence is parsed in the left-to-right or- der as usual; then a right-to-left transition sys- tem analyzes the sentence in reverse order using addition features extracted from the left-to-right parser.',
 'In our version, we connect the right-to-left  parsing TBRU directly to the phrase representa- tions of the left-to-right parsing TBRU, again us- ing the SUBTREE function.',
 'Our parser has the sig- ni?cant advantage that the two directions of pars- ing can affect each other during training.',
 'During each training step the right-to-left parser uses rep- resentations obtained using the predictions of the left-to-right parser.',
 'Thus, the right-to-left parser can backpropagate error signals through the left- to-right parser and reduce cascading errors caused by the pipeline.',
 'Our ?nal model uses 5 TBRU units.',
 'Inspired by Zhang and Weiss (2016), a left-to-right POS tagging TBRU provides the ?rst layer of rep- resentations.',
 'Next, two shift-only TBRUs, one in each direction, provide representations to the parsers.',
 'Finally, we connect the left-to-right parser to the right-to-left parser using links de?ned via the SUBTREE function.',
 'The result (Table 3) is a state-of-the-art dependency parser, yielding the highest published accuracy on the Treebank Union setup for both part of speech tagging and parsing.',
 '5 Conclusions  We presented a compact, modular framework for describing recurrent neural architectures.',
 'We eval- uated our dynamically structured model and found it to be signi?cantly more ef?cient and accurate than attention mechanisms for dependency pars- ing and extractive sentence summarization in both single- and multi-task setups.',
 'While we focused primarily on syntactic parsing, the framework pro- vides a general means of sharing representations between tasks.',
 'There remains low-hanging fruit still to be explored: in particular, our approach can be globally normalized with multiple hypothe- ses in the intermediate structure.',
 'We also plan to push the limits of multi-task learning by combin- ing many different NLP tasks, such as translation, summarization, tagging problems, and reasoning tasks, into a single model.',
 'References Daniel Andor, Chris Alberti, David Weiss, Aliaksei Severyn, Alessandro Presta, Kuzman Ganchev, Slav Petrov, and Michael Collins.',
 '2016.',
 'Globally nor- In Pro- malized transition-based neural networks.',
 'ceedings of the 54th Annual Meeting of the Asso- ciation for Computational Linguistics.',
 'pages 2442– 2452.',
 'Giuseppe Attardi and Felice Dell’Orletta.',
 '2009.',
 'Re- verse revision and linear tree combination for de- In Proceedings of Human Lan- pendency parsing.',
 'guage Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, Companion Volume: Short Papers.',
 'Association for Computational Lin- guistics, pages 261–264.',
 'Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben- gio.',
 '2015.',
 'Neural machine translation by jointly learning to align and translate.',
 'ICLR ."
2_B1,"['Aggregated Residual Transformations for Deep Neural Networks  Saining Xie1  Ross Girshick2  Piotr Doll´ar2  Zhuowen Tu1  Kaiming He2  1UC San Diego  {s9xie,ztu}@ucsd.edu  2Facebook AI Research {rbg,pdollar,kaiminghe}@fb.com  7 1 0 2    r p A 1 1         ]  V C .',
 's c [      2 v 1 3 4 5 0  .',
 '1 1 6 1 : v i X r a  Abstract  We present a simple, highly modularized network archi- tecture for image classi?cation.',
 'Our network is constructed by repeating a building block that aggregates a set of trans- formations with the same topology.',
 'Our simple design re- sults in a homogeneous, multi-branch architecture that has only a few hyper-parameters to set.',
 'This strategy exposes a new dimension, which we call “cardinality” (the size of the set of transformations), as an essential factor in addition to the dimensions of depth and width.',
 'On the ImageNet-1K dataset, we empirically show that even under the restricted condition of maintaining complexity, increasing cardinality is able to improve classi?cation accuracy.',
 'Moreover, in- creasing cardinality is more effective than going deeper or wider when we increase the capacity.',
 'Our models, named ResNeXt, are the foundations of our entry to the ILSVRC 2016 classi?cation task in which we secured 2nd place.',
 'We further investigate ResNeXt on an ImageNet-5K set and the COCO detection set, also showing better results than its ResNet counterpart.',
 'The code and models are publicly available online1.',
 '1.',
 'Introduction  Research on visual recognition is undergoing a transi- tion from “feature engineering” to “network engineering” [25, 24, 44, 34, 36, 38, 14].',
 'In contrast to traditional hand- designed features (e.g., SIFT [29] and HOG [5]), features learned by neural networks from large-scale data [33] re- quire minimal human involvement during training, and can be transferred to a variety of recognition tasks [7, 10, 28].',
 'Nevertheless, human effort has been shifted to designing better network architectures for learning representations.',
 'Designing architectures becomes increasingly dif?cult with the growing number of hyper-parameters (width2, ?l- ter sizes, strides, etc.',
 '), especially when there are many lay- ers.',
 'The VGG-nets [36] exhibit a simple yet effective strat- egy of constructing very deep networks: stacking build-  1https://github.com/facebookresearch/ResNeXt 2Width refers to the number of channels in a layer.',
 'Figure 1.',
 'Left: A block of ResNet [14].',
 'Right: A block of ResNeXt with cardinality = 32, with roughly the same complex- ity.',
 'A layer is shown as (# in channels, ?lter size, # out channels).',
 'ing blocks of the same shape.',
 'This strategy is inherited by ResNets [14] which stack modules of the same topol- ogy.',
 'This simple rule reduces the free choices of hyper- parameters, and depth is exposed as an essential dimension in neural networks.',
 'Moreover, we argue that the simplicity of this rule may reduce the risk of over-adapting the hyper- parameters to a speci?c dataset.',
 'The robustness of VGG- nets and ResNets has been proven by various visual recog- nition tasks [7, 10, 9, 28, 31, 14] and by non-visual tasks involving speech [42, 30] and language [4, 41, 20].',
 'Unlike VGG-nets, the family of Inception models [38, 17, 39, 37] have demonstrated that carefully designed topologies are able to achieve compelling accuracy with low theoretical complexity.',
 'The Inception models have evolved over time [38, 39], but an important common property is a split-transform-merge strategy.',
 'In an Inception module, the input is split into a few lower-dimensional embeddings (by 1×1 convolutions), transformed by a set of specialized ?lters (3×3, 5×5, etc.',
 '), and merged by concatenation.',
 'It can be shown that the solution space of this architecture is a strict subspace of the solution space of a single large layer (e.g., 5×5) operating on a high-dimensional embedding.',
 'The split-transform-merge behavior of Inception modules is expected to approach the representational power of large and dense layers, but at a considerably lower computational complexity.',
 'Despite good accuracy, the realization of Inception mod- els has been accompanied with a series of complicating fac-  1  256, 1x1, 44, 3x3, 44, 1x1, 256+256, 1x1, 44, 3x3, 44, 1x1, 256256, 1x1, 44, 3x3, 44, 1x1, 256....total 32paths256-d in+256, 1x1, 6464, 3x3, 6464, 1x1, 256+256-d in256-d out256-d out\x0ctors — the ?lter numbers and sizes are tailored for each individual transformation, and the modules are customized stage-by-stage.',
 'Although careful combinations of these components yield excellent neural network recipes, it is in general unclear how to adapt the Inception architectures to new datasets/tasks, especially when there are many factors and hyper-parameters to be designed.',
 'In this paper, we present a simple architecture which adopts VGG/ResNets’ strategy of repeating layers, while exploiting the split-transform-merge strategy in an easy, ex- tensible way.',
 'A module in our network performs a set of transformations, each on a low-dimensional embedding, whose outputs are aggregated by summation.',
 'We pursuit a simple realization of this idea — the transformations to be aggregated are all of the same topology (e.g., Fig.',
 '1 (right)).',
 'This design allows us to extend to any large number of transformations without specialized designs.',
 'Interestingly, under this simpli?ed situation we show that our model has two other equivalent forms (Fig.',
 '3).',
 'The re- formulation in Fig.',
 '3(b) appears similar to the Inception- ResNet module [37] in that it concatenates multiple paths; but our module differs from all existing Inception modules in that all our paths share the same topology and thus the number of paths can be easily isolated as a factor to be in- vestigated.',
 'In a more succinct reformulation, our module can be reshaped by Krizhevsky et al.’s grouped convolu- tions [24] (Fig.',
 '3(c)), which, however, had been developed as an engineering compromise.',
 'We empirically demonstrate that our aggregated trans- formations outperform the original ResNet module, even under the restricted condition of maintaining computational complexity and model size — e.g., Fig.',
 '1(right) is designed to keep the FLOPs complexity and number of parameters of Fig.',
 '1(left).',
 'We emphasize that while it is relatively easy to increase accuracy by increasing capacity (going deeper or wider), methods that increase accuracy while maintaining (or reducing) complexity are rare in the literature.',
 'Our method indicates that cardinality (the size of the set of transformations) is a concrete, measurable dimen- sion that is of central importance, in addition to the dimen- sions of width and depth.',
 'Experiments demonstrate that in- creasing cardinality is a more effective way of gaining accu- racy than going deeper or wider, especially when depth and width starts to give diminishing returns for existing models.',
 'Our neural networks, named ResNeXt (suggesting the next dimension), outperform ResNet-101/152 [14], ResNet- 200 [15], Inception-v3 [39], and Inception-ResNet-v2 [37] on the ImageNet classi?cation dataset.',
 'In particular, a 101-layer ResNeXt is able to achieve better accuracy than ResNet-200 [15] but has only 50% complexity.',
 'Moreover, ResNeXt exhibits considerably simpler designs than all In- ception models.',
 'ResNeXt was the foundation of our sub- mission to the ILSVRC 2016 classi?cation task, in which  we secured second place.',
 'This paper further evaluates ResNeXt on a larger ImageNet-5K set and the COCO object detection dataset [27], showing consistently better accuracy than its ResNet counterparts.',
 'We expect that ResNeXt will also generalize well to other visual (and non-visual) recog- nition tasks.',
 '2.',
 'Related Work Multi-branch convolutional networks.',
 'The Inception models [38, 17, 39, 37] are successful multi-branch ar- chitectures where each branch is carefully customized.',
 'ResNets [14] can be thought of as two-branch networks where one branch is the identity mapping.',
 'Deep neural de- cision forests [22] are tree-patterned multi-branch networks with learned splitting functions.',
 'Grouped convolutions.',
 'The use of grouped convolutions dates back to the AlexNet paper [24], if not earlier.',
 'The motivation given by Krizhevsky et al.',
 '[24] is for distributing the model over two GPUs.',
 'Grouped convolutions are sup- ported by Caffe [19], Torch [3], and other libraries, mainly for compatibility of AlexNet.',
 'To the best of our knowledge, there has been little evidence on exploiting grouped convo- lutions to improve accuracy.',
 'A special case of grouped con- volutions is channel-wise convolutions in which the number of groups is equal to the number of channels.',
 'Channel-wise convolutions are part of the separable convolutions in [35].',
 'Compressing convolutional networks.',
 'Decomposition (at spatial [6, 18] and/or channel [6, 21, 16] level) is a widely adopted technique to reduce redundancy of deep convo- lutional networks and accelerate/compress them.',
 'Ioan- nou et al.',
 '[16] present a “root”-patterned network for re- ducing computation, and branches in the root are realized by grouped convolutions.',
 'These methods [6, 18, 21, 16] have shown elegant compromise of accuracy with lower complexity and smaller model sizes.',
 'Instead of compres- sion, our method is an architecture that empirically shows stronger representational power.',
 'Ensembling.',
 'Averaging a set of independently trained net- works is an effective solution to improving accuracy [24], widely adopted in recognition competitions [33].',
 'Veit et al.',
 '[40] interpret a single ResNet as an ensemble of shallower networks, which results from ResNet’s additive behaviors [15].',
 'Our method harnesses additions to aggregate a set of transformations.',
 'But we argue that it is imprecise to view our method as ensembling, because the members to be ag- gregated are trained jointly, not independently.',
 '3.',
 'Method 3.1.',
 'Template  We adopt a highly modularized design following VGG/ResNets.',
 'Our network consists of a stack of resid-  \x0coutput stage conv1 112×112  conv2  56×56  conv3  28×28  conv4  14×14  conv5  7×7  ResNet-50  7×7, 64, stride 2  3×3 max pool, stride 2  3×3, 64 1×1, 256  \uf8ee\uf8ef\uf8f0 1×1, 64 \uf8ee\uf8ef\uf8f0 1×1, 128 \uf8ee\uf8ef\uf8f0 1×1, 256 \uf8ee\uf8ef\uf8f0 1×1, 512  3×3, 128 1×1, 512  3×3, 256 1×1, 1024  3×3, 512 1×1, 2048  \uf8f9\uf8fa\uf8fb×3 \uf8f9\uf8fa\uf8fb×4 \uf8f9\uf8fa\uf8fb×6 \uf8f9\uf8fa\uf8fb×3  1×1  # params.',
 'FLOPs  global average pool 1000-d fc, softmax  25.5×106 4.1×109  ResNeXt-50 (32×4d)  7×7, 64, stride 2  3×3 max pool, stride 2  3×3, 128, C=32 1×1, 256  3×3, 256, C=32 1×1, 512  \uf8ee\uf8ef\uf8f0 1×1, 128 \uf8ee\uf8ef\uf8f0 1×1, 256 \uf8ee\uf8ef\uf8f0 1×1, 512 \uf8ee\uf8ef\uf8f0 1×1, 1024  3×3, 512, C=32 1×1, 1024  \uf8f9\uf8fa\uf8fb×3 \uf8f9\uf8fa\uf8fb×4 \uf8f9\uf8fa\uf8fb×6 \uf8f9\uf8fa\uf8fb×3  3×3, 1024, C=32 1×1, 2048 global average pool 1000-d fc, softmax  25.0×106 4.2×109  (Right) ResNeXt-50 with a 32×4d Table 1.',
 '(Left) ResNet-50.',
 'template (using the reformulation in Fig.',
 '3(c)).',
 'Inside the brackets are the shape of a residual block, and outside the brackets is the number of stacked blocks on a stage.',
 '“C=32” suggests grouped convolutions [24] with 32 groups.',
 'The numbers of parameters and FLOPs are similar between these two models.',
 'ual blocks.',
 'These blocks have the same topology, and are subject to two simple rules inspired by VGG/ResNets: (i) if producing spatial maps of the same size, the blocks share the same hyper-parameters (width and ?lter sizes), and (ii) each time when the spatial map is downsampled by a fac- tor of 2, the width of the blocks is multiplied by a factor of 2.',
 'The second rule ensures that the computational com- plexity, in terms of FLOPs (?oating-point operations, in # of multiply-adds), is roughly the same for all blocks.',
 'With these two rules, we only need to design a template module, and all modules in a network can be determined accordingly.',
 'So these two rules greatly narrow down the design space and allow us to focus on a few key factors.',
 'The networks constructed by these rules are in Table 1.',
 '3.2.',
 'Revisiting Simple Neurons  The simplest neurons in arti?cial neural networks per- form inner product (weighted sum), which is the elemen- tary transformation done by fully-connected and convolu- tional layers.',
 'Inner product can be thought of as a form of aggregating transformation:  D(cid:88)  wixi,  (1)  i=1  where x = [x1, x2, ..., xD] is a D-channel input vector to the neuron and wi is a ?lter’s weight for the i-th chan-  Figure 2.',
 'A simple neuron that performs inner product.',
 'nel.',
 'This operation (usually including some output non- linearity) is referred to as a “neuron”.',
 'See Fig.',
 '2.',
 'The above operation can be recast as a combination of splitting, transforming, and aggregating.',
 '(i) Splitting: the vector x is sliced as a low-dimensional embedding, and in the above, it is a single-dimension subspace xi.',
 '(ii) Transforming: the low-dimensional representation is trans- formed, and in the above, it is simply scaled: wixi.',
 '(iii) Aggregating: the transformations in all embeddings are ag-  gregated by(cid:80)D  i=1.',
 '3.3.',
 'Aggregated Transformations  Given the above analysis of a simple neuron, we con- sider replacing the elementary transformation (wixi) with a more generic function, which in itself can also be a net- work.',
 'In contrast to “Network-in-Network” [26] that turns out to increase the dimension of depth, we show that our “Network-in-Neuron” expands along a new dimension.',
 'Formally, we present aggregated transformations as:  F(x) =  Ti(x),  (2)  i=1  where Ti(x) can be an arbitrary function.',
 'Analogous to a simple neuron, Ti should project x into an (optionally low- dimensional) embedding and then transform it.',
 'In Eqn.',
 '(2), C is the size of the set of transformations to be aggregated.',
 'We refer to C as cardinality [2].',
 'In Eqn.',
 '(2) C is in a position similar to D in Eqn.',
 '(1), but C need not equal D and can be an arbitrary number.',
 'While the dimension of width is related to the number of simple transformations (inner product), we argue that the dimen- sion of cardinality controls the number of more complex transformations.',
 'We show by experiments that cardinality is an essential dimension and can be more effective than the dimensions of width and depth.',
 'In this paper, we consider a simple way of designing the transformation functions: all Ti’s have the same topology.',
 'This extends the VGG-style strategy of repeating layers of the same shape, which is helpful for isolating a few factors and extending to any large number of transformations.',
 'We set the individual transformation Ti to be the bottleneck- shaped architecture [14], as illustrated in Fig.',
 '1 (right).',
 'In this case, the ?rst 1×1 layer in each Ti produces the low- dimensional embedding.',
 'C(cid:88)  .......+x1x2xDx3w1w2w3wDx\x0cFigure 3.',
 'Equivalent building blocks of ResNeXt.',
 '(a): Aggregated residual transformations, the same as Fig.',
 '1 right.',
 '(b): A block equivalent to (a), implemented as early concatenation.',
 '(c): A block equivalent to (a,b), implemented as grouped convolutions [24].',
 'Notations in bold text highlight the reformulation changes.',
 'A layer is denoted as (# input channels, ?lter size, # output channels).',
 'The aggregated transformation in Eqn.',
 '(2) serves as the  residual function [14] (Fig.',
 '1 right):  C(cid:88)  y = x +  Ti(x),  (3)  i=1  where y is the output.',
 'Relation to Inception-ResNet.',
 'Some tensor manipula- tions show that the module in Fig.',
 '1(right) (also shown in Fig.',
 '3(a)) is equivalent to Fig.',
 '3(b).3 Fig.',
 '3(b) appears sim- ilar to the Inception-ResNet [37] block in that it involves branching and concatenating in the residual function.',
 'But unlike all Inception or Inception-ResNet modules, we share the same topology among the multiple paths.',
 'Our module requires minimal extra effort designing each path.',
 'Relation to Grouped Convolutions.',
 'The above module be- comes more succinct using the notation of grouped convo- lutions [24].4 This reformulation is illustrated in Fig.',
 '3(c).',
 'All the low-dimensional embeddings (the ?rst 1×1 layers) can be replaced by a single, wider layer (e.g., 1×1, 128-d in Fig 3(c)).',
 'Splitting is essentially done by the grouped convolutional layer when it divides its input channels into groups.',
 'The grouped convolutional layer in Fig.',
 '3(c) per- forms 32 groups of convolutions whose input and output channels are 4-dimensional.',
 'The grouped convolutional layer concatenates them as the outputs of the layer.',
 'The block in Fig.',
 '3(c) looks like the original bottleneck resid- ual block in Fig.',
 '1(left), except that Fig.',
 '3(c) is a wider but sparsely connected module.',
 '3An informal but descriptive proof is as follows.',
 'Note the equality: A1B1 + A2B2 = [A1, A2][B1; B2] where [ , ] is horizontal concatena- tion and [ ; ] is vertical concatenation.',
 'Let Ai be the weight of the last layer and Bi be the output response of the second-last layer in the block.',
 'In the case of C = 2, the element-wise addition in Fig.',
 '3(a) is A1B1 + A2B2, the weight of the last layer in Fig.',
 '3(b) is [A1, A2], and the concatenation of outputs of second-last layers in Fig.',
 '3(b) is [B1; B2].',
 '4In a group conv layer [24], input and output channels are divided into C groups, and convolutions are separately performed within each group.',
 'Figure 4.',
 '(Left): Aggregating transformations of depth = 2.',
 '(Right): An equivalent block, which is trivially wider.',
 'We note that  the reformulations produce nontrivial topologies only when the block has depth ?3.',
 'If the block has depth = 2 (e.g., the basic block in [14]), the reformula- tions lead to trivially a wide, dense module.',
 'See the illus- tration in Fig.',
 '4.',
 'Discussion.',
 'We note that although we present reformula- tions that exhibit concatenation (Fig.',
 '3(b)) or grouped con- volutions (Fig.',
 '3(c)), such reformulations are not always ap- plicable for the general form of Eqn.',
 '(3), e.g., if the trans- formation Ti takes arbitrary forms and are heterogenous.',
 'We choose to use homogenous forms in this paper because they are simpler and extensible.',
 'Under this simpli?ed case, grouped convolutions in the form of Fig.',
 '3(c) are helpful for easing implementation.',
 '3.4.',
 'Model Capacity  Our experiments in the next section will show that our models improve accuracy when maintaining the model complexity and number of parameters.',
 'This is not only in- teresting in practice, but more importantly, the complexity and number of parameters represent inherent capacity of models and thus are often investigated as fundamental prop- erties of deep networks [8].',
 'When we evaluate different cardinalities C while pre- serving complexity, we want to minimize the modi?cation of other hyper-parameters.',
 'We choose to adjust the width of  equivalent256, 1x1, 44, 3x3, 44, 1x1, 256+256, 1x1, 44, 3x3, 44, 1x1, 256256, 1x1, 44, 3x3, 44, 1x1, 256(a)....total 32paths256-d in+256, 1x1, 44, 3x3, 4256, 1x1, 44, 3x3, 4256, 1x1, 44, 3x3, 4+concatenate128, 1x1, 256256-d in....total 32paths(b)256, 1x1, 128128, 3x3, 128group = 32128, 1x1, 256+256-d in(c)256-d out256-d out256-d outequivalent64, 3x3, 464, 3x3, 464, 3x3, 4....total 32paths+4, 3x3, 64+4, 3x3, 644, 3x3, 6464, 3x3, 128128, 3x3, 64+\x0ccardinality C  width of bottleneck d width of group conv.',
 '1 64 64  2 40 80  4 24 96  8 14 112  32 4 128  Table 2.',
 'Relations between cardinality and width (for the template of conv2), with roughly preserved complexity on a residual block.',
 'The number of parameters is ?70k for the template of conv2.',
 'The number of FLOPs is ?0.22 billion (# params×56×56 for conv2).',
 'the bottleneck (e.g., 4-d in Fig 1(right)), because it can be isolated from the input and output of the block.',
 'This strat- egy introduces no change to other hyper-parameters (depth or input/output width of blocks), so is helpful for us to focus on the impact of cardinality.',
 'In Fig.',
 '1(left), the original ResNet bottleneck block [14] has 256· 64 + 3· 3· 64· 64 + 64· 256 ? 70k parameters and proportional FLOPs (on the same feature map size).',
 'With bottleneck width d, our template in Fig.',
 '1(right) has:  C · (256 · d + 3 · 3 · d · d + d · 256)  (4)  parameters and proportional FLOPs.',
 'When C = 32 and d = 4, Eqn.',
 '(4) ? 70k.',
 'Table 2 shows the relationship be- tween cardinality C and bottleneck width d.  Because we adopt the two rules in Sec.',
 '3.1, the above approximate equality is valid between a ResNet bottleneck block and our ResNeXt on all stages (except for the sub- sampling layers where the feature maps size changes).',
 'Ta- ble 1 compares the original ResNet-50 and our ResNeXt-50 that is of similar capacity.5 We note that the complexity can only be preserved approximately, but the difference of the complexity is minor and does not bias our results.',
 '4.',
 'Implementation details  Our implementation follows [14] and the publicly avail- able code of fb.resnet.torch [11].',
 'On the ImageNet dataset, the input image is 224×224 randomly cropped from a resized image using the scale and aspect ratio aug- mentation of [38] implemented by [11].',
 'The shortcuts are identity connections except for those increasing dimensions which are projections (type B in [14]).',
 'Downsampling of conv3, 4, and 5 is done by stride-2 convolutions in the 3×3 layer of the ?rst block in each stage, as suggested in [11].',
 'We use SGD with a mini-batch size of 256 on 8 GPUs (32 per GPU).',
 'The weight decay is 0.0001 and the momentum is 0.9.',
 'We start from a learning rate of 0.1, and divide it by 10 for three times using the schedule in [11].',
 'We adopt the weight initialization of [13].',
 'In all ablation comparisons, we evaluate the error on the single 224×224 center crop from an image whose shorter side is 256.',
 'Our models are realized by the form of Fig.',
 '3(c).',
 'We perform batch normalization (BN) [17] right after the con-  volutions in Fig.',
 '3(c).6 ReLU is performed right after each BN, expect for the output of the block where ReLU is per- formed after the adding to the shortcut, following [14].',
 'We note that the three forms in Fig.',
 '3 are strictly equiv- alent, when BN and ReLU are appropriately addressed as mentioned above.',
 'We have trained all three forms and obtained the same results.',
 'We choose to implement by Fig.',
 '3(c) because it is more succinct and faster than the other two forms.',
 '5.',
 'Experiments 5.1.',
 'Experiments on ImageNet-1K  We conduct ablation experiments on the 1000-class Im- ageNet classi?cation task [33].',
 'We follow [14] to construct 50-layer and 101-layer residual networks.',
 'We simply re- place all blocks in ResNet-50/101 with our blocks.',
 'Notations.',
 'Because we adopt the two rules in Sec.',
 '3.1, it is suf?cient for us to refer to an architecture by the template.',
 'For example, Table 1 shows a ResNeXt-50 constructed by a template with cardinality = 32 and bottleneck width = 4d (Fig.',
 '3).',
 'This network is denoted as ResNeXt-50 (32×4d) for simplicity.',
 'We note that the input/output width of the template is ?xed as 256-d (Fig.',
 '3), and all widths are dou- bled each time when the feature map is subsampled (see Table 1).',
 'Cardinality vs. Width.',
 'We ?rst evaluate the trade-off be- tween cardinality C and bottleneck width, under preserved complexity as listed in Table 2.',
 'Table 3 shows the results and Fig.',
 '5 shows the curves of error vs. epochs.',
 'Compar- ing with ResNet-50 (Table 3 top and Fig.',
 '5 left), the 32×4d ResNeXt-50 has a validation error of 22.2%, which is 1.7% lower than the ResNet baseline’s 23.9%.',
 'With cardinality C increasing from 1 to 32 while keeping complexity, the error rate keeps reducing.',
 'Furthermore, the 32×4d ResNeXt also has a much lower training error than the ResNet counter- part, suggesting that the gains are not from regularization but from stronger representations.',
 'Similar trends are observed in the case of ResNet-101 (Fig.',
 '5 right, Table 3 bottom), where the 32×4d ResNeXt- 101 outperforms the ResNet-101 counterpart by 0.8%.',
 'Al- though this improvement of validation error is smaller than that of the 50-layer case, the improvement of training er- ror is still big (20% for ResNet-101 and 16% for 32×4d ResNeXt-101, Fig.',
 '5 right).',
 'In fact, more training data will enlarge the gap of validation error, as we show on an ImageNet-5K set in the next subsection.',
 'Table 3 also suggests that with complexity preserved, in- creasing cardinality at the price of reducing width starts to show saturating accuracy when the bottleneck width is  5The marginally smaller number of parameters and marginally higher  FLOPs are mainly caused by the blocks where the map sizes change.',
 '6With BN, for the equivalent form in Fig.',
 '3(a), BN is employed after  aggregating the transformations and before adding to the shortcut.',
 'Figure 5.',
 'Training curves on ImageNet-1K.',
 '(Left): ResNet/ResNeXt-50 with preserved complexity (?4.1 billion FLOPs, ?25 million parameters); (Right): ResNet/ResNeXt-101 with preserved complexity (?7.8 billion FLOPs, ?44 million parameters).',
 'ResNet-50 ResNeXt-50 ResNeXt-50 ResNeXt-50 ResNeXt-50 ResNet-101 ResNeXt-101 ResNeXt-101 ResNeXt-101 ResNeXt-101  setting 1 × 64d 2 × 40d 4 × 24d 8 × 14d 32 × 4d 1 × 64d 2 × 40d 4 × 24d 8 × 14d 32 × 4d  top-1 error (%)  23.9 23.0 22.6 22.3 22.2 22.0 21.7 21.4 21.3 21.2  Table 3.',
 'Ablation experiments on ImageNet-1K.',
 '(Top): ResNet- 50 with preserved complexity (?4.1 billion FLOPs); (Bottom): ResNet-101 with preserved complexity (?7.8 billion FLOPs).',
 'The error rate is evaluated on the single crop of 224×224 pixels.',
 'small.',
 'We argue that it is not worthwhile to keep reducing width in such a trade-off.',
 'So we adopt a bottleneck width no smaller than 4d in the following.',
 'Increasing Cardinality vs. Deeper/Wider.',
 'Next we in- vestigate increasing complexity by increasing cardinality C or increasing depth or width.',
 'The following comparison can also be viewed as with reference to 2× FLOPs of the ResNet-101 baseline.',
 'We compare the following variants that have ?15 billion FLOPs.',
 '(i) Going deeper to 200 lay- ers.',
 'We adopt the ResNet-200 [15] implemented in [11].',
 '(ii) Going wider by increasing the bottleneck width.',
 '(iii) Increasing cardinality by doubling C. Table 4 shows that increasing complexity by 2× consis- tently reduces error vs. the ResNet-101 baseline (22.0%).',
 'But the improvement is small when going deeper (ResNet- 200, by 0.3%) or wider (wider ResNet-101, by 0.7%).',
 'On the contrary, increasing cardinality C shows much  setting  1× complexity references: 1 × 64d ResNet-101 32 × 4d ResNeXt-101 2× complexity models follow: 1 × 64d ResNet-200 [15] ResNet-101, wider 1 × 100d 2 × 64d ResNeXt-101 64 × 4d ResNeXt-101  top-1 err (%)  top-5 err (%)  22.0 21.2  21.7 21.3 20.7 20.4  6.0 5.6  5.8 5.7 5.5 5.3  Table 4.',
 'Comparisons on ImageNet-1K when the number of FLOPs is increased to 2× of ResNet-101’s.',
 'The error rate is evalu- ated on the single crop of 224×224 pixels.',
 'The highlighted factors are the factors that increase complexity.',
 'better results than going deeper or wider.',
 'The 2×64d ResNeXt-101 (i.e., doubling C on 1×64d ResNet-101 base- line and keeping the width) reduces the top-1 error by 1.3% to 20.7%.',
 'The 64×4d ResNeXt-101 (i.e., doubling C on 32×4d ResNeXt-101 and keeping the width) reduces the top-1 error to 20.4%.',
 'We also note that 32×4d ResNet-101 (21.2%) performs better than the deeper ResNet-200 and the wider ResNet- 101, even though it has only ?50% complexity.',
 'This again shows that cardinality is a more effective dimension than the dimensions of depth and width.',
 'Residual connections.',
 'The following table shows the ef- fects of the residual (shortcut) connections:  setting 1 × 64d ResNet-50 ResNeXt-50 32 × 4d  w/ residual w/o residual  23.9 22.2  31.2 26.1  Removing shortcuts from the ResNeXt-50 increases the er- ror by 3.9 points to 26.1%.',
 'Removing shortcuts from its  0102030405060708090epochs1520253035404550top-1 error (%)ResNet-50 (1 x 64d) trainResNet-50 (1 x 64d) valResNeXt-50 (32 x 4d) trainResNeXt-50 (32 x 4d) val0102030405060708090epochs1520253035404550top-1 error (%)ResNet-101 (1 x 64d) trainResNet-101 (1 x 64d) valResNeXt-101 (32 x 4d) trainResNeXt-101 (32 x 4d) val\x0cResNet-50 counterpart is much worse (31.2%).',
 'These com- parisons suggest that the residual connections are helpful for optimization, whereas aggregated transformations are stronger representations, as shown by the fact that they perform consistently better than their counterparts with or without residual connections.',
 'Performance.',
 'For simplicity we use Torch’s built-in grouped convolution implementation, without special opti- mization.',
 'We note that this implementation was brute-force and not parallelization-friendly.',
 'On 8 GPUs of NVIDIA M40, training 32×4d ResNeXt-101 in Table 3 takes 0.95s per mini-batch, vs. 0.70s of ResNet-101 baseline that has similar FLOPs.',
 'We argue that this is a reasonable overhead.',
 'We expect carefully engineered lower-level implementation (e.g., in CUDA) will reduce this overhead.',
 'We also expect that the inference time on CPUs will present less overhead.',
 'Training the 2×complexity model (64×4d ResNeXt-101) takes 1.7s per mini-batch and 10 days total on 8 GPUs.',
 'Comparisons with state-of-the-art results.',
 'Table 5 shows more results of single-crop testing on the ImageNet val- In addition to testing a 224×224 crop, we idation set.',
 'also evaluate a 320×320 crop following [15].',
 'Our re- sults compare favorably with ResNet, Inception-v3/v4, and Inception-ResNet-v2, achieving a single-crop top-5 error rate of 4.4%.',
 'In addition, our architecture design is much simpler than all Inception models, and requires consider- ably fewer hyper-parameters to be set by hand.',
 'ResNeXt is the foundation of our entries to the ILSVRC 2016 classi?cation task, in which we achieved 2nd place.',
 'We note that many models (including ours) start to get sat- urated on this dataset after using multi-scale and/or multi- crop testing.',
 'We had a single-model top-1/top-5 error rates of 17.7%/3.7% using the multi-scale dense testing in [14], on par with Inception-ResNet-v2’s single-model results of 17.8%/3.7% that adopts multi-scale, multi-crop testing.',
 'We had an ensemble result of 3.03% top-5 error on the test set, on par with the winner’s 2.99% and Inception-v4/Inception- ResNet-v2’s 3.08% [37].',
 '224×224  top-1 err top-5 err top-1 err  320×320 / 299×299 top-5 err  ResNet-101 [14] ResNet-200 [15] Inception-v3 [39] Inception-v4 [37] Inception-ResNet-v2 [37] ResNeXt-101 (64 × 4d)  22.0 21.7  - - -  20.4  6.0 5.8 - - - 5.3  -  20.1 21.2 20.0 19.9 19.1  - 4.8 5.6 5.0 4.9 4.4  Table 5.',
 'State-of-the-art models on the ImageNet-1K validation set (single-crop testing).',
 'The test size of ResNet/ResNeXt is 224×224 and 320×320 as in [15] and of the Inception models is 299×299.',
 'Figure 6.',
 'ImageNet-5K experiments.',
 'Models are trained on the 5K set and evaluated on the original 1K validation set, plotted as a 1K-way classi?cation task.',
 'ResNeXt and its ResNet counterpart have similar complexity.',
 '5K-way classi?cation 1K-way classi?cation  setting 1 × 64d ResNet-50 ResNeXt-50 32 × 4d 1 × 64d ResNet-101 ResNeXt-101 32 × 4d  top-1 45.5 42.3 42.4 40.1  top-5 19.4 16.8 16.9 15.1  top-1 27.1 24.4 24.2 22.2  top-5 8.2 6.6 6.8 5.7  Table 6.',
 'Error (%) on ImageNet-5K.',
 'The models are trained on ImageNet-5K and tested on the ImageNet-1K val set, treated as a 5K-way classi?cation task or a 1K-way classi?cation task at test time.',
 'ResNeXt and its ResNet counterpart have similar complex- ity.',
 'The error is evaluated on the single crop of 224×224 pixels.',
 '5.2.',
 'Experiments on ImageNet-5K  The performance on ImageNet-1K appears to saturate.',
 'But we argue that this is not because of the capability of the models but because of the complexity of the dataset.',
 'Next we evaluate our models on a larger ImageNet subset that has 5000 categories.',
 'Our 5K dataset is a subset of the full "
3_B1,"['An End-to-End Trainable Neural Network for Image-based Sequence  Recognition and Its Application to Scene Text Recognition  Baoguang Shi, Xiang Bai and Cong Yao  School of Electronic Information and Communications  Huazhong University of Science and Technology, Wuhan, China  {shibaoguang,xbai}@hust.edu.cn, yaocong2010@gmail.com  5 1 0 2    l u J    1 2      ]  V C .',
 's c [      1 v 7 1 7 5 0  .',
 '7 0 5 1 : v i X r a  Abstract  Image-based sequence recognition has been a long- standing research topic in computer vision.',
 'In this pa- per, we investigate the problem of scene text recognition, which is among the most important and challenging tasks in image-based sequence recognition.',
 'A novel neural net- work architecture, which integrates feature extraction, se- quence modeling and transcription into a uni?ed frame- work, is proposed.',
 'Compared with previous systems for scene text recognition, the proposed architecture possesses four distinctive properties: (1) It is end-to-end trainable, in contrast to most of the existing algorithms whose compo- nents are separately trained and tuned.',
 '(2) It naturally han- dles sequences in arbitrary lengths, involving no character segmentation or horizontal scale normalization.',
 '(3) It is not con?ned to any prede?ned lexicon and achieves remarkable performances in both lexicon-free and lexicon-based scene text recognition tasks.',
 '(4) It generates an effective yet much smaller model, which is more practical for real-world ap- plication scenarios.',
 'The experiments on standard bench- marks, including the IIIT-5K, Street View Text and ICDAR datasets, demonstrate the superiority of the proposed algo- rithm over the prior arts.',
 'Moreover, the proposed algorithm performs well in the task of image-based music score recog- nition, which evidently veri?es the generality of it.',
 '1.',
 'Introduction  Recently, the community has seen a strong revival of neural networks, which is mainly stimulated by the great success of deep neural network models, speci?cally Deep Convolutional Neural Networks (DCNN), in various vision tasks.',
 'However, majority of the recent works related to deep neural networks have devoted to detection or classi?cation of object categories [12, 25].',
 'In this paper, we are con- cerned with a classic problem in computer vision: image- based sequence recognition.',
 'In real world, a stable of vi-  sual objects, such as scene text, handwriting and musical score, tend to occur in the form of sequence, not in isola- tion.',
 'Unlike general object recognition, recognizing such sequence-like objects often requires the system to predict a series of object labels, instead of a single label.',
 'There- fore, recognition of such objects can be naturally cast as a sequence recognition problem.',
 'Another unique property of sequence-like objects is that their lengths may vary drasti- cally.',
 'For instance, English words can either consist of 2 characters such as “OK” or 15 characters such as “congrat- ulations”.',
 'Consequently, the most popular deep models like DCNN [25, 26] cannot be directly applied to sequence pre- diction, since DCNN models often operate on inputs and outputs with ?xed dimensions, and thus are incapable of producing a variable-length label sequence.',
 'Some attempts have been made to address this problem for a speci?c sequence-like object (e.g.',
 'scene text).',
 'For example, the algorithms in [35, 8] ?rstly detect individual characters and then recognize these detected characters with DCNN models, which are trained using labeled character images.',
 'Such methods often require training a strong char- acter detector for accurately detecting and cropping each character out from the original word image.',
 'Some other approaches (such as [22]) treat scene text recognition as an image classi?cation problem, and assign a class label to each English word (90K words in total).',
 'It turns out a large trained model with a huge number of classes, which is dif?cult to be generalized to other types of sequence- like objects, such as Chinese texts, musical scores, etc., be- cause the numbers of basic combinations of such kind of sequences can be greater than 1 million.',
 'In summary, cur- rent systems based on DCNN can not be directly used for image-based sequence recognition.',
 'Recurrent neural networks (RNN) models, another im- portant branch of the deep neural networks family, were mainly designed for handling sequences.',
 'One of the ad- vantages of RNN is that it does not need the position of each element in a sequence object image in both training and testing.',
 'However, a preprocessing step that converts  1  \x0can input object image into a sequence of image features, is usually essential.',
 'For example, Graves et al.',
 '[16] extract a set of geometrical or image features from handwritten texts, while Su and Lu [33] convert word images into sequential HOG features.',
 'The preprocessing step is independent of the subsequent components in the pipeline, thus the existing systems based on RNN can not be trained and optimized in an end-to-end fashion.',
 'Several conventional scene text recognition methods that are not based on neural networks also brought insightful ideas and novel representations into this ?eld.',
 'For example, Almaz`an et al.',
 '[5] and Rodriguez-Serrano et al.',
 '[30] pro- posed to embed word images and text strings in a common vectorial subspace, and word recognition is converted into a retrieval problem.',
 'Yao et al.',
 '[36] and Gordo et al.',
 '[14] used mid-level features for scene text recognition.',
 'Though achieved promising performance on standard benchmarks, these methods are generally outperformed by previous al- gorithms based on neural networks [8, 22], as well as the approach proposed in this paper.',
 'The main contribution of this paper is a novel neural network model, whose network architecture is speci?cally designed for recognizing sequence-like objects in images.',
 'The proposed neural network model is named as Convo- lutional Recurrent Neural Network (CRNN), since it is a combination of DCNN and RNN.',
 'For sequence-like ob- jects, CRNN possesses several distinctive advantages over conventional neural network models: 1) It can be directly learned from sequence labels (for instance, words), requir- ing no detailed annotations (for instance, characters); 2) It has the same property of DCNN on learning informative representations directly from image data, requiring neither hand-craft features nor preprocessing steps, including bi- narization/segmentation, component localization, etc.',
 '; 3) It has the same property of RNN, being able to produce a se- quence of labels; 4) It is unconstrained to the lengths of sequence-like objects, requiring only height normalization in both training and testing phases; 5) It achieves better or highly competitive performance on scene texts (word recog- nition) than the prior arts [23, 8]; 6) It contains much less parameters than a standard DCNN model, consuming less storage space.',
 '2.',
 'The Proposed Network Architecture  The network architecture of CRNN, as shown in Fig.',
 '1, consists of three components, including the convolutional layers, the recurrent layers, and a transcription layer, from bottom to top.',
 'At the bottom of CRNN, the convolutional layers auto- matically extract a feature sequence from each input image.',
 'On top of the convolutional network, a recurrent network is built for making prediction for each frame of the feature sequence, outputted by the convolutional layers.',
 'The tran-  scription layer at the top of CRNN is adopted to translate the per-frame predictions by the recurrent layers into a label se- quence.',
 'Though CRNN is composed of different kinds of network architectures (eg.',
 'CNN and RNN), it can be jointly trained with one loss function.',
 'Figure 1.',
 'The network architecture.',
 'The architecture consists of three parts: 1) convolutional layers, which extract a feature se- quence from the input image; 2) recurrent layers, which predict a label distribution for each frame; 3) transcription layer, which translates the per-frame predictions into the ?nal label sequence.',
 '2.1.',
 'Feature Sequence Extraction  In CRNN model, the component of convolutional layers is constructed by taking the convolutional and max-pooling layers from a standard CNN model (fully-connected layers are removed).',
 'Such component is used to extract a sequen- tial feature representation from an input image.',
 'Before be- ing fed into the network, all the images need to be scaled to the same height.',
 'Then a sequence of feature vectors is extracted from the feature maps produced by the compo- nent of convolutional layers, which is the input for the re- current layers.',
 'Speci?cally, each feature vector of a feature sequence is generated from left to right on the feature maps by column.',
 'This means the i-th feature vector is the con- catenation of the i-th columns of all the maps.',
 'The width of each column in our settings is ?xed to single pixel.',
 'As the layers of convolution, max-pooling, and element- wise activation function operate on local regions, they are translation invariant.',
 'Therefore, each column of the feature maps corresponds to a rectangle region of the original im-  ...ConvolutionalLayers...RecurrentLayers-s-t-aatte""state""TranscriptionLayerInput imageConvolutionalfeature mapsConvolutionalfeature mapsFeaturesequenceDeepbidirectionalLSTMPer-framepredictions(disbritutions)Predictedsequence..................\x0cage (termed the receptive ?eld), and such rectangle regions are in the same order to their corresponding columns on the feature maps from left to right.',
 'As illustrated in Fig.',
 '2, each vector in the feature sequence is associated with a receptive ?eld, and can be considered as the image descriptor for that region.',
 'Figure 2.',
 'The receptive ?eld.',
 'Each vector in the extracted feature sequence is associated with a receptive ?eld on the input image, and can be considered as the feature vector of that ?eld.',
 'Being robust, rich and trainable, deep convolutional fea- tures have been widely adopted for different kinds of vi- sual recognition tasks [25, 12].',
 'Some previous approaches have employed CNN to learn a robust representation for sequence-like objects such as scene text [22].',
 'However, these approaches usually extract holistic representation of the whole image by CNN, then the local deep features are collected for recognizing each component of a sequence- like object.',
 'Since CNN requires the input images to be scaled to a ?xed size in order to satisfy with its ?xed input dimension, it is not appropriate for sequence-like objects due to their large length variation.',
 'In CRNN, we convey deep features into sequential representations in order to be invariant to the length variation of sequence-like objects.',
 '2.2.',
 'Sequence Labeling  A deep bidirectional Recurrent Neural Network is built on the top of the convolutional layers, as the recurrent lay- ers.',
 'The recurrent layers predict a label distribution yt for each frame xt in the feature sequence x = x1, .',
 '.',
 '.',
 ', xT .',
 'The advantages of the recurrent layers are three-fold.',
 'Firstly, RNN has a strong capability of capturing contextual in- formation within a sequence.',
 'Using contextual cues for image-based sequence recognition is more stable and help- ful than treating each symbol independently.',
 'Taking scene text recognition as an example, wide characters may re- quire several successive frames to fully describe (refer to Fig.',
 '2).',
 'Besides, some ambiguous characters are easier to distinguish when observing their contexts, e.g.',
 'it is easier to recognize “il” by contrasting the character heights than by recognizing each of them separately.',
 'Secondly, RNN can back-propagates error differentials to its input, i.e.',
 'the con- volutional layer, allowing us to jointly train the recurrent layers and the convolutional layers in a uni?ed network.',
 'Figure 3.',
 '(a) The structure of a basic LSTM unit.',
 'An LSTM con- sists of a cell module and three gates, namely the input gate, the output gate and the forget gate.',
 '(b) The structure of deep bidirec- tional LSTM we use in our paper.',
 'Combining a forward (left to right) and a backward (right to left) LSTMs results in a bidirec- tional LSTM.',
 'Stacking multiple bidirectional LSTM results in a deep bidirectional LSTM.',
 'Thirdly, RNN is able to operate on sequences of arbitrary lengths, traversing from starts to ends.',
 'A traditional RNN unit has a self-connected hidden layer between its input and output layers.',
 'Each time it receives a frame xt in the sequence, it updates its internal state ht with a non-linear function that takes both current input xt and past state ht?1 as its inputs: ht = g(xt, ht?1).',
 'Then the prediction yt is made based on ht.',
 'In this way, past con- texts {xt(cid:48)}t(cid:48)<t are captured and utilized for prediction.',
 'Tra- ditional RNN unit, however, suffers from the vanishing gra- dient problem [7], which limits the range of context it can store, and adds burden to the training process.',
 'Long-Short Term Memory [18, 11] (LSTM) is a type of RNN unit that is specially designed to address this problem.',
 'An LSTM (il- lustrated in Fig.',
 '3) consists of a memory cell and three mul- tiplicative gates, namely the input, output and forget gates.',
 'Conceptually, the memory cell stores the past contexts, and the input and output gates allow the cell to store contexts for a long period of time.',
 'Meanwhile, the memory in the cell can be cleared by the forget gate.',
 'The special design of LSTM allows it to capture long-range dependencies, which often occur in image-based sequences.',
 'LSTM is directional, it only uses past contexts.',
 'How- ever, in image-based sequences, contexts from both direc- tions are useful and complementary to each other.',
 'There- fore, we follow [17] and combine two LSTMs, one forward and one backward, into a bidirectional LSTM.',
 'Furthermore, multiple bidirectional LSTMs can be stacked, resulting in a deep bidirectional LSTM as illustrated in Fig.',
 '3.b.',
 'The deep structure allows higher level of abstractions than a shallow one, and has achieved signi?cant performance im- provements in the task of speech recognition [17].',
 'In recurrent layers, error differentials are propagated in the opposite directions of the arrows shown in Fig.',
 '3.b,  ...Feature SequenceReceptive fieldCell......ForgetGateOutputGatetanhInputGatetanh(a)..................(b)\x0ci.e.',
 'Back-Propagation Through Time (BPTT).',
 'At the bot- tom of the recurrent layers, the sequence of propagated dif- ferentials are concatenated into maps, inverting the opera- tion of converting feature maps into feature sequences, and fed back to the convolutional layers.',
 'In practice, we create a custom network layer, called “Map-to-Sequence”, as the bridge between convolutional layers and recurrent layers.',
 '2.3.',
 'Transcription  Transcription is the process of converting the per-frame predictions made by RNN into a label sequence.',
 'Mathe- matically, transcription is to ?nd the label sequence with the highest probability conditioned on the per-frame pre- dictions.',
 'In practice, there exists two modes of transcrip- tion, namely the lexicon-free and lexicon-based transcrip- tions.',
 'A lexicon is a set of label sequences that prediction is constraint to, e.g.',
 'a spell checking dictionary.',
 'In lexicon- free mode, predictions are made without any lexicon.',
 'In lexicon-based mode, predictions are made by choosing the label sequence that has the highest probability.',
 '2.3.1 Probability of label sequence  We adopt the conditional probability de?ned in the Con- nectionist Temporal Classi?cation (CTC) layer proposed by Graves et al.',
 '[15].',
 'The probability is de?ned for la- bel sequence l conditioned on the per-frame predictions y = y1, .',
 '.',
 '.',
 ', yT , and it ignores the position where each la- bel in l is located.',
 'Consequently, when we use the negative log-likelihood of this probability as the objective to train the network, we only need images and their corresponding la- bel sequences, avoiding the labor of labeling positions of individual characters.',
 'The formulation of the conditional probability is brie?y described as follows: The input is a sequence y = y1, .',
 '.',
 '.',
 ', yT where T is the sequence length.',
 'Here, each yt ? (cid:60)|L(cid:48)| is a probability distribution over the set L(cid:48) = L ? , where L contains all labels in the task (e.g.',
 'all En- glish characters), as well as a ’blank’ label denoted by .',
 'A sequence-to-sequence mapping function B is de?ned on se- quence ? ? L(cid:48)T , where T is the length.',
 'B maps ? onto l by ?rstly removing the repeated labels, then removing the ’blank’s.',
 'For example, B maps “--hh-e-l-ll-oo--” (’-’ represents ’blank’) onto “hello”.',
 'Then, the condi- tional probability is de?ned as the sum of probabilities of all ? that are mapped by B onto l:  p(l|y) =  p(?|y),  (1)  (cid:81)T  where the probability of ? is de?ned as p(?|y) = ?t is the probability of having label ?t at time stamp t. Directly computing Eq.',
 '1 would be computa- tionally infeasible due to the exponentially large number  ?t, yt  t=1 yt  (cid:88)  ?:B(?)=l  of summation items.',
 'However, Eq.',
 '1 can be ef?ciently computed using the forward-backward algorithm described in [15].',
 '2.3.2 Lexicon-free transcription In this mode, the sequence l? that has the highest proba- bility as de?ned in Eq.',
 '1 is taken as the prediction.',
 'Since there exists no tractable algorithm to precisely ?nd the so- lution, we use the strategy adopted in [15].',
 'The sequence l? is approximately found by l? ? B(arg max? p(?|y)), i.e.',
 'taking the most probable label ?t at each time stamp t, and map the resulted sequence onto l?.',
 '2.3.3 Lexicon-based transcription  In lexicon-based mode, each test sample is associated with a lexicon D. Basically, the label sequence is recognized by choosing the sequence in the lexicon that has high- l? = est conditional probability de?ned in Eq.',
 '1, i.e.',
 'arg maxl?D p(l|y).',
 'However, for large lexicons, e.g.',
 'the 50k-words Hunspell spell-checking dictionary [1], it would be very time-consuming to perform an exhaustive search over the lexicon, i.e.',
 'to compute Equation 1 for all se- quences in the lexicon and choose the one with the high- est probability.',
 'To solve this problem, we observe that the label sequences predicted via lexicon-free transcription, de- scribed in 2.3.2, are often close to the ground-truth under the edit distance metric.',
 'This indicates that we can limit our search to the nearest-neighbor candidates N?(l(cid:48)), where ? is the maximal edit distance and l(cid:48) is the sequence transcribed from y in lexicon-free mode:  p(l|y).',
 'l? = arg max l?N?(l(cid:48))  (2) The candidates N?(l(cid:48)) can be found ef?ciently with the BK-tree data structure [9], which is a metric tree speci?- cally adapted to discrete metric spaces.',
 'The search time complexity of BK-tree is O(log |D|), where |D| is the lex- icon size.',
 'Therefore this scheme readily extends to very large lexicons.',
 'In our approach, a BK-tree is constructed of?ine for a lexicon.',
 'Then we perform fast online search with the tree, by ?nding sequences that have less or equal to ? edit distance to the query sequence.',
 '2.4.',
 'Network Training  Denote the training dataset by X = {Ii, li}i, where Ii is the training image and li is the ground truth label sequence.',
 'The objective is to minimize the negative log-likelihood of conditional probability of ground truth:  log p(li|yi),  (3)  O = ? (cid:88)  Ii,li?X  \x0cwhere yi is the sequence produced by the recurrent and con- volutional layers from Ii.',
 'This objective function calculates a cost value directly from an image and its ground truth label sequence.',
 'Therefore, the network can be end-to-end trained on pairs of images and sequences, eliminating the procedure of manually labeling all individual components in training images.',
 'The network is trained with stochastic gradient descent (SGD).',
 'Gradients are calculated by the back-propagation al- gorithm.',
 'In particular, in the transcription layer, error dif- ferentials are back-propagated with the forward-backward algorithm, as described in [15].',
 'In the recurrent layers, the Back-Propagation Through Time (BPTT) is applied to cal- culate the error differentials.',
 'For optimization, we use the ADADELTA [37] to au- tomatically calculate per-dimension learning rates.',
 'Com- pared with the conventional momentum [31] method, ADADELTA requires no manual setting of a learning rate.',
 'More importantly, we ?nd that optimization using ADADELTA converges faster than the momentum method.',
 '3.',
 'Experiments  To evaluate the effectiveness of the proposed CRNN model, we conducted experiments on standard benchmarks for scene text recognition and musical score recognition, which are both challenging vision tasks.',
 'The datasets and setting for training and testing are given in Sec.',
 '3.1, the de- tailed settings of CRNN for scene text images is provided in Sec.',
 '3.2, and the results with the comprehensive compar- isons are reported in Sec.',
 '3.3.',
 'To further demonstrate the generality of CRNN, we verify the proposed algorithm on a music score recognition task in Sec.',
 '3.4.',
 '3.1.',
 'Datasets  For all the experiments for scene text recognition, we use the synthetic dataset (Synth) released by Jaderberg et al.',
 '[20] as the training data.',
 'The dataset contains 8 millions training images and their corresponding ground truth words.',
 'Such images are generated by a synthetic text engine and are highly realistic.',
 'Our network is trained on the synthetic data once, and tested on all other real-world test datasets without any ?ne-tuning on their training data.',
 'Even though the CRNN model is purely trained with synthetic text data, it works well on real images from standard text recognition benchmarks.',
 'Four popular benchmarks for scene text recognition are used for performance evaluation, namely ICDAR 2003 (IC03), ICDAR 2013 (IC13), IIIT 5k-word (IIIT5k), and Street View Text (SVT).',
 'IC03 [27] test dataset contains 251 scene images with la- beled text bounding boxes.',
 'Following Wang et al.',
 '[34], we ignore images that either contain non-alphanumeric charac- ters or have less than three characters, and get a test set with  Table 1.',
 'Network con?guration summary.',
 'The ?rst row is the top layer.',
 '‘k’, ‘s’ and ‘p’ stand for kernel size, stride and padding size respectively  Type  Con?gurations  Transcription  Bidirectional-LSTM Bidirectional-LSTM Map-to-Sequence  Convolution MaxPooling  BatchNormalization  Convolution  BatchNormalization  Convolution MaxPooling Convolution Convolution MaxPooling Convolution MaxPooling Convolution  Input  -  #hidden units:256 #hidden units:256  -  #maps:512, k:2 × 2, s:1, p:0  Window:1 × 2, s:2  -  -  Window:1 × 2, s:2  #maps:512, k:3 × 3, s:1, p:1 #maps:512, k:3 × 3, s:1, p:1 #maps:256, k:3 × 3, s:1, p:1 #maps:256, k:3 × 3, s:1, p:1 #maps:128, k:3 × 3, s:1, p:1 #maps:64, k:3 × 3, s:1, p:1 W × 32 gray-scale image  Window:2 × 2, s:2 Window:2 × 2, s:2  860 cropped text images.',
 'Each test image is associated with a 50-words lexicon which is de?ned by Wang et al.',
 '[34].',
 'A full lexicon is built by combining all the per-image lexi- cons.',
 'In addition, we use a 50k words lexicon consisting of the words in the Hunspell spell-checking dictionary [1].',
 'IC13 [24] test dataset inherits most of its data from IC03.',
 'It contains 1,015 ground truths cropped word images.',
 'IIIT5k [28] contains 3,000 cropped word test images collected from the Internet.',
 'Each image has been associ- ated to a 50-words lexicon and a 1k-words lexicon.',
 'SVT [34] test dataset consists of 249 street view images collected from Google Street View.',
 'From them 647 word images are cropped.',
 'Each word image has a 50 words lexi- con de?ned by Wang et al.',
 '[34].',
 '3.2.',
 'Implementation Details  The network con?guration we use in our experiments is summarized in Table 1.',
 'The architecture of the con- volutional layers is based on the VGG-VeryDeep architec- tures [32].',
 'A tweak is made in order to make it suitable for recognizing English texts.',
 'In the 3rd and the 4th max- pooling layers, we adopt 1 × 2 sized rectangular pooling windows instead of the conventional squared ones.',
 'This tweak yields feature maps with larger width, hence longer feature sequence.',
 'For example, an image containing 10 characters is typically of size 100×32, from which a feature sequence 25 frames can be generated.',
 'This length exceeds the lengths of most English words.',
 'On top of that, the rect- angular pooling windows yield rectangular receptive ?elds (illustrated in Fig.',
 '2), which are bene?cial for recognizing some characters that have narrow shapes, such as ’i’ and ’l’.',
 'The network not only has deep convolutional layers, but also has recurrent layers.',
 'Both are known to be hard to  \x0ctrain.',
 'We ?nd that the batch normalization [19] technique is extremely useful for training network of such depth.',
 'Two batch normalization layers are inserted after the 5th and 6th convolutional layers respectively.',
 'With the batch normal- ization layers, the training process is greatly accelerated.',
 'We implement the network within the Torch7 [10] frame- work, with custom implementations for the LSTM units (in Torch7/CUDA), the transcription layer (in C++) and the BK-tree data structure (in C++).',
 'Experiments are carried out on a workstation with a 2.50 GHz Intel(R) Xeon(R) E5- 2609 CPU, 64GB RAM and an NVIDIA(R) Tesla(TM) K40 GPU.',
 'Networks are trained with ADADELTA, setting the parameter ? to 0.9.',
 'During training, all images are scaled to 100 × 32 in order to accelerate the training process.',
 'The training process takes about 50 hours to reach convergence.',
 'Testing images are scaled to have height 32.',
 'Widths are proportionally scaled with heights, but at least 100 pixels.',
 'The average testing time is 0.16s/sample, as measured on IC03 without a lexicon.',
 'The approximate lexicon search is applied to the 50k lexicon of IC03, with the parameter ? set to 3.',
 'Testing each sample takes 0.53s on average.',
 '3.3.',
 'Comparative Evaluation  All the recognition accuracies on the above four public datasets, obtained by the proposed CRNN model and the recent state-of-the-arts techniques including the approaches based on deep models [23, 22, 21], are shown in Table 2.',
 'In the constrained lexicon cases, our method consistently outperforms most state-of-the-arts approaches, and in aver- age beats the best text reader proposed in [22].',
 'Speci?cally, we obtain superior performance on IIIT5k, and SVT com- pared to [22], only achieved lower performance on IC03 with the “Full” lexicon.',
 'Note that the model in[22] is trained on a speci?c dictionary, namely that each word is associated to a class label.',
 'Unlike [22], CRNN is not lim- ited to recognize a word in a known dictionary, and able to handle random strings (e.g.',
 'telephone numbers), sentences or other scripts like Chinese words.',
 'Therefore, the results of CRNN are competitive on all the testing datasets.',
 'In the unconstrained lexicon cases, our method achieves the best performance on SVT, yet, is still behind some ap- proaches [8, 22] on IC03 and IC13.',
 'Note that the blanks in the “none” columns of Table 2 denote that such ap- proaches are unable to be applied to recognition without lexicon or did not report the recognition accuracies in the unconstrained cases.',
 'Our method uses only synthetic text with word level labels as the training data, very different to PhotoOCR [8] which used 7.9 millions of real word images with character-level annotations for training.',
 'The best per- formance is reported by [22] in the unconstrained lexicon cases, bene?ting from its large dictionary, however, it is not a model strictly unconstrained to a lexicon as mentioned be- fore.',
 'In this sense, our results in the unconstrained lexicon  Table 3.',
 'Comparison among various methods.',
 'Attributes for com- parison include: 1) being end-to-end trainable (E2E Train); 2) using convolutional features that are directly learned from im- ages rather than using hand-crafted ones (Conv Ftrs); 3) requir- ing no ground truth bounding boxes for characters during training (CharGT-Free); 4) not con?ned to a pre-de?ned dictionary (Un- constrained); 5) the model size (if an end-to-end trainable model is used), measured by the number of model parameters (Model Size, M stands for millions).',
 'e e r F - T G r a h C  d e n i a r t s n o c n U  n i a r T E 2 E  s r t F v n o C  e z i  S  l e d o M - - - - - - - - - - - -  Wang et al.',
 '[34] Mishra et al.',
 '[28] Wang et al.',
 '[35] Goel et al.',
 '[13] Bissacco et al.',
 '[8] Alsharif and Pineau [6] Almaz´an et al.',
 '[5] Yao et al.',
 '[36] Rodrguez-Serrano et al.',
 '[30] Jaderberg et al.',
 '[23] Su and Lu [33] Gordo [14] Jaderberg et al.',
 '[22] Jaderberg et al.',
 '[21] CRNN  \x17 \x17  \x17 \x17 \x14 \x17 \x17 \x17 \x17 \x14 \x17 \x14 \x17 \x14 \x17 \x17 \x17 \x17 \x17 \x14 \x17 \x14 \x17 \x14 \x17 \x14 \x17 \x17 \x17 \x17 \x17 \x14 \x17 \x17 \x14 \x17 \x17 \x14 \x17 \x14 \x17 \x14 \x14 \x17 \x17 \x17 \x17 490M \x14 \x14 \x14 \x17 \x14 \x14 \x14 \x14 304M \x14 \x14 \x14 \x14 8.3M  \x17  case are still promising.',
 'For further understanding the advantages of the pro- posed algorithm over other text recognition approaches, we provide a comprehensive comparison on several properties named E2E Train, Conv Ftrs, CharGT-Free, Unconstrained, and Model Size, as summarized in Table 3.',
 'E2E Train: This column is to show whether a certain text reading model is end-to-end trainable, without any pre- process or through several separated steps, which indicates such approaches are elegant and clean for training.',
 'As can be observed from Table 3, only the models based on deep neural networks including [22, 21] as well as CRNN have this property.',
 'Conv Ftrs: This column is to indicate whether an ap- proach uses the convolutional features learned from training images directly or handcraft features as the basic represen- tations.',
 'CharGT-Free: This column is to indicate whether the character-level annotations are essential for training the model.',
 'As the input and output labels of CRNN can be a sequence, character-level annotations are not necessary.',
 'Unconstrained: This column is to indicate whether the trained model is constrained to a speci?c dictionary, unable to handling out-of-dictionary words or random sequences.',
 'Table 2.',
 'Recognition accuracies (%) on four datasets.',
 'In the second row, “50”, “1k”, “50k” and “Full” denote the lexicon used, and “None” denotes recognition without a lexicon.',
 '(*[22] is not lexicon-free in the strict sense, as its outputs are constrained to a 90k dictionary.',
 'ABBYY [34] Wang et al.',
 '[34] Mishra et al.',
 '[28] Wang et al.',
 '[35] Goel et al.',
 '[13] Bissacco et al.',
 '[8] Alsharif and Pineau [6] Almaz´an et al.',
 '[5] Yao et al.',
 '[36] Rodrguez-Serrano et al.',
 '[30] Jaderberg et al.',
 '[23] Su and Lu [33] Gordo [14] Jaderberg et al.',
 '[22] Jaderberg et al.',
 '[21] CRNN  IIIT5k  50 24.3  -  1k - -  64.1  57.5  - - - -  91.2 80.2 76.1  - -  93.3 97.1 95.5 97.6  - - - -  82.1 69.3 57.4  - -  86.6 92.7 89.6 94.4  None  - - - - - - - - - - - - - - -  78.2  SVT  None  - - - - -  78.0  - - - - - - -  80.7* 71.7 80.8  50 35.0 57.0 73.2 70.0 77.3 90.4 74.3 89.2 75.9 70.0 86.1 83.0 91.8 95.4 93.2 96.4  IC03  50 56.0 76.0 81.8 90.0 89.7  -  Full 55.0 62.0 67.8 84.0  - -  50k - - - - - -  93.1  88.6  85.1  - - - - - -  -  -  88.5  80.3  -  96.2 92.0  -  98.7 97.8 98.7  -  91.5 82.0  -  98.6 97.0 97.6  None  - - - - - - - - - - - - -  IC13 None  - - - - -  87.6  - - - - - - -  93.3 93.4 95.5  93.1* 89.6 89.4  90.8* 81.8 86.7  Notice that though the recent models learned by label em- bedding [5, 14] and incremental learning [22] achieved highly competitive performance, they are constrained to a speci?c dictionary.',
 'Model Size: This column is to report the storage space of the learned model.',
 'In CRNN, all layers have weight- sharing connections, and the fully-connected layers are not needed.',
 'Consequently, th"
4_B1,"
['8 1 0 2     y a M   6 2      ]  .',
 'M B o i b - q [      2 v 2 4 3 0 1  .',
 '3 0 8 1 : v i X r a  Classification of crystallization outcomes using deep convolutional neural networks  Center for Computational Research, University at Buffalo, Buffalo, New York, United States of America.',
 'Andrew E. Bruno  Department of Chemistry, Duke University, Durham, North Carolina, USA and  Department of Physics, Duke University, Durham, North Carolina, USA  Patrick Charbonneau  Collaborative Crystallisation Centre CSIRO, Parkville, Victoria, Australia.',
 'Janet Newman  Edward H. Snell  Hauptman-Woodward Medical Research Institute and SUNY Buffalo,  Department of Materials, Design, and Innovation,  Buffalo, New York 14203, United States of America.',
 'Google Brain, Google Inc., Mountain View, California, United States of America.',
 'David R. So and Vincent Vanhoucke  IM&T Scientific Computing, CSIRO, Clayton South, Victoria, Australia  Christopher J. Watkins  Shawn Williams  Platform Technology and Sciences, GlaxoSmithKline Inc.,  Collegeville, Pennsylvania, United States of America.',
 'Department of Mathematics, University of York, York, United Kingdom.',
 '(Dated: May 29, 2018)  Julie Wilson  The Machine Recognition of Crystallization Outcomes (MARCO) initiative has assembled roughly half a million annotated images of macromolecular crystallization experiments from various sources and setups.',
 'Here, state-of-the-art machine learning algorithms are trained and tested on different parts of this data set.',
 'We find that more than 94% of the test images can be correctly labeled, irrespective of their experimental origin.',
 'Because crystal recognition is key to high-density screening and the systematic analysis of crystallization experiments, this approach opens the door to both industrial and fundamental research applications.',
 'Author summary: Protein crystal growth experiments are routinely imaged, but the mass of accumulated data is difficult to manage and analyze.',
 'Using state-of-the-art machine learning algorithms on a large and diverse set of reference images, we manage to recapitulate the labels of a remarkably large fraction of the set.',
 'This automation should enable a number of industrial and fundamental applications.',
 'I.',
 'INTRODUCTION  X-ray crystallography provides the atomic structure of molecules and molecular complexes.',
 'These structures in turn provide insight into the molecular driving forces for small molecule binding, protein-protein interactions, supramolecular assembly and other biomolecular pro- cesses.',
 'The technique is thus foundational to molecular modeling and design.',
 'Beyond the obvious importance of structure information for understanding and altering the role of biomolecules, it also has important industrial applications.',
 'The pharmaceutical industry, for instance, uses structures to guide chemistry as part of a “predict first” strategy [1], employing expert systems to reduce op- timization cycle times and more effectively bring medicine  to patients.',
 'Yet, despite decades of methodological ad- vances, crystallizing molecular targets of interest remains the bottleneck of the entire crystallography program in structural biology.',
 'Even when crystallization is facile, it is microscopically rare; for macromolecules it is also uncommon [2–5].',
 'Ex- perimental trials typically involve: (i) mixing a purified sample with chemical cocktails designed to promote molec- ular association, (ii) generating a supersaturated solution of the desired molecule via evaporation or equilibration, and (iii) visually monitoring the outcomes, before (iv) optimizing those conditions and analyzing the resultant crystal with an X-ray beam.',
 'One hopes for the formation of a crystal instead of non-specific (amorphous) precipi- tates or of nothing at all.',
 'In order to help run these trials,  \x0ccommercial crystallization screens have been developed; each screen generally contains 96 formulations designed to promote crystal growth.',
 'Whether these screens are equally effective or not [5, 6] remains debated, but their overall yield is in any case paltry.',
 'Typically fewer than 5% of crystallization attempts produce useful results (with a success rate as low as 0.2% in some contexts [7]).',
 'The practical solution to this hurdle has been to in- crease the convenience and number of crystallization tri- als.',
 'To offset the expense of reagents and scientist time, labs routinely employ industrial robotic liquid handlers, nanoliter-size drops, and record trial outcomes using auto- mated imaging systems [5, 8–11].',
 'Hoping to compensate for the rarity of crystallization, commercially available systems readily probe a large area of chemical space with minimal sample volume with a throughput of ? 1000 individual experiments per hour.',
 'While liquid handling is readily automated, crystal recognition is not.',
 'Imaging systems may have made view- ing results more comfortable than bending over a micro- scope, but crystallographers still manually inspect images and/or drops, looking for crystals or, more commonly, conditions that are likely to produce good crystals when optimized.',
 'This human cost makes crystal recognition a key experimental bottleneck within the larger challenge of crystallizing biomolecules [7].',
 'A typical experiment for a given sample includes four 96-well screens at two temperatures, i.e., 768 conditions (and can have up to twice that [12]).',
 'Assuming that it takes 2 seconds to man- ually scan a droplet (and noting that the scans have to be repeated, as crystallization is time dependent), simply looking at a single set of 96 trials over the lifetime of an experiment can take the better part of an hour [13].',
 'For the sake of illustration, the U.S.',
 'Structural Science group at GlaxoSmithKline performs ? 1200 96-well ex- periments per year.',
 'If the targeted observation schedule were rigorously followed, the group would spend a quarter of the year staring at drops, of which the vast majority contains no crystal.',
 'Recording outcomes and analyzing the results of the 96 trials would further increase the time burden.',
 'Current operations are already straining existing resources, and the approach simply does not scale for proposed higher-density screening [10].',
 'Crystal growth is also sufficiently uncommon that the tolerance for false negatives is almost nil.',
 'Yet most crystal- lographers are misguided in thinking that they themselves would never miss identifying a crystal given an image con- taining an crystal, or indeed miss a crystal in a droplet viewed directly under a microscope [14].',
 'In fact, not only do crystallographers miss crystals due to lack of atten- tion through boredom, they often disagree on the class an image should be assigned to.',
 'An overall agreement rate of ? 70% was found when the classes assigned to 1200 images by 16 crystallographers were compared [14].',
 '(When considering only crystalline outcomes, agreement rose to ? 93%.)',
 'Consistency in visual scoring was also considered by Snell et al.',
 'when compiling a ? 150, 000 image dataset [15].',
 'They found that viewers give different  2  scores to the same image on different occasions during the study, with the average agreement rate for scores on a control set at the beginning and middle of the study being 77%, rising to 84% for the agreement in scores be- tween the middle and end of the study.',
 'Crystallographers also tend to be optimistically biased when scoring their own experiments [16].',
 'A better use of expert time and attention would be to focus on scientific inquiry.',
 'An algorithm that could analyze images of drops, dis- tinguish crystals from trivial outcomes, and reduce the effort spent cataloging failure, would present clear value both to the discipline and to industry.',
 'Ideally, such an algorithm would act like an experienced crystallographer in:  • recognizing macromolecular crystals appropriate for  diffraction experiments;  • recognizing outcomes that, while requiring optimiza- tion, would lead to crystals for diffraction experi- ments;  • recognizing non-macromolecular crystals; • ignoring technical failures; • identifying non-crystalline outcomes that require  follow up;  • being agnostic as to the imaging platform used; • being indefatigable and unbiased; • occurring in a time frame that does not impede the  process;  • learning from experience.',
 'Such an algorithm would further reduce the variance in the assessments, irrespective of its accuracy.',
 'A high-variance, manual process is not conducive to automating the quality control of the system end-to-end, including the imaging equipment.',
 'Enhanced reproducibility enables traceability of the outcomes, and paves the way for putting in place measurable, continuous improvement processes across the entire imaging chain.',
 'Automated crystallization image classifications that attempt to meet the above criteria have been previously attempted.',
 'The research laboratories that first automated crystallization inspection quickly realized that image anal- ysis would be a huge problem, and concomitantly devel- oped algorithms to interpret them [17–20].',
 'None of these programs was ever widely adopted.',
 'This may have been due in part to their dependence on a particular imag- ing system, and to the relatively limited use of imaging systems at the time.',
 'Many of the early image analysis programs further required very time consuming collation of features and significant preprocessing, e.g., drop seg- mentation to locate the experimental droplet within the image.',
 'To the best of our knowledge, there was also no widespread effort to make a widely available image anal- ysis package in the same way that that the diffraction  \x0coriented programs have been organized, e.g., the CCP4 package [21].',
 'Can a better algorithm be constructed and trained?',
 'In order to help answer this question, the Machine Recog- nition of Crystallization Outcomes (MARCO) initiative was set up [22].',
 'MARCO assembled a set of roughly half a million classified images of crystallization trials through an international collaboration with five separate institutions.',
 'Here, we present a machine-learning based approach to categorize these images.',
 'Remarkably, the algorithm we employ manages to obtain an accuracy ex- ceeding 94%, which is even above what was once thought possible for human categorization.',
 'This suggests that a deployment of this technology in a variety of laboratory settings is now conceivable.',
 'The rest of this paper is as follows.',
 'Section II describes the dataset and the scoring scheme, Sec.',
 'III describes the machine-learning model and training procedure, Secs.',
 'IV and V describe and discuss the results, respectively, and Sec.',
 'VI briefly concludes.',
 'II.',
 'MATERIAL AND METHODS  Image Data  The MARCO data set used in this study contains 493,214 scored images from five institutions (See Ta- ble I [22]).',
 'The images were collected from imagers made from two different manufacturers (Rigaku Automa- tion and Formulatrix), which have different optical sys- tems, as well as by the in-house imaging equipment built at the Hauptman-Woodward Medical Research Insti- tute (HWMRI) High-Throughput Crystallization Center (HTCC).',
 'Different versions of the setups were also used – some Rigaku images are collected with a true color camera, some are collected as greyscale images.',
 'The zoom extent varies, with some imagers set up to collect a field-of-view (FOV) of only the experimental droplet, and some set for the FOV to encompass a larger area of the experimental setup.',
 'The Rigaku and Formulatrix automation imaged vapor diffusion based experiments while the HTCC sys- tem imaged microbatch-under-oil experiments.',
 'A random selection of 50,284 test images was held out for validation.',
 'Images in the test set were not represented in the training set.',
 'The precise data split is available from the MARCO website [22].',
 '3  images not obviously falling into the three major classes, and came to assume a functional significance as the classi- fication process was further investigated.',
 'Examination of the least classifiable five percent of images indeed revealed many instances of process failure, such as dispensing er- rors or illumination problems.',
 'These uninterpretable images were then labelled as “other” during the rescoring, which added an element of quality control to the overall process [25].',
 'Relabeling  After a first baseline system was trained (see Sec.',
 'III), the 5% of the images that were most in disagreement with the classifier (independently of whether the image was in the training or the test set), were relabeled by one expert, in order to obtain a systematic eye on the most problematic images.',
 'Because no rules were established and no exemplars were circulated prior to the initial scoring, individual viewpoints varied on classifying certain outcomes.',
 'For example, the bottom 5% contained many instances of phase separation, where the protein forms oil droplets or an oily film that coats the bottom of the crystallization well.',
 'Images were found to be inconsistently scored as “clear”, “precipitate”, or “other” depending on the amount and visibility of the oil film.',
 'This example highlights the difficulty of scoring experimental outcomes beyond crystal identification.',
 'A more serious source of ambiguity arises from process failure.',
 'Many of the problematic images did not capture experimental results at all.',
 'They were out of focus, dark, overexposed, dropless, etc.',
 'Whatever labeling convention was initially followed, for the relabeling the “other” category was deemed to also diagnose problems with the imaging process.',
 'A total of 42.6% of annotations for the images that were revisited disagreed with the original label, suggesting somewhat high (1 to 2%) label noise in this difficult fraction of the dataset.',
 'For a fraction of this data, multiple raters were asked to label the images independently and had an inter-rater disagreement rate of approximately 22%.',
 'The inherent difficulty of assigning a label to a small fraction of the images is therefore consistent with the results of Ref.',
 '[14].',
 'Table II shows the final image counts after relabeling.',
 'Labeling  III.',
 'MACHINE LEARNING MODEL  Images were scored by one or more crystallographers.',
 'As the dataset is composed of archival data, no common scoring system was imposed, nor were exemplar images distributed to the various contributors.',
 'Instead, existing scores were collapsed into four comprehensive and fairly robust categories: clear, precipitate, crystal, and other.',
 'This last category was originally used as a catchall for  The goal of the classifier here is to take an image as an input, and output the probability of it belonging to each of four classes (crystals, precipitate, clear, other) (see Fig.',
 '1).',
 'The classifier used is a deep Convolutional Neural Network (CNN).',
 'CNNs, originally proposed in Ref.',
 '[26], and their modern ‘deep’ variants (see, e.g., Refs.',
 '[27, 28] for recent reviews), have proven to consistently provide reliable results on a broad variety of visual recognition  \x0cTABLE I. Breakdown of data sources and imaging technology per institution contributing to MARCO.',
 'Institution  Technical Setup  # of Images  Bristol-Myers Squibb  Formulatrix Rock Imager (FRI)  CSIRO HWMRI  GlaxoSmithKline  Merck  Sitting drop, FRI, Rigaku Minstrel [23, 24]  Under oil, Home system [15]  Sitting drop, FRI Sitting drop, FRI  8719 15933 79632 83126 305804  4  TABLE II.',
 'Data distribution.',
 'Final number of images in the dataset for each category after collapsing the labels and relabeling.',
 'Label  Crystals  Number of images Training Validation 56,672 Precipitate 212,541 148,861 24,856  6632 23,892 16760 3,000  Clear Other  tasks, and are particularly amenable to addressing data- rich problems.',
 'They have been, for instance, state of the art on the very competitive ILSVRC image recognition challenge [29] since 2012.',
 'This approach to visual perception has been making un- precedented inroads in areas such as medical imaging [30] and computational biology [31], and have also shown to be human-competitive on a variety of specialized visual identification [32, 33].',
 'The chosen classifier is thus well suited for the current analysis.',
 'Model Architecture  The model is a variation on the widely-used Inception- v3 architecture [36], which was state of the art on the ILSVRC challenge around 2015.',
 'Several more recent al- ternatives were tried, including Inception-ResNet-v2 [37], and automatically generated variants of NASNet [38], but none yielded any significant improvements.',
 'An ex- tensive hyperparameter search was also conducted using Vizier [39], also without providing significant improvement over the baseline.',
 'The Inception-v3 architecture is a complex deep CNN architecture described in detail in Ref.',
 '[36] as well as the reference implementation [40].',
 'We only describe here the modifications made to tailor the model to the task at hand.',
 'Standard Inception-v3 operates on a 299x299 square im- age.',
 'Because the current problem involves very detailed, thin structures, it is plausible to assume that a larger input image may yield better outcomes.',
 'We use instead 599x599 images, and compress them down to 299x299 using an additional convolutional layer at the very bottom of the network, before the layer labeled Conv2d 1a 3x3 in the reference implementation.',
 'The additional convo- lutional layer has a depth (number of filters) of 16, a 3 × 3 receptive field (it operates on a 3 × 3 square patch convolved over the image) and a stride of 2 (it skips over  Fig 1.',
 'Conceptual Representation of a Convolutional Neural Network.',
 'A CNN is a stack of nonlinear filters (three filter levels are depicted here) that progressively reduce the spatial extent of the image, while increasing the number of filter outputs that describe the image at every location.',
 'On top of this stack sits a multinomial logistic regression classifier, which maps the representation to one probability value per output class (Crystals vs.',
 'Precipitate vs. Clear vs. Others).',
 'The entire network is jointly optimized through backpropagation [34], in general by means of a variant of stochastic gradient descent [35].',
 'every other location in the image to reduce the dimen- sionality of the feature map).',
 'This modification improved classification absolute accuracy by approximately 0.3%.',
 'A few other convolutional layers were shrunk compared to the standard Inception-v3 by capping their depth as described in Table III, using the conventions from the  \x0creference implementation.',
 'TABLE III.',
 'Limits applied to layer depths to reduce the model complexity.',
 'In each named layer of the deep network – here named after the conventions of the reference implementation – every convolutional subblock had its number of filters reduced to contain no more than these many outputs.',
 'Layer  Max depth  Conv2d 4a 3x3  Mixed 6b Mixed 6c Mixed 6d Mixed 6e Mixed 7a Mixed 7b Mixed 7c  144 128 144 144 96 96 192 192  While these parameters are exhaustively reported here to ensure reproducibility of the results, their fine tuning is not essential to maximizing the success rate, and was mainly motivated by improving the speed of training.',
 'In the end, it was possible to train the model at larger batch size (64 instead of 32) and still fit within the memory of a NVidia K80 GPU (see more details in the training section below).',
 'Given the large number of examples available, all dropout [41] regularizers were removed from the model definition at no cost in performance.',
 'Data Preprocessing and Augmentation  The source data is partitioned randomly into 415990  training images and 47062 test images.',
 'The training data is generated dynamically by taking random 599x599 patches of the input images, and sub- jecting them to a wide array of photometric distortions, identical to the reference implementation:  • randomized brightness (± 32 out of 255), • randomized saturation (from 50% to 150%), • randomized hue (± 0.2 out of 0.5), • randomized contrast (from 50% to 150%).',
 'In addition, images are randomly flipped left to right with 50% probability, and, in contrast to the usual practice for natural scenes which don’t have a vertical symmetry, they are also flipped upside down with 50% probability.',
 'Because images in this dataset have full rotational invari- ance, one could also consider rotations beyond the mere 90?, 180?, 270? that these flips provide, but we didn’t attempt it here, as we surmise the incremental benefits would likely be minimal for the additional computational cost.',
 'This form of aggressive data augmentation greatly improves the robustness of image classifiers, and partly alleviates the need for large quantities of human labels.',
 'For evaluation, no distortion is applied.',
 'The test images  are center cropped and resized to 599x599.',
 '5  Fig 2.',
 'Classifier Accuracy.',
 'Accuracy on the training and validation sets as a function of the number of steps of training.',
 'Training halts when the performance on the evaluation set no longer increases (‘early stopping’).',
 'As is typical for this type of stochastic training, performance increases rapidly at first as large training steps are taken, and slows down as the learning rate is annealed and the model fine-tunes its weights.',
 'Training  The model is implemented in TensorFlow [42], and trained using an asynchronous distributed training setup [43] across 50 NVidia K80 GPUs.',
 'The optimizer is RmsProp [44], with a batch size of 64, a learning rate of 0.045, a momentum of 0.9, a decay of 0.9 and an epsilon of 0.1.',
 'The learning rate is decayed every two epochs by a factor of 0.94.',
 'Training completed after 1.7M steps (Fig.',
 '2) in approximately 19 hours, having processed 100M images, which is the equivalent of 260 epochs.',
 'The model thus sees every training sample 260 times on average, with a different crop and set of distortions applied each time.',
 'The model used at test time is a running average of the training model over a short window to help stabilize the predictions.',
 'IV.',
 'RESULTS  Classification  The original labeling gave rise to a model with 94.2% accuracy on the test set.',
 'Relabeling improved reported classification accuracy by approximately 0.3% absolute, with the caveat that the figures are not precisely compara- ble since some of the test labels changed in between.',
 'The revised model thus achieves 94.5% accuracy on the test set for the four-way classification task.',
 'It overfits mod- estly to the training set, reaching just above 97% at the early-stopping mark of 1.7M steps.',
 'Table IV summarizes the confusions between classes.',
 'Although the classifier does not perform equally well on images from the various datasets, the standard deviation in performance from one  \x0cTABLE IV.',
 'Confusion Matrix.',
 'Fraction of the test data that is assigned to each class based on the posterior probability assigned by the classifier.',
 'For instance, 0.8% of images labeled as Precipitate in the test set were classified as Crystals.',
 'True Label  Crystals  Precipitate  Clear Other  Predictions  Crystals Precipitate Clear Other 5.8% 91.0% 1.7% 1.5% 96.1% 2.3% 0.7% 0.8% 97.9% 0.2% 1.8% 0.2% 4.8% 19.7% 5.9% 69.6%  TABLE V. Standard Deviation of the predictions across data sources.',
 'Note in particular the large variability in the consistency of the label ’Other’ across datasets, which leads to comparatively poor selectivity of that less well-defined class.',
 'True Label  Crystals  Precipitate  Clear Other  Predictions  Crystals Precipitate Clear Other 1% 1% 1% 2% 5% 1% 6% 21%  4% 4% 3% 15%  5% 2% 1% 7%  set to another is fairly small, about 5% (see Table V), compared to the overall performance of the classifier.',
 'The classifier outputs a posterior probability for each class.',
 'By varying the acceptance threshold for a proposed classification, one can trade precision of the classifica- tion against recall.',
 'The receiver operating characteristic (ROC) curves can be seen in Fig.',
 '3.',
 'Validation  6  TABLE VI.',
 'Validation at C3 Precision, recall and accuracy from an independent set of images collected after the MARCO tool was developed.',
 'The 38K images of sitting drop trials were collected between January 1 and March 30, 2018 on two Formulatrix Rock Imager (FRI) instruments.',
 'DL tool  Precision Recall Accuracy  DeepCrystal  MARCO  0.4928 0.7777  0.4520 0.8663  0.7391 0.9018  Pixel Attribution  We visually inspect to what parts of the image the classifier learns to attend by aggregating noisy gradients of the image with respect to its label on a per-pixel basis.',
 'The SmoothGrad [47] approach is used to visualize the focus of the classifier.',
 'The images in Fig.',
 '4 are constructed by overlaying a heat map of the classifier’s attention over a grayscale version of the input image.',
 'Note that saliency methods are imperfect and do not in general weigh faithfully all the evidence present in an image according to their contributions to the decision, es- pecially when the evidence is highly correlated.',
 'Although these visualizations paint a simplified and very partial picture of the classifier’s decision mechanisms, they help confirm that it is likely not picking up and overfitting to cues that are irrelevant to the task.',
 'Inference and Availability  The model is open-sourced and available online at [48].',
 'It can be run locally using TensorFlow or TensorFlow Lite, or as a Google Cloud Machine Learning [49] endpoint.',
 'At time of writing, inference on a standard Cloud instance takes approximately 260ms end-to-end per standalone query.',
 'However, due to the very efficient parallelism properties of convolutional networks, latency per image can be dramatically cut down for batch requests.',
 'At CSIRO C3 a workflow [45] has been set up which uses a variation of the analysis tool from DeepCrystal [46] to analyze newly collected crystallisation images and to assign either no score, ‘crystal score or ‘clear score.',
 'A total of 37,851 images were collected in Q1 2018 and assigned a human score by a C3 user were used as an independent dataset to test the MARCO tool.',
 'Within this dataset, 9746 images had been identified as containing crystals.',
 'The current, DeepCrystal tool (which assigns only crystal or clear scores) was found to have an overall accuracy rate of 74%, while the MARCO tool has 90%.',
 'Although this retrospective analysis doesnt allow for a direct comparison of the ROC, the precision, recall and accuracy of the two tools all favor the MARCO tool, as shown in table 6.',
 'The precision achieved by MARCO on this dataset is also very similar to that seen for the CSIRO images in the training data.',
 'V. DISCUSSION  Previous attempts at automating the analysis of crys- tallisation images have employed various pattern recog- nition and machine learning techniques, including linear discriminant analysis [50, 51], decision trees and random forests [52–54], and support vector machines [20, 55].',
 'Neu- ral networks, including self-organizing maps, have also been used classify these images [17, 56], with the most recent involving deep learning [57].',
 'However, all previ- ous approaches have required a consistent set of images with the same field of view and resolution, in order to identify the crystallization droplet in the well [23], and thereby restrict the analysis.',
 'Various statistical, geomet- ric or textural features were then extracted, either directly from the image or from some transformation of the region  \x0c7  (a) Fig 3.',
 'Receiver Operating Characteristic Curves.',
 '(Q) Percentage of the correctly accepted detection of crystals as a function of the percentage of incorrect detections (AUC: 98.8).',
 '98.7% of the crystal images can be recalled at the cost of less than 19% false positives.',
 'Alternatively, 94% of the crystals can be retrieved with less than 1.6% false positives.',
 '(B) Percentage of the correctly accepted detection of precipitate as a function of the percentage of incorrect detections (AUC: 98.9).',
 '99.6% of the precipitate images can be recalled at the cost of less than 25% false positives.',
 'Alternatively, 94% of the precipitates can be retrieved with less than 3.4% false positives.',
 '(b)  (b)  (a) Fig 4.',
 'Sample heatmaps for various types of images.',
 '(A) Crystal: the classifier focuses on some of the angular geometric features of individual crystals (arrows).',
 '(B) Precipitate: the classifier lands on the precipitate (arrows).',
 '(C) Clear: The classifier broadly samples the image, likely because this label is characterized by the absence of structures rather than their presence.',
 'Note the slightly more pronounced focus on some darker areas (circle and arrows) that could be confused for crystals or precipitate.',
 'Because the ‘Others’ class is defined negatively by the the image being not identifiable as belonging to the other three classes, heatmaps for images of that class are not particularly informative.',
 '(c)  of interest, to be used as variables in the classification algorithms.',
 'The results from various studies can be difficult to compare head-to-head because different groups present confusion matrices with the number of classes ranging from 2 to 11, only sometimes aggregating results for crystals/crytalline materials.',
 'There is also a tradeoff between the number of false negatives and the number of false positives.',
 'Yet most report classification rates for crystals around 80-85% even in more recent work [8,  54, 58], in which missed crystals are reported with much lower rates.',
 'This advance comes at the expense of more false positives.',
 'For example, Pan et al.',
 'report just under 3% false negatives, but almost 38% false positives [55].',
 'As the trained algorithms are specific to a set of im- ages, they are also restricted to a particular type of crys- tallisation experiment.',
 'Prior to the curation of the cur- rent dataset, the largest set of images (by far) came from the Hauptman-Woodward Medical Research Insti- tute HTCC [15].',
 'This dataset, which contains 147,456  \x0cimages from 96 different proteins but is limited to ex- periments with the microbatch-under-oil technique, has been used in a number of studies [57, 59].',
 'Most notably, Yann et al.',
 'used a deep convolutional neural network that automatically extracted features, and reported a correct classification rates as high as 97% for crystals and 96% for non-crystals.',
 'Although impressive, these results were however obtained from a curated subset of 85,188 clean images, i.e., images with class labels on which sev- eral human experts agreed [57].',
 'In order to validate our approach, we retrained our model to perform the same 10- way classification on that subset of the data alone without any tuning of the model’s hyperparameters and achieved 94.7% accuracy, compared to the reported 90.8%.',
 'In this context, the current results are especially re- markable.',
 'A crystallographer can classify images of exper- iments independently of the systems used to create those images.',
 'They can view an experiment with a microscope or look at a computer image and reach similar conclu- sions.',
 'They can look at a vapor diffusion experiment or a microbatch-under-oil setup and, again, asses either with confidence.',
 'Here, we show that this can be accomplished equally well, if not better, using deep CNNs.',
 'A benchtop researcher can classify many images, especially if they relate to a project that has been years in the making.',
 'For high-throughput approaches, however, that task becomes challenging.',
 'The strength of computational approaches is that each image is treated like the previous one, with no fatigue.',
 'Classification of 10,000 images is as consistent as classification of one.',
 'This advance opens the door for complete classification of all results in a high-throughput setting and for data mining of repositories of past image data.',
 'Another remarkable aspect of our results is that they leverage a very generic computer vision architecture orig- inally designed for a different classification problem – categorization of natural images – with very distinct char- acteristics.',
 'For instance, one c"
5_B1,"['Convolutional Neural Networks for Sentence Classi?cation  Yoon Kim  New York University yhk255@nyu.edu  4 1 0 2     p e S 3         ] L C .',
 's c [      2 v 2 8 8 5  .',
 '8 0 4 1 : v i X r a  Abstract  We report on a series of experiments with convolutional neural networks (CNN) trained on top of pre-trained word vec- tors for sentence-level classi?cation tasks.',
 'We show that a simple CNN with lit- tle hyperparameter tuning and static vec- tors achieves excellent results on multi- ple benchmarks.',
 'Learning task-speci?c vectors through ?ne-tuning offers further gains in performance.',
 'We additionally propose a simple modi?cation to the ar- chitecture to allow for the use of both task-speci?c and static vectors.',
 'The CNN models discussed herein improve upon the state of the art on 4 out of 7 tasks, which include sentiment analysis and question classi?cation.',
 'Introduction  1 Deep learning models have achieved remarkable results in computer vision (Krizhevsky et al., 2012) and speech recognition (Graves et al., 2013) in recent years.',
 'Within natural language process- ing, much of the work with deep learning meth- ods has involved learning word vector representa- tions through neural language models (Bengio et al., 2003; Yih et al., 2011; Mikolov et al., 2013) and performing composition over the learned word vectors for classi?cation (Collobert et al., 2011).',
 'Word vectors, wherein words are projected from a sparse, 1-of-V encoding (here V is the vocabulary size) onto a lower dimensional vector space via a hidden layer, are essentially feature extractors that encode semantic features of words in their dimen- sions.',
 'In such dense representations, semantically close words are likewise close—in euclidean or cosine distance—in the lower dimensional vector space.',
 'Convolutional neural networks (CNN) utilize layers with convolving ?lters that are applied to  local features (LeCun et al., 1998).',
 'Originally invented for computer vision, CNN models have subsequently been shown to be effective for NLP and have achieved excellent results in semantic parsing (Yih et al., 2014), search query retrieval (Shen et al., 2014), sentence modeling (Kalch- brenner et al., 2014), and other traditional NLP tasks (Collobert et al., 2011).',
 'In the present work, we train a simple CNN with one layer of convolution on top of word vectors obtained from an unsupervised neural language model.',
 'These vectors were trained by Mikolov et al.',
 '(2013) on 100 billion words of Google News, and are publicly available.1 We initially keep the word vectors static and learn only the other param- eters of the model.',
 'Despite little tuning of hyper- parameters, this simple model achieves excellent results on multiple benchmarks, suggesting that the pre-trained vectors are ‘universal’ feature ex- tractors that can be utilized for various classi?ca- tion tasks.',
 'Learning task-speci?c vectors through ?ne-tuning results in further improvements.',
 'We ?nally describe a simple modi?cation to the archi- tecture to allow for the use of both pre-trained and task-speci?c vectors by having multiple channels.',
 'Our work is philosophically similar to Razavian et al.',
 '(2014) which showed that for image clas- si?cation, feature extractors obtained from a pre- trained deep learning model perform well on a va- riety of tasks—including tasks that are very dif- ferent from the original task for which the feature extractors were trained.',
 '2 Model  The model architecture, shown in ?gure 1, is a slight variant of the CNN architecture of Collobert et al.',
 '(2011).',
 'Let xi ? Rk be the k-dimensional word vector corresponding to the i-th word in the sentence.',
 'A sentence of length n (padded where  1https://code.google.com/p/word2vec/  \x0cFigure 1: Model architecture with two channels for an example sentence.',
 'necessary) is represented as  x1:n = x1 ? x2 ? .',
 '.',
 '.',
 '? xn, (1) where ? is the concatenation operator.',
 'In gen- eral, let xi:i+j refer to the concatenation of words xi, xi+1, .',
 '.',
 '.',
 ', xi+j.',
 'A convolution operation in- volves a ?lter w ? Rhk, which is applied to a window of h words to produce a new feature.',
 'For example, a feature ci is generated from a window of words xi:i+h?1 by  ci = f (w · xi:i+h?1 + b).',
 '(2) Here b ? R is a bias term and f is a non-linear function such as the hyperbolic tangent.',
 'This ?lter is applied to each possible window of words in the sentence {x1:h, x2:h+1, .',
 '.',
 '.',
 ', xn?h+1:n} to produce a feature map  c = [c1, c2, .',
 '.',
 '.',
 ', cn?h+1],  (3) with c ? Rn?h+1.',
 'We then apply a max-over- time pooling operation (Collobert et al., 2011) over the feature map and take the maximum value ˆc = max{c} as the feature corresponding to this particular ?lter.',
 'The idea is to capture the most im- portant feature—one with the highest value—for each feature map.',
 'This pooling scheme naturally deals with variable sentence lengths.',
 'We have described the process by which one feature is extracted from one ?lter.',
 'The model uses multiple ?lters (with varying window sizes) to obtain multiple features.',
 'These features form the penultimate layer and are passed to a fully con- nected softmax layer whose output is the probabil- ity distribution over labels.',
 'In one of the model variants, we experiment with having two ‘channels’ of word vectors—one  that is kept static throughout training and one that is ?ne-tuned via backpropagation (section 3.2).2 In the multichannel architecture, illustrated in ?g- ure 1, each ?lter is applied to both channels and the results are added to calculate ci in equation (2).',
 'The model is otherwise equivalent to the sin- gle channel architecture.',
 '2.1 Regularization For regularization we employ dropout on the penultimate layer with a constraint on l2-norms of the weight vectors (Hinton et al., 2012).',
 'Dropout prevents co-adaptation of hidden units by ran- domly dropping out—i.e., setting to zero—a pro- portion p of the hidden units during foward- backpropagation.',
 'That is, given the penultimate layer z = [ˆc1, .',
 '.',
 '.',
 ', ˆcm] (note that here we have m ?lters), instead of using  y = w · z + b  (4)  for output unit y in forward propagation, dropout uses  y = w · (z ? r) + b,  (5) where ? is the element-wise multiplication opera- tor and r ? Rm is a ‘masking’ vector of Bernoulli random variables with probability p of being 1.',
 'Gradients are backpropagated only through the unmasked units.',
 'At test time, the learned weight vectors are scaled by p such that ˆw = pw, and ˆw is used (without dropout) to score unseen sen- tences.',
 'We additionally constrain l2-norms of the weight vectors by rescaling w to have ||w||2 = s whenever ||w||2 > s after a gradient descent step.',
 '2We employ language from computer vision where a color image has red, green, and blue channels.',
 ""wait for the video and do n't rent it n x k representation of sentence with static and non-static channels Convolutional layer with multiple filter widths and feature maps Max-over-time pooling Fully connected layer with dropout and  softmax output \x0cN  |V |  |Vpre|  Data MR SST-1 SST-2 Subj TREC  CR  Test l 20 10662 18765 16448 CV 18 11855 17836 16262 2210 19 9613 16185 14838 1821 23 10000 21323 17913 CV 500 10 5952 CV 19 3775 CV 3  c 2 5 2 2 6 2 MPQA 2 Table 1: Summary statistics for the datasets after tokeniza- tion."",
 'c: Number of target classes.',
 'l: Average sentence length.',
 'N: Dataset size.',
 '|V |: Vocabulary size.',
 '|Vpre|: Number of words present in the set of pre-trained word vectors.',
 'Test: Test set size (CV means there was no standard train/test split and thus 10-fold CV was used).',
 '9592 5340 10606 6246  9125 5046 6083  3 Datasets and Experimental Setup  We test our model on various benchmarks.',
 'Sum- mary statistics of the datasets are in table 1.',
 '• MR: Movie reviews with one sentence per re- view.',
 'Classi?cation involves detecting posi- tive/negative reviews (Pang and Lee, 2005).3 • SST-1: Stanford Sentiment Treebank—an extension of MR but with train/dev/test splits provided and ?ne-grained labels (very pos- itive, positive, neutral, negative, very nega- tive), re-labeled by Socher et al.',
 '(2013).4  • SST-2: Same as SST-1 but with neutral re-  views removed and binary labels.',
 '• Subj: Subjectivity dataset where the task is to classify a sentence as being subjective or objective (Pang and Lee, 2004).',
 '• TREC: TREC question dataset—task in- volves classifying a question into 6 question types (whether the question is about person, location, numeric information, etc.)',
 '(Li and Roth, 2002).5  • CR: Customer reviews of various products (cameras, MP3s etc.).',
 'Task is to predict pos- itive/negative reviews (Hu and Liu, 2004).6  3https://www.cs.cornell.edu/people/pabo/movie-review-data/ 4http://nlp.stanford.edu/sentiment/ Data is actually provided at the phrase-level and hence we train the model on both phrases and sentences but only score on sentences at test time, as in Socher et al.',
 '(2013), Kalchbrenner et al.',
 '(2014), and Le and Mikolov (2014).',
 'Thus the training set is an order of magnitude larger than listed in table 1.',
 '5http://cogcomp.cs.illinois.edu/Data/QA/QC/ 6http://www.cs.uic.edu/?liub/FBS/sentiment-analysis.html  • MPQA: Opinion polarity detection subtask of the MPQA dataset (Wiebe et al., 2005).7  3.1 Hyperparameters and Training For all datasets we use: recti?ed linear units, ?lter windows (h) of 3, 4, 5 with 100 feature maps each, dropout rate (p) of 0.5, l2 constraint (s) of 3, and mini-batch size of 50.',
 'These values were chosen via a grid search on the SST-2 dev set.',
 'We do not otherwise perform any dataset- speci?c tuning other than early stopping on dev sets.',
 'For datasets without a standard dev set we randomly select 10% of the training data as the dev set.',
 'Training is done through stochastic gra- dient descent over shuf?ed mini-batches with the Adadelta update rule (Zeiler, 2012).',
 '3.2 Pre-trained Word Vectors Initializing word vectors with those obtained from an unsupervised neural language model is a popu- lar method to improve performance in the absence of a large supervised training set (Collobert et al., 2011; Socher et al., 2011; Iyyer et al., 2014).',
 'We use the publicly available word2vec vectors that were trained on 100 billion words from Google News.',
 'The vectors have dimensionality of 300 and were trained using the continuous bag-of-words architecture (Mikolov et al., 2013).',
 'Words not present in the set of pre-trained words are initial- ized randomly.',
 '3.3 Model Variations We experiment with several variants of the model.',
 '• CNN-rand: Our baseline model where all words are randomly initialized and then mod- i?ed during training.',
 '• CNN-static: A model with pre-trained vectors from word2vec.',
 'All words— including the unknown ones that are ran- domly initialized—are kept static and only the other parameters of the model are learned.',
 '• CNN-non-static: Same as above but the pre- trained vectors are ?ne-tuned for each task.',
 '• CNN-multichannel: A model with two sets of word vectors.',
 'Each set of vectors is treated as a ‘channel’ and each ?lter is applied  7http://www.cs.pitt.edu/mpqa/  \x0cModel CNN-rand CNN-static CNN-non-static CNN-multichannel RAE (Socher et al., 2011) MV-RNN (Socher et al., 2012) RNTN (Socher et al., 2013) DCNN (Kalchbrenner et al., 2014) Paragraph-Vec (Le and Mikolov, 2014) CCAE (Hermann and Blunsom, 2013) Sent-Parser (Dong et al., 2014) NBSVM (Wang and Manning, 2012) MNB (Wang and Manning, 2012) G-Dropout (Wang and Manning, 2013) F-Dropout (Wang and Manning, 2013) Tree-CRF (Nakagawa et al., 2010) CRF-PR (Yang and Cardie, 2014) SVMS (Silva et al., 2011)  MR 76.1 81.0 81.5 81.1 77.7 79.0 ? ? ? 77.8 79.5 79.4 79.0 79.0 79.1 77.3 ? ?  SST-1 45.0 45.5 48.0 47.4 43.2 44.4 45.7 48.5 48.7 ? ? ? ? ? ? ? ? ?  SST-2 82.7 86.8 87.2 88.1 82.4 82.9 85.4 86.8 87.8 ? ? ? ? ? ? ? ? ?  Subj 89.6 93.0 93.4 93.2 ? ? ? ? ? ? ? 93.2 93.6 93.4 93.6 ? ? ?  TREC 91.2 92.8 93.6 92.2 ? ? ? 93.0 ? ? ? ? ? ? ? ? ? 95.0  CR MPQA 79.8 84.7 84.3 85.0 ? ? ? ? ? ? ? 81.8 80.0 82.1 81.9 81.4 82.7 ?  83.4 89.6 89.5 89.4 86.4 ? ? ? ? 87.2 86.3 86.3 86.3 86.1 86.3 86.1 ? ?  Table 2: Results of our CNN models against other methods.',
 'RAE: Recursive Autoencoders with pre-trained word vectors from Wikipedia (Socher et al., 2011).',
 'MV-RNN: Matrix-Vector Recursive Neural Network with parse trees (Socher et al., 2012).',
 'RNTN: Recursive Neural Tensor Network with tensor-based feature function and parse trees (Socher et al., 2013).',
 'DCNN: Dynamic Convolutional Neural Network with k-max pooling (Kalchbrenner et al., 2014).',
 'Paragraph-Vec: Logistic regres- sion on top of paragraph vectors (Le and Mikolov, 2014).',
 'CCAE: Combinatorial Category Autoencoders with combinatorial category grammar operators (Hermann and Blunsom, 2013).',
 'Sent-Parser: Sentiment analysis-speci?c parser (Dong et al., 2014).',
 'NBSVM, MNB: Naive Bayes SVM and Multinomial Naive Bayes with uni-bigrams from Wang and Manning (2012).',
 'G-Dropout, F-Dropout: Gaussian Dropout and Fast Dropout from Wang and Manning (2013).',
 'Tree-CRF: Dependency tree with Conditional Random Fields (Nakagawa et al., 2010).',
 'CRF-PR: Conditional Random Fields with Posterior Regularization (Yang and Cardie, 2014).',
 'SVMS: SVM with uni-bi-trigrams, wh word, head word, POS, parser, hypernyms, and 60 hand-coded rules as features from Silva et al.',
 '(2011).',
 'to both channels, but gradients are back- propagated only through one of the chan- nels.',
 'Hence the model is able to ?ne-tune one set of vectors while keeping the other static.',
 'Both channels are initialized with word2vec.',
 'In order to disentangle the effect of the above variations versus other random factors, we elim- inate other sources of randomness—CV-fold as- signment, initialization of unknown word vec- tors, initialization of CNN parameters—by keep- ing them uniform within each dataset.',
 '4 Results and Discussion  Results of our models against other methods are listed in table 2.',
 'Our baseline model with all ran- domly initialized words (CNN-rand) does not per- form well on its own.',
 'While we had expected per- formance gains through the use of pre-trained vec- tors, we were surprised at the magnitude of the gains.',
 'Even a simple model with static vectors (CNN-static) performs remarkably well, giving  competitive results against the more sophisticated deep learning models that utilize complex pool- ing schemes (Kalchbrenner et al., 2014) or require parse trees to be computed beforehand (Socher et al., 2013).',
 'These results suggest that the pre- trained vectors are good, ‘universal’ feature ex- tractors and can be utilized across datasets.',
 'Fine- tuning the pre-trained vectors for each task gives still further improvements (CNN-non-static).',
 '4.1 Multichannel vs.',
 'Single Channel Models We had initially hoped that the multichannel ar- chitecture would prevent over?tting (by ensuring that the learned vectors do not deviate too far from the original values) and thus work better than the single channel model, especially on smaller datasets.',
 'The results, however, are mixed, and fur- ther work on regularizing the ?ne-tuning process is warranted.',
 'For instance, instead of using an additional channel for the non-static portion, one could maintain a single channel but employ extra dimensions that are allowed to be modi?ed during training.',
 'Most Similar Words for  Static Channel Non-static Channel  bad  good  n’t  !',
 ',  good terrible horrible lousy great bad  terri?c decent  os ca  ireland  wo 2,500 entire jez  changer decasia abysmally  demise valiant  terrible horrible lousy stupid nice decent solid terri?c  not never nothing neither 2,500 lush  beautiful terri?c  but  dragon  a and  Table 3: Top 4 neighboring words—based on cosine similarity—for vectors in the static channel (left) and ?ne- tuned vectors in the non-static channel (right) from the mul- tichannel model on the SST-2 dataset after training.',
 '4.2 Static vs. Non-static Representations As is the case with the single channel non-static model, the multichannel model is able to ?ne-tune the non-static channel to make it more speci?c to the task-at-hand.',
 'For example, good is most sim- ilar to bad in word2vec, presumably because they are (almost) syntactically equivalent.',
 'But for vectors in the non-static channel that were ?ne- tuned on the SST-2 dataset, this is no longer the case (table 3).',
 'Similarly, good is arguably closer to nice than it is to great for expressing sentiment, and this is indeed re?ected in the learned vectors.',
 'For (randomly initialized) tokens not in the set of pre-trained vectors, ?ne-tuning allows them to learn more meaningful representations: the net- work learns that exclamation marks are associ- ated with effusive expressions and that commas are conjunctive (table 3).',
 '4.3 Further Observations We report on some further experiments and obser- vations:  • Kalchbrenner et al.',
 '(2014) report much worse results with a CNN that has essentially  the same architecture as our single channel model.',
 'For example, their Max-TDNN (Time Delay Neural Network) with randomly ini- tialized words obtains 37.4% on the SST-1 dataset, compared to 45.0% for our model.',
 'We attribute such discrepancy to our CNN having much more capacity (multiple ?lter widths and feature maps).',
 '• Dropout proved to be such a good regularizer that it was ?ne to use a larger than necessary network and simply let dropout regularize it.',
 'Dropout consistently added 2%–4% relative performance.',
 '• When randomly initializing words not  in improve- word2vec, we obtained slight ments by sampling each dimension from U [?a, a] where a was chosen such that the randomly initialized vectors have the same variance as the pre-trained ones.',
 'It would be interesting to see if employing more sophis- ticated methods to mirror the distribution of pre-trained vectors in the initialization pro- cess gives further improvements.',
 '• We brie?y experimented with another set of publicly available word vectors trained by (2011) on Wikipedia,8 and Collobert et al.',
 'found that word2vec gave far superior per- formance.',
 'It is not clear whether this is due to Mikolov et al.',
 '(2013)’s architecture or the 100 billion word Google News dataset.',
 '• Adadelta (Zeiler, 2012) gave similar results to Adagrad (Duchi et al., 2011) but required fewer epochs.',
 '5 Conclusion In the present work we have described a series of experiments with convolutional neural networks built on top of word2vec.',
 'Despite little tuning of hyperparameters, a simple CNN with one layer of convolution performs remarkably well.',
 'Our re- sults add to the well-established evidence that un- supervised pre-training of word vectors is an im- portant ingredient in deep learning for NLP.',
 'Acknowledgments We would like to thank Yann LeCun and the anonymous reviewers for their helpful feedback and suggestions.',
 '8http://ronan.collobert.com/senna/  \x0cReferences Y. Bengio, R. Ducharme, P. Vincent.',
 '2003.',
 'Neu- ral Probabilitistic Language Model.',
 'Journal of Ma- chine Learning Research 3:1137–1155.',
 'R. Collobert, J. Weston, L. Bottou, M. Karlen, K. Kavukcuglu, P. Kuksa.',
 '2011.',
 'Natural Language Processing (Almost) from Scratch.',
 'Journal of Ma- chine Learning Research 12:2493–2537.',
 'J. Duchi, E. Hazan, Y.',
 'Singer.',
 '2011 Adaptive subgra- dient methods for online learning and stochastic op- timization.',
 'Journal of Machine Learning Research, 12:2121–2159.',
 'L. Dong, F. Wei, S. Liu, M. Zhou, K. Xu.',
 '2014.',
 'A Statistical Parsing Framework for Sentiment Classi- ?cation.',
 'CoRR, abs/1401.6330.',
 'A. Graves, A. Mohamed, G. Hinton.',
 '2013.',
 'Speech recognition with deep recurrent neural networks.',
 'In Proceedings of ICASSP 2013.',
 'B. Pang, L. Lee.',
 '2004.',
 'A sentimental education: Sentiment analysis using subjectivity summarization In Proceedings of ACL based on minimum cuts.',
 '2004.',
 'B. Pang, L. Lee.',
 '2005.',
 'Seeing stars: Exploiting class relationships for sentiment categorization with re- spect to rating scales.',
 'In Proceedings of ACL 2005.',
 'A.S. Razavian, H. Azizpour, J. Sullivan, S. Carlsson 2014.',
 'CNN Features off-the-shelf: an Astounding Baseline.',
 'CoRR, abs/1403.6382.',
 'Y. Shen, X.',
 'He, J. Gao, L. Deng, G. Mesnil.',
 '2014.',
 'Learning Semantic Representations Using Convolu- tional Neural Networks for Web Search.',
 'In Proceed- ings of WWW 2014.',
 'J. Silva, L. Coheur, A. Mendes, A. Wichert.',
 '2011.',
 'From symbolic to sub-symbolic information in ques- tion classi?cation.',
 'Arti?cial Intelligence Review, 35(2):137–154.',
 'G. Hinton, N. Srivastava, A. Krizhevsky, I. Sutskever, R. Salakhutdinov.',
 'Improving neural net- works by preventing co-adaptation of feature detec- tors.',
 'CoRR, abs/1207.0580.',
 '2012.',
 'R. Socher, J. Pennington, E. Huang, A. Ng, C. Man- ning.',
 '2011.',
 'Semi-Supervised Recursive Autoen- In coders for Predicting Sentiment Distributions.',
 'Proceedings of EMNLP 2011.',
 'K. Hermann, P. Blunsom.',
 '2013.',
 'The Role of Syntax in Vector Space Models of Compositional Semantics.',
 'In Proceedings of ACL 2013.',
 'R. Socher, B. Huval, C. Manning, A. Ng.',
 '2012.',
 'Se- mantic Compositionality through Recursive Matrix- Vector Spaces.',
 'In Proceedings of EMNLP 2012.',
 'M. Hu, B. Liu.',
 '2004.',
 'Mining and Summarizing Cus- In Proceedings of ACM SIGKDD  tomer Reviews.',
 '2004.',
 'M. Iyyer, P. Enns, J. Boyd-Graber, P. Resnik 2014.',
 'Political Ideology Detection Using Recursive Neural Networks.',
 'In Proceedings of ACL 2014.',
 'N. Kalchbrenner, E. Grefenstette, P. Blunsom.',
 '2014.',
 'A Convolutional Neural Network for Modelling Sen- tences.',
 'In Proceedings of ACL 2014.',
 'A. Krizhevsky, I. Sutskever, G. Hinton.',
 '2012.',
 'Ima- geNet Classi?cation with Deep Convolutional Neu- ral Networks.',
 'In Proceedings of NIPS 2012.',
 'Q.',
 'Le, T. Mikolov.',
 '2014.',
 'Distributed Represenations In Proceedings of  of Sentences and Documents.',
 'ICML 2014.',
 'Y. LeCun, L. Bottou, Y. Bengio, P. Haffner.',
 '1998.',
 'Gradient-based learning applied to document recog- In Proceedings of the IEEE, 86(11):2278– nition.',
 '2324, November.',
 'X. Li, D. Roth.',
 '2002.',
 'Learning Question Classi?ers.',
 'In Proceedings of ACL 2002.',
 'T. Mikolov, I. Sutskever, K. Chen, G. Corrado, J.',
 'Dean.',
 '2013.',
 'Distributed Representations of Words and Phrases and their Compositionality.',
 'In Proceedings of NIPS 2013.',
 'T. Nakagawa, K. Inui, S. Kurohashi.',
 '2010.',
 'De- pendency tree-based sentiment classi?cation using CRFs with hidden variables.',
 'In Proceedings of ACL 2010.',
 'R. Socher, A. Perelygin, J. Wu, J. Chuang, C. Manning, A. Ng, C. Potts.',
 '2013.',
 'Recursive Deep Models for Semantic Compositionality Over a Sentiment Tree- bank.',
 'In Proceedings of EMNLP 2013.',
 'J. Wiebe, T. Wilson, C. Cardie.',
 '2005.',
 'Annotating Ex- pressions of Opinions and Emotions in Language.',
 'Language Resources and Evaluation, 39(2-3): 165– 210.',
 'S. Wang, C. Manning.',
 '2012.',
 'Baselines and Bigrams: Simple, Good Sentiment and Topic Classi?cation.',
 'In Proceedings of ACL 2012.',
 'S. Wang, C. Manning.',
 '2013.',
 'Fast Dropout Training.',
 'In Proceedings of ICML 2013.',
 'B. Yang, C. Cardie.',
 '2014.',
 'Context-aware Learning for Sentence-level Sentiment Analysis with Poste- rior Regularization.',
 'In Proceedings of ACL 2014.',
 'W. Yih, K. Toutanova, J. Platt, C. Meek.',
 '2011.',
 'Learn- ing Discriminative Projections for Text Similarity Measures.',
 'Proceedings of the Fifteenth Confer- ence on Computational Natural Language Learning, 247–256.',
 'W. Yih, X.',
 'He, C. Meek.',
 '2014.',
 'Semantic Parsing for In Proceed-  Single-Relation Question Answering.',
 'ings of ACL 2014.',
 'M. Zeiler.',
 '2012.',
 'Adadelta: An adaptive learning rate  method.',
 'CoRR, abs/1212.5701.']
txt2"
6_B1,"
['5 1 0 2    r a  M 9         ] L M  .',
 't a t s [      1 v 1 3 5 2 0  .',
 '3 0 5 1 : v i X r a  Distilling the Knowledge in a Neural Network  Geoffrey Hinton? †  Google Inc.  Mountain View  Oriol Vinyals†  Google Inc.  Mountain View  Jeff Dean Google Inc.  Mountain View  geoffhinton@google.com  vinyals@google.com  jeff@google.com  Abstract  A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions [3].',
 'Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow de- ployment to a large number of users, especially if the individual models are large neural nets.',
 'Caruana and his collaborators [1] have shown that it is possible to compress the knowledge in an ensemble into a single model which is much eas- ier to deploy and we develop this approach further using a different compression technique.',
 'We achieve some surprising results on MNIST and we show that we can signi?cantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model.',
 'We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish ?ne-grained classes that the full mod- els confuse.',
 'Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel.',
 '1 Introduction  Many insects have a larval form that is optimized for extracting energy and nutrients from the envi- ronment and a completely different adult form that is optimized for the very different requirements of traveling and reproduction.',
 'In large-scale machine learning, we typically use very similar models for the training stage and the deployment stage despite their very different requirements: For tasks like speech and object recognition, training must extract structure from very large, highly redundant datasets but it does not need to operate in real time and it can use a huge amount of computation.',
 'Deployment to a large number of users, however, has much more stringent requirements on latency and computational resources.',
 'The analogy with insects suggests that we should be willing to train very cumbersome models if that makes it easier to extract structure from the data.',
 'The cumbersome model could be an ensemble of separately trained models or a single very large model trained with a very strong regularizer such as dropout [9].',
 'Once the cumbersome model has been trained, we can then use a different kind of training, which we call “distillation” to transfer the knowledge from the cumbersome model to a small model that is more suitable for deployment.',
 'A version of this strategy has already been pioneered by Rich Caruana and his collaborators [1].',
 'In their important paper they demonstrate convincingly that the knowledge acquired by a large ensemble of models can be transferred to a single small model.',
 'A conceptual block that may have prevented more investigation of this very promising approach is that we tend to identify the knowledge in a trained model with the learned parameter values and this makes it hard to see how we can change the form of the model but keep the same knowledge.',
 'A more abstract view of the knowledge, that frees it from any particular instantiation, is that it is a learned  ?Also af?liated with the University of Toronto and the Canadian Institute for Advanced Research.',
 '†Equal contribution.',
 '1  \x0cmapping from input vectors to output vectors.',
 'For cumbersome models that learn to discriminate between a large number of classes, the normal training objective is to maximize the average log probability of the correct answer, but a side-effect of the learning is that the trained model assigns probabilities to all of the incorrect answers and even when these probabilities are very small, some of them are much larger than others.',
 'The relative probabilities of incorrect answers tell us a lot about how the cumbersome model tends to generalize.',
 'An image of a BMW, for example, may only have a very small chance of being mistaken for a garbage truck, but that mistake is still many times more probable than mistaking it for a carrot.',
 'It is generally accepted that the objective function used for training should re?ect the true objective of the user as closely as possible.',
 'Despite this, models are usually trained to optimize performance on the training data when the real objective is to generalize well to new data.',
 'It would clearly be better to train models to generalize well, but this requires information about the correct way to generalize and this information is not normally available.',
 'When we are distilling the knowledge from a large model into a small one, however, we can train the small model to generalize in the same way as the large model.',
 'If the cumbersome model generalizes well because, for example, it is the average of a large ensemble of different models, a small model trained to generalize in the same way will typically do much better on test data than a small model that is trained in the normal way on the same training set as was used to train the ensemble.',
 'An obvious way to transfer the generalization ability of the cumbersome model to a small model is to use the class probabilities produced by the cumbersome model as “soft targets” for training the small model.',
 'For this transfer stage, we could use the same training set or a separate “transfer” set.',
 'When the cumbersome model is a large ensemble of simpler models, we can use an arithmetic or geometric mean of their individual predictive distributions as the soft targets.',
 'When the soft targets have high entropy, they provide much more information per training case than hard targets and much less variance in the gradient between training cases, so the small model can often be trained on much less data than the original cumbersome model and using a much higher learning rate.',
 'For tasks like MNIST in which the cumbersome model almost always produces the correct answer with very high con?dence, much of the information about the learned function resides in the ratios of very small probabilities in the soft targets.',
 'For example, one version of a 2 may be given a probability of 10?6 of being a 3 and 10?9 of being a 7 whereas for another version it may be the other way around.',
 'This is valuable information that de?nes a rich similarity structure over the data (i. e. it says which 2’s look like 3’s and which look like 7’s) but it has very little in?uence on the cross-entropy cost function during the transfer stage because the probabilities are so close to zero.',
 'Caruana and his collaborators circumvent this problem by using the logits (the inputs to the ?nal softmax) rather than the probabilities produced by the softmax as the targets for learning the small model and they minimize the squared difference between the logits produced by the cumbersome model and the logits produced by the small model.',
 'Our more general solution, called “distillation”, is to raise the temperature of the ?nal softmax until the cumbersome model produces a suitably soft set of targets.',
 'We then use the same high temperature when training the small model to match these soft targets.',
 'We show later that matching the logits of the cumbersome model is actually a special case of distillation.',
 'The transfer set that is used to train the small model could consist entirely of unlabeled data [1] or we could use the original training set.',
 'We have found that using the original training set works well, especially if we add a small term to the objective function that encourages the small model to predict the true targets as well as matching the soft targets provided by the cumbersome model.',
 'Typically, the small model cannot exactly match the soft targets and erring in the direction of the correct answer turns out to be helpful.',
 '2 Distillation  Neural networks typically produce class probabilities by using a “softmax” output layer that converts the logit, zi, computed for each class into a probability, qi, by comparing zi with the other logits.',
 'qi =  exp(zi/T )  Pj exp(zj/T )  2  (1)  \x0cwhere T is a temperature that is normally set to 1.',
 'Using a higher value for T produces a softer probability distribution over classes.',
 'In the simplest form of distillation, knowledge is transferred to the distilled model by training it on a transfer set and using a soft target distribution for each case in the transfer set that is produced by using the cumbersome model with a high temperature in its softmax.',
 'The same high temperature is used when training the distilled model, but after it has been trained it uses a temperature of 1.',
 'When the correct labels are known for all or some of the transfer set, this method can be signi?cantly improved by also training the distilled model to produce the correct labels.',
 'One way to do this is to use the correct labels to modify the soft targets, but we found that a better way is to simply use a weighted average of two different objective functions.',
 'The ?rst objective function is the cross entropy with the soft targets and this cross entropy is computed using the same high temperature in the softmax of the distilled model as was used for generating the soft targets from the cumbersome model.',
 'The second objective function is the cross entropy with the correct labels.',
 'This is computed using exactly the same logits in softmax of the distilled model but at a temperature of 1.',
 'We found that the best results were generally obtained by using a condiderably lower weight on the second objective function.',
 'Since the magnitudes of the gradients produced by the soft targets scale as 1/T 2 it is important to multiply them by T 2 when using both hard and soft targets.',
 'This ensures that the relative contributions of the hard and soft targets remain roughly unchanged if the temperature used for distillation is changed while experimenting with meta-parameters.',
 '2.1 Matching logits is a special case of distillation  Each case in the transfer set contributes a cross-entropy gradient, dC/dzi, with respect to each logit, zi of the distilled model.',
 'If the cumbersome model has logits vi which produce soft target probabilities pi and the transfer training is done at a temperature of T , this gradient is given by:  If the temperature is high compared with the magnitude of the logits, we can approximate:  ?C ?zi  =  1 T  (qi ? pi) =  1  T   ezi/T  Pj ezj/T ?  evi/T  Pj evj /T!',
 'N +Pj vj/T!',
 '1 + vi/T  ?C ?zi  ?  1  T   1 + zi/T N +Pj zj/T  ?  (2)  (3)  If we now assume that the logits have been zero-meaned separately for each transfer case so that  Pj zj =Pj vj = 0 Eq.',
 '3 simpli?es to:  ?C ?zi  ?  1 N T 2 (zi ? vi)  (4)  So in the high temperature limit, distillation is equivalent to minimizing 1/2(zi ? vi)2, provided the logits are zero-meaned separately for each transfer case.',
 'At lower temperatures, distillation pays much less attention to matching logits that are much more negative than the average.',
 'This is poten- tially advantageous because these logits are almost completely unconstrained by the cost function used for training the cumbersome model so they could be very noisy.',
 'On the other hand, the very negative logits may convey useful information about the knowledge acquired by the cumbersome model.',
 'Which of these effects dominates is an empirical question.',
 'We show that when the distilled model is much too small to capture all of the knowledege in the cumbersome model, intermedi- ate temperatures work best which strongly suggests that ignoring the large negative logits can be helpful.',
 '3 Preliminary experiments on MNIST  To see how well distillation works, we trained a single large neural net with two hidden layers of 1200 recti?ed linear hidden units on all 60,000 training cases.',
 'The net was strongly regularized using dropout and weight-constraints as described in [5].',
 'Dropout can be viewed as a way of training an exponentially large ensemble of models that share weights.',
 'In addition, the input images were  3  \x0cjittered by up to two pixels in any direction.',
 'This net achieved 67 test errors whereas a smaller net with two hidden layers of 800 recti?ed linear hidden units and no regularization achieved 146 errors.',
 'But if the smaller net was regularized solely by adding the additional task of matching the soft targets produced by the large net at a temperature of 20, it achieved 74 test errors.',
 'This shows that soft targets can transfer a great deal of knowledge to the distilled model, including the knowledge about how to generalize that is learned from translated training data even though the transfer set does not contain any translations.',
 'When the distilled net had 300 or more units in each of its two hidden layers, all temperatures above 8 gave fairly similar results.',
 'But when this was radically reduced to 30 units per layer, temperatures in the range 2.5 to 4 worked signi?cantly better than higher or lower temperatures.',
 'We then tried omitting all examples of the digit 3 from the transfer set.',
 'So from the perspective of the distilled model, 3 is a mythical digit that it has never seen.',
 'Despite this, the distilled model only makes 206 test errors of which 133 are on the 1010 threes in the test set.',
 'Most of the errors are caused by the fact that the learned bias for the 3 class is much too low.',
 'If this bias is increased by 3.5 (which optimizes overall performance on the test set), the distilled model makes 109 errors of which 14 are on 3s.',
 'So with the right bias, the distilled model gets 98.6% of the test 3s correct despite never having seen a 3 during training.',
 'If the transfer set contains only the 7s and 8s from the training set, the distilled model makes 47.3% test errors, but when the biases for 7 and 8 are reduced by 7.6 to optimize test performance, this falls to 13.2% test errors.',
 '4 Experiments on speech recognition  In this section, we investigate the effects of ensembling Deep Neural Network (DNN) acoustic models that are used in Automatic Speech Recognition (ASR).',
 'We show that the distillation strategy that we propose in this paper achieves the desired effect of distilling an ensemble of models into a single model that works signi?cantly better than a model of the same size that is learned directly from the same training data.',
 'State-of-the-art ASR systems currently use DNNs to map a (short) temporal context of features derived from the waveform to a probability distribution over the discrete states of a Hidden Markov Model (HMM) [4].',
 'More speci?cally, the DNN produces a probability distribution over clusters of tri-phone states at each time and a decoder then ?nds a path through the HMM states that is the best compromise between using high probability states and producing a transcription that is probable under the language model.',
 'Although it is possible (and desirable) to train the DNN in such a way that the decoder (and, thus, the language model) is taken into account by marginalizing over all possible paths, it is common to train the DNN to perform frame-by-frame classi?cation by (locally) minimizing the cross entropy between the predictions made by the net and the labels given by a forced alignment with the ground truth sequence of states for each observation:  ? = arg max  ??  P (ht|st; ??)  where ? are the parameters of our acoustic model P which maps acoustic observations at time t, st, to a probability, P (ht|st; ??) , of the “correct” HMM state ht, which is determined by a forced alignment with the correct sequence of words.',
 'The model is trained with a distributed stochastic gradient descent approach.',
 'We use an architecture with 8 hidden layers each containing 2560 recti?ed linear units and a ?nal softmax layer with 14,000 labels (HMM targets ht).',
 'The input is 26 frames of 40 Mel-scaled ?lter- bank coef?cients with a 10ms advance per frame and we predict the HMM state of 21st frame.',
 'The total number of parameters is about 85M.',
 'This is a slightly outdated version of the acoustic model used by Android voice search, and should be considered as a very strong baseline.',
 'To train the DNN acoustic model we use about 2000 hours of spoken English data, which yields about 700M training examples.',
 'This system achieves a frame accuracy of 58.9%, and a Word Error Rate (WER) of 10.9% on our development set.',
 '4  \x0cSystem Baseline  10xEnsemble  Distilled Single model  Test Frame Accuracy WER 10.9% 10.7% 10.7%  58.9% 61.1% 60.8%  Table 1: Frame classi?cation accuracy and WER showing that the distilled single model performs about as well as the averaged predictions of 10 models that were used to create the soft targets.',
 '4.1 Results  We trained 10 separate models to predict P (ht|st; ?), using exactly the same architecture and train- ing procedure as the baseline.',
 'The models are randomly initialized with different initial parameter values and we ?nd that this creates suf?cient diversity in the trained models to allow the averaged predictions of the ensemble to signi?cantly outperform the individual models.',
 'We have explored adding diversity to the models by varying the sets of data that each model sees, but we found this to not signi?cantly change our results, so we opted for the simpler approach.',
 'For the distillation we tried temperatures of [1, 2, 5, 10] and used a relative weight of 0.5 on the cross-entropy for the hard targets, where bold font indicates the best value that was used for table 1 .',
 'Table 1 shows that, indeed, our distillation approach is able to extract more useful information from the training set than simply using the hard labels to train a single model.',
 'More than 80% of the improvement in frame classi?cation accuracy achieved by using an ensemble of 10 models is trans- ferred to the distilled model which is similar to the improvement we observed in our preliminary experiments on MNIST.',
 'The ensemble gives a smaller improvement on the ultimate objective of WER (on a 23K-word test set) due to the mismatch in the objective function, but again, the im- provement in WER achieved by the ensemble is transferred to the distilled model.',
 'We have recently become aware of related work on learning a small acoustic model by matching the class probabilities of an already trained larger model [8].',
 'However, they do the distillation at a temperature of 1 using a large unlabeled dataset and their best distilled model only reduces the error rate of the small model by 28% of the gap between the error rates of the large and small models when they are both trained with hard labels.',
 '5 Training ensembles of specialists on very big datasets  Training an ensemble of models is a very simple way to take advantage of parallel computation and the usual objection that an ensemble requires too much computation at test time can be dealt with by using distillation.',
 'There is, however, another important objection to ensembles: If the individual models are large neural networks and the dataset is very large, the amount of computation required at training time is excessive, even though it is easy to parallelize.',
 'In this section we give an example of such a dataset and we show how learning specialist models that each focus on a different confusable subset of the classes can reduce the total amount of computation required to learn an ensemble.',
 'The main problem with specialists that focus on making ?ne-grained distinctions is that they over?t very easily and we describe how this over?tting may be prevented by using soft targets.',
 '5.1 The JFT dataset  JFT is an internal Google dataset that has 100 million labeled images with 15,000 labels.',
 'When we did this work, Google’s baseline model for JFT was a deep convolutional neural network [7] that had been trained for about six months using asynchronous stochastic gradient descent on a large number of cores.',
 'This training used two types of parallelism [2].',
 'First, there were many replicas of the neural net running on different sets of cores and processing different mini-batches from the training set.',
 'Each replica computes the average gradient on its current mini-batch and sends this gradient to a sharded parameter server which sends back new values for the parameters.',
 'These new values re?ect all of the gradients received by the parameter server since the last time it sent parameters to the replica.',
 'Second, each replica is spread over multiple cores by putting different subsets of the neurons on each core.',
 'Ensemble training is yet a third type of parallelism that can be wrapped  5  \x0cJFT 1: Tea party; Easter; Bridal shower; Baby shower; Easter Bunny; ... JFT 2: Bridge; Cable-stayed bridge; Suspension bridge; Viaduct; Chimney; ... JFT 3: Toyota Corolla E100; Opel Signum; Opel Astra; Mazda Familia; ...  Table 2: Example classes from clusters computed by our covariance matrix clustering algorithm  around the other two types, but only if a lot more cores are available.',
 'Waiting for several years to train an ensemble of models was not an option, so we needed a much faster way to improve the baseline model.',
 '5.2 Specialist Models  When the number of classes is very large, it makes sense for the cumbersome model to be an en- semble that contains one generalist model trained on all the data and many “specialist” models, each of which is trained on data that is highly enriched in examples from a very confusable subset of the classes (like different types of mushroom).',
 'The softmax of this type of specialist can be made much smaller by combining all of the classes it does not care about into a single dustbin class.',
 'To reduce over?tting and share the work of learning lower level feature detectors, each specialist model is initialized with the weights of the generalist model.',
 'These weights are then slightly modi- ?ed by training the specialist with half its examples coming from its special subset and half sampled at random from the remainder of the training set.',
 'After training, we can correct for the biased train- ing set by incrementing the logit of the dustbin class by the log of the proportion by which the specialist class is oversampled.',
 '5.3 Assigning classes to specialists  In order to derive groupings of object categories for the specialists, we decided to focus on categories that our full network often confuses.',
 'Even though we could have computed the confusion matrix and used it as a way to ?nd such clusters, we opted for a simpler approach that does not require the true labels to construct the clusters.',
 'In particular, we apply a clustering algorithm to the covariance matrix of the predictions of our generalist model, so that a set of classes Sm that are often predicted together will be used as targets for one of our specialist models, m. We applied an on-line version of the K-means algorithm to the columns of the covariance matrix, and obtained reasonable clusters (shown in Table 2).',
 'We tried several clustering algorithms which produced similar results.',
 '5.4 Performing inference with ensembles of specialists  Before investigating what happens when specialist models are distilled, we wanted to see how well ensembles containing specialists performed.',
 'In addition to the specialist models, we always have a generalist model so that we can deal with classes for which we have no specialists and so that we can decide which specialists to use.',
 'Given an input image x, we do top-one classi?cation in two steps:  Step 1: For each test case, we ?nd the n most probable classes according to the generalist model.',
 'Call this set of classes k. In our experiments, we used n = 1.',
 'Step 2: We then take all the specialist models, m, whose special subset of confusable classes, Sm, has a non-empty intersection with k and call this the active set of specialists Ak (note that this set may be empty).',
 'We then ?nd the full probability distribution q over all the classes that minimizes:  KL(pg, q) + Xm?Ak  KL(pm, q)  (5)  where KL denotes the KL divergence, and pm pg denote the probability distribution of a specialist model or the generalist full model.',
 'The distribution pm is a distribution over all the specialist classes of m plus a single dustbin class, so when computing its KL divergence from the full q distribution we sum all of the probabilities that the full q distribution assigns to all the classes in m’s dustbin.',
 '6  \x0cSystem Baseline  + 61 Specialist models  Conditional Test Accuracy  Test Accuracy  43.1% 45.9%  25.0% 26.1%  Table 3: Classi?cation accuracy (top 1) on the JFT development set.',
 '# of specialists covering 0 1 2 3 4 5 6 7 8 9 10 or more  # of test examples 350037 141993 67161 38801 26298 16474 10682 7376 4703 4706 9082  delta in top1 correct 0 +1421 +1572 +1124 +835 +561 +362 +232 +182 +208 +324  relative accuracy change 0.0% +3.4% +7.4% +8.8% +10.5% +11.1% +11.3% +12.8% +13.6% +16.6% +14.1%  Table 4: Top 1 accuracy improvement by # of specialist models covering correct class on the JFT test set.',
 'Eq.',
 '5 does not have a general closed form solution, though when all the models produce a single probability for each class the solution is either the arithmetic or geometric mean, depending on whether we use KL(p, q) or KL(q, p)).',
 'We parameterize q = sof tmax(z) (with T = 1) and we use gradient descent to optimize the logits z w.r.t.',
 'eq.',
 '5.',
 'Note that this optimization must be carried out for each image.',
 '5.5 Results  Starting from the trained baseline full network, the specialists train extremely fast (a few days in- stead of many weeks for JFT).',
 'Also, all the specialists are trained completely independently.',
 'Table 3 shows the absolute test accuracy for the baseline system and the baseline system combined with the specialist models.',
 'With 61 specialist models, there is a 4.4% relative improvement in test ac- curacy overall.',
 'We also report conditional test accuracy, which is the accuracy by only considering examples belonging to the specialist classes, and restricting our predictions to that subset of classes.',
 'For our JFT specialist experiments, we trained 61 specialist models, each with 300 classes (plus the dustbin class).',
 'Because the sets of classes for the specialists are not disjoint, we often had multiple specialists covering a particular image class.',
 'Table 4 shows the number of test set examples, the change in the number of examples correct at position 1 when using the specialist(s), and the rela- tive percentage improvement in top1 accuracy for the JFT dataset broken down by the number of specialists covering the class.',
 'We are encouraged by the general trend that accuracy improvements are larger when we have more specialists covering a particular class, since training independent specialist models is very easy to parallelize.',
 '6 Soft Targets as Regularizers  One of our main claims about using soft targets instead of hard targets is that a lot of helpful infor- mation can be carried in soft targets that could not possibly be encoded with a single hard target.',
 'In this section we demonstrate that this is a very large effect by using far less data to ?t the 85M pa- rameters of the baseline speech model described earlier.',
 'Table 5 shows that with only 3% of the data (about 20M examples), training the baseline model with hard targets leads to severe over?tting (we did early stopping, as the accuracy drops sharply after reaching 44.5%), whereas the same model trained with soft targets is able to recover almost all the information in the full training set (about 2% shy).',
 'It is even more remarkable to note that we did not have to do early stopping: the system with soft targets simply “converged” to 57%.',
 'This shows that soft targets are a very effective way of communicating the regularities discovered by a model trained on all of the data to another model.',
 '7  \x0cSystem & training set Baseline (100% of training set) Baseline (3% of training set) Soft Targets (3% of training set)  Train Frame Accuracy  Test Frame Accuracy  63.4% 67.3% 65.4%  58.9% 44.5% 57.0%  Table 5: Soft targets allow a new model to generalize well from only 3% of the training set.',
 'The soft targets are obtained by training on the full training set.',
 '6.1 Using soft targets to prevent specialists from over?tting  The specialists that we used in our experiments on the JFT dataset collapsed all of their non-specialist classes into a single dustbin class.',
 'If we allow specialists to have a full softmax over all classes, there may be a much better way to prevent them over?tting than using early stopping.',
 'A specialist is trained on data that is highly enriched in its special classes.',
 'This means that the effective size of its training set is much smaller and it has a strong tendency to over?t on its special classes.',
 'This problem cannot be solved by making the specialist a lot smaller because then we lose the very helpful transfer effects we get from modeling all of the non-specialist classes.',
 'Our experiment using 3% of the speech data strongly suggests that if a specialist is initialized with the weights of the generalist, we can make it retain nearly all of its knowledge about the non-special classes by training it with soft targets for the non-special classes in addition to training it with hard targets.',
 'The soft targets can be provided by the generalist.',
 'We are currently exploring this approach.',
 '7 Relationship to Mixtures of Experts  The use of specialists that are trained on subsets of the data has some resemblance to mixtures of experts [6] which use a gating network to compute the probability of assigning each example to each expert.',
 'At the same time as the experts are learning to deal with the examples assigned to them, the gating network is learning to choose which experts to assign each example to based on the relative discriminative performance of the experts for that example.',
 'Using the discriminative performance of the experts to determine the learned assignments is much better than simply clustering the input vectors and assigning an expert to each cluster, but it makes the training hard to parallelize: First, the weighted training set for each expert keeps changing in a way that depends on all the other experts and second, the gating network needs to compare the performance of different experts on the same example to know how to revise its assignment probabilities.',
 'These dif?culties have meant that mixtures of experts are rarely used in the regime where they might be most bene?cial: tasks with huge datasets that contain distinctly different subsets.',
 'It is much easier to parallelize the training of multiple specialists.',
 'We ?rst train a generalist model and then use the confusion matrix to de?ne the subsets that the specialists are trained on.',
 'Once these subsets have been de?ned the specialists can be trained entirely independently.',
 'At test time we can use the predictions from the generalist model to decide which specialists are relevant and only these specialists need to be run.',
 '8 Discussion  We have shown that distilling works very well for transferring knowledge from an ensemble or from a large highly regularized model into a smaller, distilled model.',
 'On MNIST distillation works remarkably well even when the transfer set that is used to train the distilled model lacks any examples of one or more of the classes.',
 'For a deep acoustic model that is version of the one used by Android voice search, we have shown that nearly all of the improvement that is achieved by training an ensemble of deep neural nets can be distilled into a single neural net of the same size which is far easier to deploy.',
 'For really big neural networks, it can be infeasible even to train a full ensemble, but we have shown that the performance of a single really big net that has been trained for a very long time can be signif- icantly improved by learning a large number of specialist nets, each of which learns to discriminate between the classes in a highly confusable cluster.',
 'We have not yet shown that we can distill the knowledge in the specialists back into the single large net.',
 '8  \x0cAcknowledgments  We thank Yangqing Jia for assistance wi"
7_B1,"['Effective Use of Word Order for Text Categorization  with Convolutional Neural Networks  Rie Johnson  RJ Research Consulting Tarrytown, NY, USA  Tong Zhang  Baidu Inc., Beijing, China  Rutgers University, Piscataway, NJ, USA  5 1 0 2    r a     M 6 2      ] L C .',
 's c [      2 v 8 5 0 1  .',
 '2 1 4 1 : v i X r a  riejohnson@gmail.com  tzhang@stat.rutgers.edu  Abstract  Convolutional neural network (CNN) is a neu- ral network that can make use of the inter- nal structure of data such as the 2D structure of image data.',
 'This paper studies CNN on text categorization to exploit the 1D structure (namely, word order) of text data for accurate prediction.',
 'Instead of using low-dimensional word vectors as input as is often done, we directly apply CNN to high-dimensional text data, which leads to directly learning embed- ding of small text regions for use in classi?- cation.',
 'In addition to a straightforward adap- tation of CNN from image to text, a sim- ple but new variation which employs bag-of- word conversion in the convolution layer is proposed.',
 'An extension to combine multiple convolution layers is also explored for higher accuracy.',
 'The experiments demonstrate the effectiveness of our approach in comparison with state-of-the-art methods.',
 '1  Introduction  Text categorization is the task of automatically as- signing pre-de?ned categories to documents writ- ten in natural languages.',
 'Several types of text cat- egorization have been studied, each of which deals with different types of documents and categories, such as topic categorization to detect discussed top- ics (e.g., sports, politics), spam detection (Sahami et al., 1998), and sentiment classi?cation (Pang et al., 2002; Pang and Lee, 2008; Maas et al., 2011) to de- termine the sentiment typically in product or movie reviews.',
 'A standard approach to text categorization is to represent documents by bag-of-word vectors,  To appear in NAACL HLT 2015.  namely, vectors that indicate which words appear in the documents but do not preserve word order, and use classi?cation models such as SVM.',
 'It has been noted that loss of word order caused by bag-of-word vectors (bow vectors) is particularly problematic on sentiment classi?cation.',
 'A simple remedy is to use word bi-grams in addition to uni- grams (Blitzer et al., 2007; Glorot et al., 2011; Wang and Manning, 2012).',
 'However, use of word n-grams with n > 1 on text categorization in general is not always effective; e.g., on topic categorization, sim- ply adding phrases or n-grams is not effective (see, e.g., references in (Tan et al., 2002)).',
 'To bene?t from word order on text categoriza- tion, we take a different approach, which employs convolutional neural networks (CNN) (LeCun et al., 1986).',
 'CNN is a neural network that can make use of the internal structure of data such as the 2D struc- ture of image data through convolution layers, where each computation unit responds to a small region of input data (e.g., a small square of a large image).',
 'We apply CNN to text categorization to make use of the 1D structure (word order) of document data so that each unit in the convolution layer responds to a small region of a document (a sequence of words).',
 'CNN has been very successful on image clas- si?cation; see e.g., the winning solutions of Im- ageNet Large Scale Visual Recognition Challenge (Krizhevsky et al., 2012; Szegedy et al., 2014; Rus- sakovsky et al., 2014).',
 'On text, since the work on token-level applica- tions (e.g., POS tagging) by Collobert et al.',
 '(2011), CNN has been used in systems for entity search, sen- tence modeling, word embedding learning, product feature mining, and so on (Xu and Sarikaya, 2013; Gao et al., 2014; Shen et al., 2014; Kalchbrenner et  \x0cal., 2014; Xu et al., 2014; Tang et al., 2014; Weston et al., 2014; Kim, 2014).',
 'Notably, in many of these CNN studies on text, the ?rst layer of the network converts words in sentences to word vectors by ta- ble lookup.',
 'The word vectors are either trained as part of CNN training, or ?xed to those learned by some other method (e.g., word2vec (Mikolov et al., 2013)) from an additional large corpus.',
 'The latter is a form of semi-supervised learning, which we study elsewhere.',
 'We are interested in the effectiveness of CNN itself without aid of additional resources; therefore, word vectors should be trained as part of network training if word vector lookup is to be done.',
 'A question arises, however, whether word vector lookup in a purely supervised setting is really useful for text categorization.',
 'The essence of convolution layers is to convert text regions of a ?xed size (e.g., “am so happy” with size 3) to feature vectors, as de- scribed later.',
 'In that sense, a word vector learning layer is a special (and unusual) case of convolution layer with region size one.',
 'Why is size one appro- priate if bi-grams are more discriminating than uni- grams?',
 'Hence, we take a different approach.',
 'We di- rectly apply CNN to high-dimensional one-hot vec- tors; i.e., we directly learn embedding1 of text re- gions without going through word embedding learn- ing.',
 'This approach is made possible by solving the computational issue2 through ef?cient handling of high-dimensional sparse data on GPU, and it turned out to have the merits of improving accuracy with fast training/prediction and simplifying the system (fewer hyper-parameters to tune).',
 'Our CNN code for text is publicly available on the internet3.',
 'We study the effectiveness of CNN on text cate- gorization and explain why CNN is suitable for the task.',
 'Two types of CNN are tested: seq-CNN is a straightforward adaptation of CNN from image to text, and bow-CNN is a simple but new variation of CNN that employs bag-of-word conversion in the convolution layer.',
 'The experiments show that seq-  1We use the term ‘embedding’ loosely to mean a structure- preserving function, in particular, a function that generates low- dimensional features that preserve the predictive structure.',
 '2CNN implemented for image would not handle sparse data ef?ciently, and without ef?cient handling of sparse data, convo- lution over high-dimensional one-hot vectors would be compu- tationally infeasible.',
 '3riejohnson.com/cnn_download.html  Figure 1: Convolutional neural network.',
 'Figure 2: Convolution layer for image.',
 'Each computation unit (oval) computes a non-linear function ?(W· r(cid:96)(x) + b) of a small region r(cid:96)(x) of input image x, where weight matrix W and bias vector b are shared by all the units in the same layer.',
 'CNN outperforms bow-CNN on sentiment classi- ?cation, vice versa on topic classi?cation, and the winner generally outperforms the conventional bag- of-n-gram vector-based methods, as well as previ- ous CNN models for text which are more complex.',
 'In particular, to our knowledge, this is the ?rst work that has successfully used word order to improve topic classi?cation performance.',
 'A simple exten- sion that combines multiple convolution layers (thus combining multiple types of text region embedding) leads to further improvement.',
 'Through empirical analysis, we will show that CNN can make effec- tive use of high-order n-grams when conventional methods fail.',
 '2 CNN for document classi?cation  We ?rst review CNN applied to image data and then discuss the application of CNN to document classi- ?cation tasks to introduce seq-CNN and bow-CNN.',
 '2.1 Preliminary: CNN for image CNN is a feed-forward neural network with convo- lution layers interleaved with pooling layers, as il- lustrated in Figure 1, where the top layer performs classi?cation using the features generated by the lay- ers below.',
 'A convolution layer consists of several computation units, each of which takes as input a region vector that represents a small region of the input image and applies a non-linear function to it.',
 'Typically, the region vector is a concatenation of  OutputInput[ 0 0 1 0 0 0 0 0 0 0 ]Convolution layerPooling layerConvolution layerPooling layerOutput layer(Linear classifier)[ 0 0 1 0 0 0 0 0 0 0 ]\x0cpixels in the region, which would be, for example, 75-dimensional if the region is 5× 5 and the number of channels is three (red, green, and blue).',
 'Concep- tually, computation units are placed over the input image so that the entire image is collectively cov- ered, as illustrated in Figure 2.',
 'The region stride (distance between the region centers) is often set to a small value such as 1 so that regions overlap with each other, though the stride in Figure 2 is set larger than the region size for illustration.',
 'A distinguishing feature of convolution layers is weight sharing.',
 'Given input x, a unit associ- ated with the (cid:96)-th region computes ?(W · r(cid:96)(x) + b), where r(cid:96)(x) is a region vector representing the region of x at location (cid:96), and ? is a pre- de?ned component-wise non-linear activation func- tion, (e.g., applying ?(x) = max(x, 0) to each vec- tor component).',
 'The matrix of weights W and the vector of biases b are learned through training, and they are shared by the computation units in the same layer.',
 'This weight sharing enables learning useful features irrespective of their location, while preserv- ing the location where the useful features appeared.',
 'We regard the output of a convolution layer as an ‘image’ so that the output of each computation unit is considered to be a ‘pixel’ of m channels where m is the number of weight vectors (i.e., the number of rows of W) or the number of neurons.',
 'In other words, a convolution layer converts image regions to m-dim vectors, and the locations of the regions are inherited through this conversion.',
 'The output image of the convolution layer is passed to a pooling layer, which essentially shrinks the image by merging neighboring pixels, so that higher layers can deal with more abstract/global in- formation.',
 'A pooling layer consists of pooling units, each of which is associated with a small region of the image.',
 'Commonly-used merging methods are average-pooling and max-pooling, which respec- tively compute the channel-wise average/maximum of each region.',
 '2.2 CNN for text Now we consider application of CNN to text data.',
 'Suppose that we are given a document D = (w1, w2, .',
 '.',
 '.)',
 'with vocabulary V .',
 'CNN requires vec- tor representation of data that preserves internal lo- cations (word order in this case) as input.',
 'A straight-  forward representation would be to treat each word as a pixel, treat D as if it were an image of |D| × 1 pixels with |V | channels, and to represent each pixel (i.e., each word) as a |V |-dimensional one-hot vec- tor4.',
 'As a running toy example, suppose that vocab- ulary V = { “don’t”, “hate”, “I”, “it”, “love” } and we associate the words with dimensions of vector in alphabetical order (as shown), and that document D=“I love it”.',
 'Then, we have a document vector:  (cid:62) x = [ 0 0 1 0 0 | 0 0 0 0 1 | 0 0 0 1 0 ]  .',
 '2.2.1  seq-CNN for text  As in the convolution layer for image, we repre- sent each region (which each computation unit re- sponds to) by a concatenation of the pixels, which makes p|V |-dimensional region vectors where p is the region size ?xed in advance.',
 'For example, on the example document vector x above, with p = 2 and stride 1, we would have two regions “I love” and “love it” represented by the following vectors:  \uf8ee\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8f0  \uf8f9\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fb  don(cid:48)t hate  I it  love don(cid:48)t hate  I it love  0 0 0 0 1 — 0 0 0 1 0  \uf8ee\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8f0  \uf8f9\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fb  don(cid:48)t hate  I it  love don(cid:48)t hate  I it  love  0 0 1 0 0 — 0 0 0 0 1  r0(x) =  r1(x) =  The rest is the same as image; the text region vec- tors are converted to feature vectors, i.e., the con- volution layer learns to embed text regions into low- dimensional vector space.',
 'We call a neural net with a convolution layer with this region representation seq-CNN (‘seq’ for keeping sequences of words) to distinguish it from bow-CNN, described next.',
 '2.2.2 bow-CNN for text  A potential problem of seq-CNN however, is that unlike image data with 3 RGB channels, the number of ‘channels’ |V | (size of vocabulary) may be very large (e.g., 100K), which could make each region vector r(cid:96)(x) very high-dimensional if the region size 4Alternatively, one could use bag-of-letter-n-gram vectors as in (Shen et al., 2014; Gao et al., 2014) to cope with out-of- vocabulary words and typos.',
 'p is large.',
 'Since the dimensionality of region vec- tors determines the dimensionality of weight vec- tors, having high-dimensional region vectors means more parameters to learn.',
 'If p|V | is too large, the model becomes too complex (w.r.t.',
 'the amount of training data available) and/or training becomes un- affordably expensive even with ef?cient handling of sparse data; therefore, one has to lower the dimen- sionality by lowering the vocabulary size |V | and/or the region size p, which may or may not be desir- able, depending on the nature of the task.',
 'An alternative we provide is to perform bag- of-word conversion to make region vectors |V |- dimensional instead of p|V |-dimensional; e.g., the example region vectors above would be converted to:  \uf8f9\uf8fa\uf8fa\uf8fa\uf8fb  \uf8ee\uf8ef\uf8ef\uf8ef\uf8f0  0 0 1 0 1  don(cid:48)t hate  \uf8ee\uf8ef\uf8ef\uf8ef\uf8f0  \uf8f9\uf8fa\uf8fa\uf8fa\uf8fb  don(cid:48)t hate  I it  love  0 0 0 1 1  r0(x) =  r1(x) =  I it  love  With this representation, we have fewer param- eters to learn.',
 'the expressiveness of bow-convolution (which loses word order only within small regions) is somewhere between seq- convolution and bow vectors.',
 'Essentially,  2.2.3 Pooling for text  Whereas the size of images is ?xed in image ap- plications, documents are naturally variable-sized, and therefore, with a ?xed stride, the output of a con- volution layer is also variable-sized as shown in Fig- ure 3.',
 'Given the variable-sized output of the convo- lution layer, standard pooling for image (which uses a ?xed pooling region size and a ?xed stride) would produce variable-sized output, which can be passed to another convolution layer.',
 'To produce ?xed-sized output, which is required by the fully-connected top layer5, we ?x the number of pooling units and dy- namically determine the pooling region size on each data point so that the entire data is covered without overlapping.',
 'In the previous CNN work on text, pooling is typically max-pooling over the entire data (i.e., one  5In this work, the top layer is fully-connected (i.e., each neu- ron responds to the entire data) as in CNN for image.',
 'Alterna- tively, the top layer could be convolutional so that it can receive variable-sized input, but such CNN would be more complex.',
 'Figure 3: Convolution layer for variable-sized text.',
 'pooling unit associated with the whole text).',
 'The dy- namic k-max pooling of (Kalchbrenner et al., 2014) for sentence modeling extends it to take the k largest values where k is a function of the sentence length, but it is again over the entire data, and the operation is limited to max-pooling.',
 'Our pooling differs in that it is a natural extension of standard pooling for im- age, in which not only max-pooling but other types can be applied.',
 'With multiple pooling units associ- ated with different regions, the top layer can receive locational information (e.g., if there are two pooling units, the features from the ?rst half and last half of a document are distinguished).',
 'This turned out to be useful (along with average-pooling) on topic classi- ?cation, as shown later.',
 '2.3 CNN vs. bag-of-n-grams Traditional methods represent each document en- tirely with one bag-of-n-gram vector and then ap- ply a classi?er model such as SVM.',
 'However, since high-order n-grams are susceptible to data sparsity, use of a large n such as 20 is not only infeasible but also ineffective.',
 'Also note that a bag-of-n-gram represents each n-gram by a one-hot vector and ig- nores the fact that some n-grams share constituent words.',
 'By contrast, CNN internally learns embed- ding of text regions (given the consituent words as input) useful for the intended task.',
 'Consequently, a large n such as 20 can be used especially with the bow-convolution layer, which turned out to be useful on topic classi?cation.',
 'A neuron trained to assign a large value to, e.g., “I love” (and a small value to “I hate”) is likely to assign a large value to “we love” (and a small value to “we hate”) as well, even though “we love” was never seen during training.',
 'We will con?rm these points empirically later.',
 '2.4 Extension: parallel CNN We have described CNN with the simplest network architecture that has one pair of convolution and pooling layers.',
 'While this can be extended in sev- eral ways (e.g., with deeper layers), in our experi- ments, we explored parallel CNN, which has two or  I  love  it This  isn’t   what   I  expected  !',
 '(a)                                                       (b)This  isn’t   what   I  expected  !',
 '(a)                                                       (b)\x0cFigure 4: CNN with two convolution layers in parallel.',
 'more convolution layers in parallel6, as illustrated in Figure 4.',
 'The idea is to learn multiple types of em- bedding of small text regions so that they can com- plement each other to improve model accuracy.',
 'In this architecture, multiple convolution-pooling pairs with different region sizes (and possibly different re- gion vector representations) are given one-hot vec- tors as input and produce feature vectors for each region; the top layer takes the concatenation of the produced feature vectors as input.',
 '3 Experiments  We experimented with CNN on two tasks, topic clas- si?cation and sentiment classi?cation.',
 'Detailed in- formation for reproducing the results is available on the internet along with our code.',
 '3.1 CNN We ?xed the activation function to recti?er ?(x) = max(x, 0) and minimized square loss with L2 reg- ularization by stochastic gradient descent (SGD).',
 'We only used the 30K words that appeared most frequently in the training set; thus, for example, in seq-CNN with region size 3, a region vector is 90K dimensional.',
 'Out-of-vocabulary words were repre- sented by a zero vector.',
 'On bow-CNN, to speed up computation, we used variable region stride so that a larger stride was taken where repetition7 of the same region vectors can be avoided by doing so.',
 'Padding8 size was ?xed to p ? 1 where p is the region size.',
 '6Similar architectures have been used for image.',
 'Kim (2014) used it for text, but it was on top of a word vector con- version layer.',
 '7For example, if we slide a window of size 3 over “* * foo * *” where “*” is out of vocabulary, a bag of “foo” will be repeated three times with stride ?xed to 1.',
 '8As is commonly done, to the beginning and the end of each document, special words that are treated as unknown words (and converted to zero vectors instead of one-hot vectors) were added as ‘padding’.',
 'The purpose is to equally treat the words at the edge and words in the middle.',
 'We used two techniques commonly used with CNN on image, which typically led to small per- formance improvements.',
 'One is dropout (Hinton et al., 2012) optionally applied to the input to the top layer.',
 'The other is response normalization as in (Krizhevsky et al., 2012), which in our case scales the output of the pooling layer z at each location by multiplying (1 + |z|2)?1/2.',
 '3.2 Baseline methods For comparison, we tested SVM with the linear ker- nel and fully-connected neural networks (see e.g., Bishop (1995)) with bag-of-n-gram vectors as in- put.',
 'To experiment with fully-connected neural nets, as in CNN, we minimized square loss with L2 reg- ularization and optional dropout by SGD, and ac- tivation was ?xed to recti?er.',
 'To generate bag-of- n-gram vectors, on topic classi?cation, we ?rst set each component to log(x + 1) where x is the word frequency in the document and then scaled them to unit vectors, which we found always improved per- formance over raw frequency.',
 'On sentiment classi- ?cation, as is often done, we generated binary vec- tors and scaled them to unit vectors.',
 'We tested three types of bag-of-n-gram: bow1 with n ? {1}, bow2 with n ? {1, 2}, and bow3 with n ? {1, 2, 3}; that is, bow1 is the traditional bow vectors, and with bow3, each component of the vectors corresponds to either uni-gram, bi-gram, or tri-gram of words.',
 'We used SVMlight9 for the SVM experiments.',
 'NB-LM We also tested NB-LM, which ?rst ap- peared (but without performance report10 ) as NB- SVM in WM12 (Wang and Manning, 2012) and later with a small modi?cation produced perfor- mance that exceeds state-of-the-art supervised meth- ods on IMDB (which we experimented with) in MMRB14 (Mesnil et al., 2014).',
 'We experimented with the MMRB14 version, which generates bi- nary bag-of-n-gram vectors, multiplies the com- ponent for each n-gram fi with log(P (fi|Y = 1)/P (fi|Y = ?1)) (NB-weight) where the prob- abilities are estimated using the training data, and does logistic regression training.',
 'We used MMRB14’s software11 with a modi?cation so that  9http://svmlight.joachims.org/ 10WM12 instead reported the performance of an ensemble of  NB and SVM as it performed better.',
 '11https://github.com/mesnilgr/nbsvm  Output1 (positive)Input: “I really love it !”                Oneregion size s1region size Convolution layersPooling layersOutput layer1 (positive)Input: “I really love it !”                One-hot vectorsregion size s2\x0cthe regularization parameter can be tuned on devel- opment data.',
 '3.3 Model selection For all the methods, the hyper-parameters such as net con?gurations and regularization parameters were chosen based on the performance on the devel- opment data (held-out portion of the training data), and using the chosen hyper-parameters, the models were re-trained using all the training data.',
 '3.4 Data, tasks, and data preprocessing IMDB: movie reviews The IMDB dataset (Maas et al., 2011) is a benchmark dataset for sentiment classi?cation.',
 'The task is to determine if the movie reviews are positive or negative.',
 'Both the training and test sets consist of 25K reviews.',
 'For preprocess- ing, we tokenized the text so that emoticons such as “:-)” are treated as tokens and converted all the char- acters to lower case.',
 'Elec: electronics product reviews Elec consists of electronic product reviews.',
 'It is part of a large Amazon review dataset (McAuley and Leskovec, 2013).',
 'We chose electronics as it seemed to be very different from movies.',
 'Following the generation of IMDB (Maas et al., 2011), we chose the training set and the test set so that one half of each set consists of positive reviews and the other half is negative, re- garding rating 1 and 2 as negative and 4 and 5 as positive, and that the reviewed products are disjoint between the training set and test set.',
 'Note that to extract text from the original data, we only used the text section, and we did not use the summary sec- tion.',
 'This way, we obtained a test set of 25K reviews (same as IMDB) and training sets of various sizes.',
 'The training and test sets are available on the inter- net12.',
 'Data preprocessing was the same as IMDB.',
 'RCV1: topic categorization RCV1 is a corpus of Reuters news articles as described in LYRL04 (Lewis et al., 2004).',
 'RCV1 has 103 topic categories in a hierarchy, and one document may be associated with more than one topic.',
 'Performance on this task (multi-label categorization) is known to be sensitive to thresholding strategies, which are algorithms ad- ditional to the models we would like to test.',
 'There- fore, we also experimented with single-label cate-  12riejohnson.com/cnn_data.html  label single Table 2 Fig.',
 '6 single Table 4 multi  #train 15,564 varies 23,149  #test 49,838 49,838 781,265  #class 55 55 103  Table 1: RCV1 data summary.',
 'gorization to assign one of 55 second-level topics to each document to directly evaluate models.',
 'For this task, we used the documents from a one-month period as the test set and generated various sizes of training sets from the documents with earlier dates.',
 'Data sizes are shown in Table 1.',
 'As in LYRL04, we used the concatenation of the headline and text ele- ments.',
 'Data preprocessing was the same as IMDB except that we used the stopword list provided by LYRL04 and regarded numbers as stopwords.',
 '3.5 Performance results Table 2 shows the error rates of CNN in comparison with the baseline methods.',
 'The ?rst thing to note is that on all the datasets, the best-performing CNN outperforms the baseline methods, which demon- strates the effectiveness of our approach.',
 'To look into the details, let us ?rst focus on CNN with one convolution layer (seq- and bow-CNN in the table).',
 'On sentiment classi?cation (IMDB and Elec), the con?guration chosen by model selection was: region size 3, stride 1, 1000 weight vectors, and max-pooling with one pooling unit, for both types of CNN; seq-CNN outperforms bow-CNN, as well as all the baseline methods except for one.',
 'Note that with a small region size and max-pooling, if a review contains a short phrase that conveys strong sentiment (e.g., “A great movie!”), the review could receive a high score irrespective of the rest of the re- view.',
 'It is sensible that this type of con?guration is effective on sentiment classi?cation.',
 'By contrast, on topic categorization (RCV1), the con?guration chosen for bow-CNN by model selec- tion was: region size 20, variable-stride?2, average- pooling with 10 pooling units, and 1000 weight vec- tors, which is very different from sentiment classi?- cation.',
 'This is presumably because on topic clas- si?cation, a larger context would be more predic- tive than short fragments (? larger region size), the entire document matters (? the effectiveness of average-pooling), and the location of predictive text also matters (? multiple pooling units).',
 'The last  \x0cpoint may be because news documents tend to have crucial sentences (as well as the headline) at the be- ginning.',
 'On this task, while both seq and bow-CNN outperform the baseline methods, bow-CNN outper- forms seq-CNN, which indicates that in this setting the merit of having fewer parameters is larger than the bene?t of keeping word order in each region.',
 'Now we turn to parallel CNN.',
 'On IMDB, seq2- CNN, which has two seq-convolution layers (region size 2 and 3; 1000 neurons each; followed by one unit of max-pooling each), outperforms seq-CNN.',
 'With more neurons (3000 neurons each; Table 3) it further exceeds the best-performing baseline, which is also the best previous supervised result.',
 'We pre- sume the effectiveness of seq2-CNN indicates that the length of predictive text regions is variable.',
 'The best performance 7.67 on IMDB was ob- tained by ‘seq2-bown-CNN’, equipped with three layers in parallel: two seq-convolution layers (1000 neurons each) as in seq2-CNN above and one layer (20 neurons) that regards the entire document as one region and represents the region (document) by a bag-of-n-gram vector (bow3) as input to the compu- tation unit; in particular, we generated bow3 vectors by multiplying the NB-weights with binary vectors, motivated by the good performance of NB-LM.',
 'This third layer is a bow-convolution layer13 with one re- gion of variable size that takes one-hot vectors with n-gram vocabulary as input to learn document em- bedding.',
 'The seq2-bown-CNN for Elec in the ta- ble is the same except that the regions sizes of seq- convolution layers are 3 and 4.',
 'On both datasets, performance is improved over seq2-CNN.',
 'The re- sults suggest that what can be learned through these three layers are distinct enough to complement each other.',
 'The effectiveness of the third layer indicates that not only short word sequences but also global context in a large window may be useful on this task; thus, inclusion of a bow-convolution layer with n- gram vocabulary with a large ?xed region size might be even more effective, providing more focused con- text, but we did not pursue it in this work.',
 'Baseline methods Comparing the baseline meth- ods with each other, on sentiment classi?cation, re- ducing the vocabulary to the most frequent n-grams  13It can also be regarded as a fully-connected layer that takes  bow3 vectors as input.',
 'methods SVM bow3 (30K) SVM bow1 (all) SVM bow2 (all) SVM bow3 (all) NN bow3 (all) NB-LM bow3 (all) bow-CNN seq-CNN seq2-CNN seq2-bown-CNN  IMDB 10.14 11.36 9.74 9.42 9.17 8.13 8.66 8.39 8.04 7.67  Elec RCV1 10.68 9.16 10.76 11.71 10.59 9.05 8.71 10.69 10.67 8.48 13.97 8.11 9.33 8.39 7.64 9.96 – 7.48 7.14 –  Table 2: Error rate (%) comparison with bag-of-n-gram- based methods.',
 'Sentiment classi?cation on IMDB and Elec (25K training documents) and 55-way topic cate- gorization on RCV1 (16K training documents).',
 '‘(30K)’ indicates that the 30K most frequent n-grams were used, and ‘(all)’ indicates that all the n-grams (up to 5M) were used.',
 'CNN used the 30K most frequent words.',
 'SVM bow2 [WM12] WRRBM+bow [DAL12] NB+SVM bow2 [WM12] NB-LM bow3 [MMRB14] Paragraph vectors [LM14] seq2-CNN (3K×2) [Ours] seq2-bown-CNN [Ours]  10.84 10.77 8.78 8.13 7.46 7.94 7.67  – –  –  ensemble  unlabeled data  – –  Table 3: Error rate (%) comparison with previous best methods on IMDB.',
 'notably hurt performance (also observed on NB-LM and NN) even though some reduction is a common practice.',
 'Error rates were clearly improved by ad- dition of bi- and tri-grams.',
 'By contrast, on topic categorization, bi-grams only slightly improved ac- curacy, and reduction of vocabulary did not hurt per- formance.',
 'NB-LM is very strong on IMDB and poor on RCV1; its effectiveness appears to be data- dependent, as also observed by WM12.',
 'Comparison with state-of-the-art results As shown in Table 3, the previous best supervised result on IMDB is 8.13 by NB-LM with bow3 (MMRB14), and our best error rate 7.67 is better by nearly 0.5%.',
 '(Le and Mikolov, 2014) reports 7.46 with the semi- supervised method that learns low-dimensional vec- tor representations of documents from unlabeled data.',
 'Their result is not directly comparable with our supervised results due to use of additional resource.',
 'Nevertheless, our best result rivals their result.',
 'We tested bow-CNN on the multi-label  topic categorization task on RCV1 to compare with  \x0cmodels  micro-F macro-F  LYRL04’s best SVM  bow-CNN  81.6 84.0  60.7 64.8  Table 4: RCV1 micro-averaged and macro-averaged F- measure results on multi-label task with LYRL04 split.',
 'LYRL04.',
 'We used the same thresholding strategy as LYRL04.',
 'As shown in Table 4, bow-CNN outper- forms LYRL04’s best results even though our data preprocessing is much simpler (no stemming and no tf-idf weighting).',
 'Previous CNN We focus on the sentence classi?- cation studies due to its relation to text categoriza- tion.',
 'Kim (2014) studied ?ne-tuning of pre-trained word vectors to produce input to parallel CNN.',
 'He reported that performance was poor when word vec- tors were trained as part of CNN training (i.e., no ad- ditional method/corpus).',
 'On our tasks, we were also unable to outperform the baselines with this type of model.',
 'Also, with our approach, a system is sim- pler with one fewer layer – no need to tune the di- mensionality of word vectors or meta-parameters for word vector learning.',
 'Kalchbrenner et al.',
 '(2014) proposed complex modi?cations of CNN for sentence modeling.',
 'No- tably, given word vectors ? Rd, their convolution with m feature maps produces for each region a ma- trix ? Rd×m (instead of a vector ? Rm as in stan- dard CNN).',
 'Using the provided code, we found that their model is too resource-demanding for our tasks.',
 'On IMDB and Elec14 the best error rates we ob- tained by training with various con?gurations that ?t in memory for 24 hours each on GPU (cf.',
 'Fig 5) were 10.13 and 9.37, respectively, which is no bet- ter than SVM bow2.',
 'Since excellent performances were reported on short sentence cla"
8_B1,"['7 1 0 2     g u A 4 2         ]  V C .',
 's c [      2 v 0 3 8 6 0  .',
 '5 0 7 1 : v i X r a  Exploring the structure of a real-time, arbitrary neural  artistic stylization network  Golnaz Ghiasi Google Brain  Honglak Lee Google Brain  Manjunath Kudlur  Google Brain  golnazg@google.com  honglak@google.com  keveman@google.com  Vincent Dumoulin  MILA, Universit´e de Montr´eal  Jonathon Shlens Google Brain  vi.dumoulin@gmail.com  shlens@google.com  August 28, 2017  Abstract  In this paper, we present a method which combines the ?exibility of the neural algorithm of artistic style with the speed of fast style transfer networks to allow real-time stylization using any content/style image pair.',
 'We build upon recent work leveraging conditional instance normalization for multi-style transfer networks by learning to predict the conditional instance normalization parameters directly from a style image.',
 'The model is successfully trained on a corpus of roughly 80,000 paintings and is able to generalize to paintings previously unobserved.',
 'We demonstrate that the learned embedding space is smooth and contains a rich structure and organizes semantic information associated with paintings in an entirely unsupervised manner.',
 '1  Introduction  Elmyr de Hory gained world-wide fame by forging thousands of pieces of artwork and selling them to art dealers and museums [13].',
 'The forger’s skill is a testament to the human talent and intelligence required to reproduce the artistic details of a diverse set of paintings.',
 'In computer vision, much work has been invested in teaching computers to likewise capture the artistic style of a painting with the goal of conferring this style in arbitrary photographs in a convincing manner.',
 'Early work in this effort in computer vision arose out of visual texture synthesis.',
 'Such work focused on building non-parametric techniques for “growing” visual textures one pixel [5, 27] or one patch [4, 17] at a time.',
 '(2001) [4] demonstrated that one may transfer a texture to an arbitrary photograph to confer it with the stylism of a drawing.',
 'Likewise, Hertzmann et al.',
 '(2001) [11] demonstrated a non-parametric technique for imbuing an arbitrary ?lter to an image based on pairs of un?ltered and ?ltered images.',
 'Interestingly, Efros et al.',
 'In parallel to non-parametric approaches, a second line of research focused on building parametric models of visual textures constrained to match the marginal spatial statistics of visual patterns [15].',
 'Early models focused on matching the marginal statistics of multi-scale linear ?lter banks [21, 6].',
 'In recent years, spatial image statistics gleaned from intermediate features of state-of-the-art image classi?ers [23]  1  \x0cFigure 1: Stylizations produced by our network trained on a large corpus of paintings and textures.',
 'The left-most column shows four content images.',
 'Left: Stylizations from paintings in training set on paintings (4 left columns) and textures (4 right columns).',
 'Right: Stylizations from paintings never previously observed by our network.',
 'proved superior for capturing visual textures [8].',
 'Pairing a secondary constraint to preserve the content of an image – as measured by the higher level layers of the same image classi?cation network – extended this idea to artistic style transfer [9] (see also [10]).',
 'Optimizing an image or photograph to obey these constraints is computationally expensive and con- tains no learned representation for artistic style.',
 'Several research groups addressed this problem by building a secondary network, i.e., style transfer network, to explicitly learn the transformation from a photograph a particular painting style [14, 16, 25].',
 'Although this method confers computational speed, much ?exibility is lost: a single style transfer network is learned for a single painting style and a separate style transfer network must be built and trained for each new painting style.',
 'Most crucially, by partitioning the style transfer problem customized for a speci?c style of painting, these methods avoid the critical ability to learn a shared representation across paintings.',
 'Recent work by Dumoulin et al.',
 '[3] demonstrated that the manipulation of the normalization parameters was suf?cient to train a single style transfer network across 32 varied painting styles.',
 'Such a network distilled the artistic style into a roughly 3000 dimensional space that is regular enough to permit smooth interpolation between these painting styles.',
 'Despite the promise, this model can cover only a limited number of styles and cannot generalize well to an unseen style.',
 'In this work, we extend these ideas further by building a style transfer network trained on about 80,000 painting and 6,000 visual textures.',
 'We demonstrate that this network can generalize to capture and transfer the artistic style of paintings never previously observed by the system (see Figure 1).',
 'Our contributions in this paper include:  1.',
 'Introduce a new algorithm for fast, arbitrary artistic style transfer trained on 80,000 paintings that  can operate in real time on never previously observed paintings.',
 '2.',
 'Represent all painting styles in a compact embedding space that captures features of the semantics  of paintings.',
 '3.',
 'Demonstrate that training with a large number of paintings uniquely affords the model the ability  to predict styles never previously observed.',
 '4.',
 'Embedding space permits novel exploration of artistic range of artist.',
 '2  \x0cFigure 2: Diagram of model architecture.',
 'The style prediction network P (·) predicts an embedding vector (cid:126)S from an input style image, which supplies a set of normalization constants for the style transfer network.',
 'The style transfer network transforms the photograph into a stylized representation.',
 'The content and style losses [9] are derived from the distance in representational space of the VGG image classi?cation network [23].',
 'The style transfer network largely follows [3] and the style prediction network largely follows the Inception-v3 architecture [24].',
 '2 Methods  Artistic style transfer may be de?ned as creating a stylized image x from a content image c and a style image s. Typically, the content image c is a photograph and the style image s is a painting.',
 'A neural algorithm of artistic style [9] posits the content and style of an image may be de?ned as follows:  • Two images are similar in content if their high-level features as extracted by an image recognition  system are close in Euclidean distance.',
 '• Two images are similar in style if their low-level features as extracted by an image recognition  system share the same spatial statistics.',
 'The ?rst de?nition is motivated by the observation that higher level features of pretrained image clas- si?cation systems are tuned to semantic information in an image [28, 14, 19].',
 'The second de?nition is motivated by the hypothesis that a painting style may be regarded as a visual texture [11, 4, 9].',
 'A rich literature suggests that repeated motifs representative of a visual texture may be characterized by lower- order spatial statistics [15, 21, 6].',
 'Images with identical lower-order spatial statistics appear perceptually identical and capture a visual texture [21, 8, 26, 6].',
 'Assuming that a visual texture is spatially homoge- neous implies that the lower-order spatial statistics may be represented by a Gram matrix expressing the spatially-averaged correlations across ?lters within a given layer’s representation [21, 8, 6].',
 'The complete optimization objective for style transfer may be expressed as  (1) where Lc(x, c) and Ls(x, s) are the content and style losses, respectively and ?s is a Lagrange multiplier weighting the relative strength of the style loss.',
 'We associate lower-level and higher-level features as the  min  Lc(x, c) + ?sLs(x, s)  x  3  \x0c(cid:88) (cid:88)  1 ni  i?S Lc(x, c) =  activations within a given set of lower layers S and higher layers C in an image classi?cation network.',
 'The content and style losses are de?ned as Ls(x, s) =  || G[fi(x)] ? G[fi(s)] ||2  (2)  F  || fj(x) ? fj(c) ||2  1 nj  (3) where fl(x) are the network activations at layer l, nl is the total number of units at layer l and G[fl(x)] is the Gram matrix associated with the layer l activations.',
 'The Gram matrix is a square, symmetric matrix measuring the spatially averaged correlation structure across the ?lters within a layer’s activations.',
 'j?C  2  Early work focused on iteratively updating an image to synthesize a visual texture [21, 6, 8] or transfer an artistic style to an image [9].',
 'This optimization procedure is slow and precludes any opportunity to learn a representation of a painting style.',
 'Subsequent work introduced a second network, a style transfer network T (·), to learn a transformation from the content image c to its artistically rendered version ˆx (i.e., ˆx = T (c)) [14, 26, 16].',
 'The style transfer network is a convolutional neural network formulated in the structure of an encoder/decoder [14, 26].',
 'The training objective is the combination of style loss and content loss obtained by replacing x in Eq.',
 '1 with the network output T (c).',
 'The parameters of the style transfer network are trained by minimizing this objective using a corpus of photographic images as content.',
 'The resulting network may artistically render an image dramatically faster, but a separate network must be learned for each painting style.',
 'Training a new network for each painting is wasteful because painting styles share common visual textures, color palettes and semantics for parsing the scene of an image.',
 'Building a style transfer network that shares its representation across many paintings would provide a rich vocabulary for representing any painting.',
 'A simple trick recognized in [3] is to build a style transfer network as a typical encoder/decoder architecture but specialize the normalization parameters speci?c to each painting style.',
 'This procedure, termed conditional instance normalization, proposes normalizing each unit’s activation z as  (cid:18) z ? µ  (cid:19)  ?  ˜z = ?s  + ?s  (4)  where µ and ? are the mean and standard deviation across the spatial axes in an activation map [26].',
 '?s and ?s constitute a linear transformation that specify the learned mean (?s) and learned standard deviation (?s) of the unit.',
 'This linear transformation is unique to each painting style s. In particular, the concatenation (cid:126)S = {?s, ?s} constitutes a roughly 3000-d embedding vector representing the artistic style of a painting.',
 'We denote this style transfer network as T (·, (cid:126)S).',
 'The set of all {?s, ?s} across N = 32 paintings constitute 0.2% of the network parameters.',
 'Dumoulin et al.',
 '[3] showed that such a network provides a fast stylization of artistic styles and the embedding space is rich and smooth enough to allow users to combine the painting styles by interpolating the learned embedding vectors of 32 styles.',
 'Although an important step forward, this “N-style network” is still limited compared to the original optimization-based technique [9] because the network is limited to only work on the styles explicitly trained on.',
 'The goal of this work is to extend this model to (1) train on N (cid:29) 32 styles and (2) perform stylizations for unseen painting styles never previously observed.',
 'The latter goal is especially important because the degree to which the network generalizes to unseen painting styles measures the degree to which the network (and embedding space) represents the true breadth and diversity of all painting styles.',
 'In this work, we propose a simple extension in the form of a style prediction network P (·) that takes as input an arbitrary style image s and predicts the embedding vector (cid:126)S of normalization constants, as illustrated in Figure 2.',
 'The crucial advantage of this approach is that the model can generalize to  4  \x0can unseen style image by predicting its proper style embedding at test time.',
 'We employ a pretrained Inception-v3 architecture [24] and compute the mean across each activation channel of the Mixed-6e layer which returns a feature vector with the dimension of 768.',
 'Then we apply two fully connected layers on top of it to predict the ?nal embedding (cid:126)S. The ?rst fully connected layer is purposefully constructed to contain 100 units which is substantially smaller than the dimensionality of (cid:126)S in order to compress the representation.',
 'We ?nd it suf?cient to jointly train the style prediction network P (·) and style transfer network T (·) on a large corpus of photographs and paintings.',
 'A parallel work has proposed another method for fast, arbitrary style transfer in real-time using deep networks [12].',
 'Brie?y, Huang et al (2017) employ the same transformation (Equation 4) to normalize activation channels, however they calculate ?s and ?s as the mean and standard deviation across the spatial axes of an encoder network applied to a style image.',
 'Although the style transformation is simpler, it provides a ?xed heuristic mapping from style image to normalization parameters, whereas our method learns the mapping from the style image to style parameters directly.',
 'Our experimental results indicate that the increased ?exibility achieves better objective values in the optimization.',
 '3 Results We train the style prediction network N (·) and style transfer network T (·) on the ImageNet dataset as a corpus of training content images and the Kaggle Painter By Numbers (PBN) dataset1, consisting of 79,433 labeled paintings across many genres, as a corpus of training style images.',
 'Additionally, we train the model when Describable Textures Dataset (DTD) is used as the corpus of training style images.',
 'This dataset consists of 5,640 images labeled across 47 categories [2].',
 'In both cases, we agument the training style images.',
 'We randomly ?ip, rescale, crop the images and change the hue and contrast of them.',
 'We present our results on both training style dataset.',
 '3.1 Trained network predicts arbitrary painting and texture styles.',
 'Figure 1 (left) shows stylizations from the network trained on the DTD and the PBN datasets.',
 'The ?gure highlights a number of stylizations across a few photographs.',
 'We note that the networks were trained jointly and unlike previous work [3, 14], it was unnecessary to select a unique Lagrange multiplier ?s for each painting style.',
 'That is, a single weighting of style loss suf?ces to produce reasonable results across all painting styles and textures.',
 'Importantly, we employed the trained networks to predict a stylization for paintings and textures never previously observed by the network (Figure 1, right).',
 'Qualitatively, the artistic stylizations appear to be indistinguishable from stylizations produced by the network on actual paintings and textures the network was trained against.',
 'We took this as an encouraging sign that the network learned a general method for artistic stylization that may be applied for arbitrary paintings and textures.',
 'In the following sections we quantify this behavior and measure the limits of this generalization.',
 '3.2 Generalization to unobserved paintings.',
 'Figure 1 indicates that the model is able to predict stylizations for paintings and textures never previously observed that are qualitatively indistinguishable from the stylizations on trained paintings and textures.',
 'In order to quantify this observation, we train a model on the PBN dataset and calculate the distribution of style and content losses across 2 photographs for 1024 observed painting styles (Figure 3A, black) and  1 https://www.kaggle.com/c/painter-by-numbers  5  \x0cFigure 3: Generalization to unobserved painting styles.',
 'A.',
 'Distribution of style and content loss for stylization using observed and unobserved paintings from PBN training set.',
 'B.',
 'Comparison of style and content loss be- tween proposed method, direct optimization [9] (blue) and AdaIN [12] (yellow).',
 'C. Sample images demonstrating stylization applied between proposed method and AdaIN [12] for selected points in panel B.',
 '1024 unobserved painting styles (Figure 3A, blue).',
 'The distribution of losses for observed styles (style: mean = 2.08e4 ± 2.50e4; content: mean = 8.92e4 ± 3.13e4) is largely similar to the distribution across unobserved styles (style: mean = 1.95e4 ± 3.73e4; content: mean = 8.94e4 ± 3.55e4).',
 'This indicates that the method performs stylizations on observed paintings with nearly equal ?delity as measured by the model objectives for unobserved styles.',
 'Importantly, if we train the model on a distinct but rich visual textures dataset (DTD) and test the stylizations on unobserved paintings from PBN, we ?nd that the model produces similar artistic stylizations both quantitatively (style: mean = 2.67e4 ± 6.49e4; content: mean = 8.76e4 ± 3.55e4) and qualitatively (in terms of visual inspection).',
 'Due to space constraints, we provide detailed analysis in the supplementary material.',
 'We next asked how well the learned networks perform on unobserved painting styles when compared to the original optimization-based method [9].',
 'Figure 3B plots the content and style loss objectives for our proposed method (x-axis) and [9] (blue points).',
 'Note that even though [9] directly optimizes for these two objectives, the proposed method obtains content and style losses that are comparable (style: 1.95e4 vs 1.12e4; content: 8.94e4 vs 9.09e4).',
 'These results indicate that the learned representation is able to achieve an objective comparable to one obtained by direct optimization on the image itself.',
 'We additionally compared our proposed method against a parallel work to perform fast, arbitrary stylization termed AdaIN [12].',
 'We found that our proposed method achieved lower content and style loss.',
 'Speci?cally, (style: 1.95e4 vs 2.56e4; content: 8.94e4 vs 12.3e4).',
 'In addition, paired t-test showed that these differences are statistically signi?cant (style: p-value of 1.9×10?9 with t-statistic of ?6.04; content: p-value of 0.0 with t-statistic of ?91.9), indicating that our proposed model achieved consistently better dual objectives (Figure 3B, yellow points).',
 'See Figure 3C for a comparison of each method.',
 'Figure 4 shows how the generalization ability of the model (measured in terms of style loss) is related to the proximity to training examples.',
 'Speci?cally, we plot style loss on unobserved paintings versus the minimum L2 distance between the Gram ma- trix of unobserved painting and the set of all Gram matrices in the training dataset of paintings.',
 'The plot shows clear positive corre- lation (r2 = 0.9), which suggests that our model achieves lower style loss when the unobserved image is similar to some of the training examples in terms of the Gram matrix.',
 'More discussion  6  Figure 4: Ability to generalize vs. prox- imity to training examples  \x0cof this ?gure is found in the supplementary material.',
 '3.3 Scaling to large numbers of paintings is criti- cal for generalization.',
 'A critical question we next asked was what endows these networks with the ability to generalize to paintings not previously observed.',
 'We had not observed this ability to generalize in previous work [3].',
 'A simple hypothesis is that the generalization is largely due to the fact that the model is trained with a far larger number of paintings than previously attempted.',
 'To test this hypothesis, we trained style transfer and style prediction networks with increasing numbers of example painting styles without data augmentation.',
 'Figure 5A reports the distribution of content and style loss on unobserved paintings for increasing numbers of paintings.',
 'First, we asked whether the model is better able to stylize photographs based on paintings in the training set by dint of having trained on larger numbers of paintings.',
 'Comparing left-most and right-most points of the dashed curves in Figure 5A for the content and style loss indicate no signi?cant difference.',
 'Hence, the quality of the stylizations for paintings in the training set do not improve with increasing numbers of paintings.',
 'We next examined how well the model is able to generalize when trained on increasing numbers of painting styles.',
 'Although the content loss is largely preserved in all networks, the distribution of style losses is notably higher for unobserved painting styles and this distribution does not asymptote until roughly 16,000 paintings.',
 'Importantly, after roughly 16,000 paintings the distribution of content and style loss roughly match the content and style loss for the trained painting styles.',
 'Figure 5B shows three pairings of content and style images that are unobserved in the training data set and the resulting styliza- tion as the model is trained on increasing number of paintings (Figure 5C).',
 'Training on a small number of paintings produces poor generalization whereas training on a large number of paintings produces rea- sonable stylizations on par with a model explicitly trained on this painting style.',
 '3.4 Embedding space captures semantic structure of styles.',
 'The style transfer model represents all paintings and textures in a style embedding vector (cid:126)S that is 2758 dimensional.',
 'The style prediction network predicts (cid:126)S from a lower dimensional representation (i.e., bottleneck) containing only 100 dimensions.',
 'Given the compressed representation for all artistic and texture styles, one might suspect that the network would automatically organize the space of artistic styles in a perceptually salient manner.',
 'Fur- thermore, the degree to which this unsupervised representation of artistic style matches our semantic categorization of paintings.',
 'We explore this question by qualitatively examining the low dimensional representation for style internal to the style prediction network.',
 'A 100 dimensional space is too large to visualize, thus we employ the t-SNE dimensional reduction technique to reduce the representation to two dimensions [18].',
 'Note that t-SNE will necessarily distort the representation signi?cantly in order compress the representation to small dimensionality, thus we restrict our analysis to qualitative description.',
 'Figure 6A (left) shows a two-dimensional t-SNE representation on a subset of 800 textures across 10 human-labeled categories.',
 'One may identify that regions of the embedding space cluster around perceptually similar visual textures: the bottom-right contains a preponderance of waf?es; the middle contains many checkerboard patterns; top-center contains many zebra-like patterns.',
 'Figure 6B (left) shows the same representation for a subset of 3768 paintings across 20 artists.',
 'Similar clustering behavior  7  \x0cFigure 5: Training on a large corpus of paintings is critical for generalization.',
 'A.',
 'Distribution of style and content loss for stylizations applied to unseen painting styles for proposed method trained on increasing numbers of painting styles.',
 'Solid line indicates median with box showing ±25% quartiles and whiskers indicating 10% and 90% of the cumulative distributions.',
 'Dashed line and gray region indicate the mean and range of the corresponding losses for training images.',
 'Three sample pairs of content and style images (B) and the resulting stylization with the proposed method as the method is trained on increasing numbers of paintings (top number).',
 'For comparison, ?nal column in (B) highlights stylizations for a model trained explicitly on the these styles.',
 'may be observed across colors and spatial structure as well.',
 'The structure of the low dimensional representation does not just contain visual similarity but also re?ect semantic similarity.',
 'To highlight this aspect, we reproduce the t-SNE plot but replace the individ- ual images with a human label (color coded).',
 'For the visual texture embedding (Figure 6A) we display a metadata label associated with each human-described texture.',
 'For the painting embedding (Figure 6B) we display the name of the artist for each painting.',
 'Interestingly, we ?nd that resides a region of the low-dimensional space that contains a large fraction of Impressionist paintings by Claude Monet (Figure 6B, magni?ed in inset).',
 'These results suggest that the style prediction network has learned a representa- tion for artistic styles that is largely organized based on our perception of visual and semantic similarity without any explicit supervision.',
 '3.5 The structure of the embedding space permits novel exploration.',
 'To explore the embedding structure further, we examined whether we can generate reasonable stylizations by varying local style changes for a speci?c painting style.',
 'In detail, we calculate the average embed- ding of the paintings from a speci?c artist and vary the embedding vector along along the two principal components of the cluster.',
 'Figure 7 shows stylizations from these embedding variations in a 5x5 grid, together with actual paintings of the artist whose embeddings are nearby the grid.',
 'The stylizations from the grid captures two axis of style variations and correspond well to the neighboring embeddings of actual paintings.',
 'The results suggest that the model might capture a local manifold from an individual artist or  8  \x0cFigure 6: Structure of a low-dimensional representation of the embedding space.',
 'A: Two-dimensional represen- tation using t-SNE for 800 textures [2] across 10 human-labeled categories.',
 'Right is the same as previous but texture replaced with a human annotated label.',
 'B: Same as previous but with Painting by Numbers dataset across for 3768 paintings across 20 labeled artists.',
 'Note the zoom-in highlighting a localized region of embedding space representing Monet paintings.',
 'Please zoom-in for details.',
 'painting style.',
 'Although we trained the style prediction network on painting images, we ?nd that embedding rep- resentation is extremely ?exible.',
 'In particular, supplying the network with a content image (i.e.',
 'photo- graph) produces an embedding that acts as the identity tranformation.',
 'Figure 8 highlights the identity transformation on a given content image.',
 'Importantly, we can now interpolate between the identity styl- ization and arbitrary (in this case, unobserved) painting in order to effectively dial in the weight of the painting style.',
 '4 Conclusions  We have presented a new method for performing fast, arbitrary artistic style transfer on images.',
 'This model is trained at a large scale and generalizes to perform stylizations based on paintings never previ- ously observed.',
 'Importantly, we demonstrate that increasing the corpus of trained painting style confers the system the ability to generalize to unobserved painting styles.',
 'We demonstrate that the ability to generalize is largely predictable based on the proximity of the unobserved style to styles trained on by the model.',
 'We ?nd that the model architecture provides a low dimensional embedding space of normalization constants that captures many semantic properties of paintings.',
 'We explore this space by demonstrating a low dimensional space that captures the artistic range and vocabulary of a given artist.',
 'In addition, we introduce a new form of interpolation that allows a user to arbitrarily to dial in the strength of an artistic stylization.',
 'This work offers several directions for future exploration.',
 'In particular, we observe that the em- bedding representation for paintings only captures a portion of the semantic information available for a painting.',
 'One might leverage metadata of paintings in order to re?ne the embedding representation through a secondary embedding loss [7, 20].',
 'Another direction is to improve the visual quality of the artistic stylization through complementary methods that preserve the color of the original photograph or restrict the stylization to a spatial region of the image [10].',
 'In addition, in a real time video, one could  9  Describable Textures DatasetPainter by NumbersAB']
txt2"
9_B1,"
['7 1 0 2    l u J    7      ]  V C .',
 's c [      2 v 8 4 1 5 0  .',
 '8 0 6 1 : v i X r a  Full Resolution Image Compression with Recurrent Neural Networks  George Toderici Google Research  gtoderici@google.com  Damien Vincent  Nick Johnston  Sung Jin Hwang  damienv@google.com  nickj@google.com  sjhwang@google.com  David Minnen  Joel Shor  Michele Covell  dminnen@google.com  joelshor@google.com  covell@google.com  Abstract  This paper presents a set of full-resolution lossy image compression methods based on neural networks.',
 'Each of the architectures we describe can provide variable compres- sion rates during deployment without requiring retraining of the network: each network need only be trained once.',
 'All of our architectures consist of a recurrent neural network (RNN)-based encoder and decoder, a binarizer, and a neural network for entropy coding.',
 'We compare RNN types (LSTM, associative LSTM) and introduce a new hybrid of GRU and ResNet.',
 'We also study “one-shot” versus additive recon- struction architectures and introduce a new scaled-additive framework.',
 'We compare to previous work, showing improve- ments of 4.3%–8.8% AUC (area under the rate-distortion curve), depending on the perceptual metric used.',
 'As far as we know, this is the ?rst neural network architecture that is able to outperform JPEG at image compression across most bitrates on the rate-distortion curve on the Kodak dataset images, with and without the aid of entropy coding.',
 'between patches in the image.',
 'In this paper, we address both problems and combine the two possible ways to improve compression rates for a given quality.',
 'In order to measure how well our architectures are doing (i.e., “quality”), we cannot rely on typical metrics such as Peak Signal to Noise Ratio (PSNR), or Lp differences be- tween compressed and reference images because the human visual system is more sensitive to certain types of distortions than others.',
 'This idea was exploited in lossy image compres- sion methods such as JPEG.',
 'In order to be able to measure such differences, we need to use a human visual system- inspired measure which, ideally should correlate with how humans perceive image differences.',
 'Moreover, if such a metric existed, and were differentiable, we could directly optimize for it.',
 'Unfortunately, in the literature there is a wide variety of metrics of varying quality, most of which are non-differentiable.',
 'For evaluation purposes, we selected two commonly used metrics, PSNR-HVS [7] and MS-SSIM [19], as discussed in Section 4.',
 '1.',
 'Introduction  1.1.',
 'Previous Work  Image compression has traditionally been one of the tasks which neural networks were suspected to be good at, but there was little evidence that it would be possible to train a single neural network that would be competitive across compression rates and image sizes.',
 '[17] showed that it is possible to train a single recurrent neural network and achieve better than state of the art compression rates for a given quality regardless of the input image, but was limited to 32×32 images.',
 'In that work, no effort was made to capture the long-range dependencies between image patches.',
 'Our goal is to provide a neural network which is compet- itive across compression rates on images of arbitrary size.',
 'There are two possible ways to achieve this: 1) design a stronger patch-based residual encoder; and 2) design an en- tropy coder that is able to capture long-term dependencies  Autoencoders have been used to reduce the dimension- ality of images [9], convert images to compressed binary codes for retrieval [13], and to extract compact visual repre- sentations that can be used in other applications [18].',
 'More recently, variational (recurrent) autoencoders have been di- rectly applied to the problem of compression [6] (with results on images of size up to 64×64 pixels), while non-variational recurrent neural networks were used to implement variable- rate encoding [17].',
 'Most image compression neural networks use a ?xed compression rate based on the size of a bottleneck layer [2].',
 'This work extends previous methods by supporting variable rate compression while maintaining high compression rates beyond thumbnail-sized images.',
 '1  \x0c2.',
 'Methods  In this section, we describe the high-level model architec- tures we explored.',
 'The subsections provide additional details about the different recurrent network components in our ex- periments.',
 'Our compression networks are comprised of an encoding network E, a binarizer B and a decoding network D, where D and E contain recurrent network components.',
 'The input images are ?rst encoded, and then transformed into binary codes that can be stored or transmitted to the decoder.',
 'The decoder network creates an estimate of the original input image based on the received binary code.',
 'We repeat this procedure with the residual error, the difference between the original image and the reconstruction from the decoder.',
 'Figure 1 shows the architecture of a single iteration of our model.',
 'While the network weights are shared between itera- tions, the states in the recurrent components are propagated to the next iteration.',
 'Therefore residuals are encoded and decoded in different contexts in different iterations.',
 'Note that the binarizer B is stateless in our system.',
 'We can compactly represent a single iteration of our net-  works as follows:  bt = B(Et(rt?1)), rt = x ? ˆxt,  ˆxt = Dt(bt) + ? ˆxt?1, r0 = x,  ˆx0 = 0  (1) (2)  where Dt and Et represent the decoder and encoder with their states at iteration t respectively, bt is the progressive binary representation; ˆxt is the progressive reconstruction of the original image x with ? = 0 for “one-shot” recon- struction or 1 for additive reconstruction (see Section 2.2); and rt is the residual between x and the reconstruction ˆxt.',
 'In every iteration, B will produce a binarized bit stream bt ? {?1, 1}m where m is the number of bits produced af- ter every iteration, using the approach reported in [17].',
 'After k iterations, the network produces m · k bits in total.',
 'Since our models are fully convolutional, m is a linear function of input size.',
 'For image patches of 32×32, m = 128.',
 'The recurrent units used to create the encoder and decoder include two convolutional kernels: one on the input vector which comes into the unit from the previous layer and the other one on the state vector which provides the recurrent nature of the unit.',
 'We will refer to the convolution on the state vector and its kernel as the “hidden convolution” and the “hidden kernel”.',
 'In Figure 1, we give the spatial extent of the input-vector convolutional kernel along with the output depth.',
 'All convo- lutional kernels allow full mixing across depth.',
 'For example, the unit D-RNN#3 has 256 convolutional kernels that operate on the input vector, each with 3×3 spatial extent and full input-depth extent (128 in this example, since the depth of D-RNN#2 is reduced by a factor of four as it goes through the “Depth-to-Space” unit).',
 'The spatial extents of the hidden kernels are all 1×1, except for in units D-RNN#3 and D-RNN#4 where the hidden kernels are 3×3.',
 'The larger hidden kernels consistently resulted in improved compression curves compared to the 1×1 hidden kernels exclusively used in [17].',
 'During training, a L1 loss is calculated on the weighted residuals generated at each iteration (see Section 4), so our total loss for the network is:  (cid:88)  ?  |rt|  (3)  t  In our networks, each 32×32×3 input image is reduced to a 2×2×32 binarized representation per iteration.',
 'This results in each iteration representing 1/8 bit per pixel (bpp).',
 'If only the ?rst iteration is used, this would be 192:1 compression, even before entropy coding (Section 3).',
 'We explore a combination of recurrent unit variants and reconstruction frameworks for our compression systems.',
 'We compare these compression results to the results from the deconvolutional network described in [17], referred to in this paper as the Baseline network.',
 '2.1.',
 'Types of Recurrent Units  In this subsection, we introduce the different types of  recurrent units that we examined.',
 'LSTM: One recurrent neural-network element we exam- ine is a LSTM [10] with the formulation proposed by [20].',
 'Let xt, ct, and ht denote the input, cell, and hidden states, respectively, at iteration t. Given the current input xt, previ- ous cell state ct?1, and previous hidden state ht?1, the new cell state ct and the new hidden state ht are computed as  [f, i, o, j]T = [?, ?, ?, tanh]T(cid:0)(W xt + U ht?1) + b(cid:1), (4)  ct = f (cid:12) ct?1 + i (cid:12) j, ht = o (cid:12) tanh(ct),  (5) (6)  where (cid:12) denotes element-wise multiplication, and b is the bias.',
 'The activation function ? is the sigmoid function ?(x) = 1/(1 + exp(?x)).',
 'The output of an LSTM layer at iteration t is ht.',
 'The transforms W and U, applied to xt and ht?1, respec- tively, are convolutional linear transformations.',
 'That is, they are composites of Toeplitz matrices with padding and stride transformations.',
 'The spatial extent and depth of the W con- volutions are as shown in Figure 1.',
 'As pointed out earlier in this section, the U convolutions have the same depths as the W convolutions.',
 'For a more in-depth explanation, see [17].',
 'Associative LSTM: Another neural network element we examine is the Associative LSTM [5].',
 'Associative LSTM extends LSTM using holographic representation.',
 'Its new  \x0cEncoder  Input (?oats) Size H×W×3  E-Conv Conv 3×3×64 Stride: 2×2  Decoder  E-RNN#1 RNN Conv 3×3×256 Stride: 2×2  E-RNN#3 RNN Conv 3×3×512 Stride: 2×2 Binary Code (bits), Size (H/16)×(W/16)×32  E-RNN#2 RNN Conv 3×3×512 Stride: 2×2  Encoded (?oats)  (H/16)×(W/16)×512  Binarizer  B-Conv Conv 1×1×32 Stride: 1×1  D-Conv#1 Conv 1×1×512 Stride: 1×1  D-RNN#1 RNN Conv 2×2×512 Stride: 1×1  Depth to Space  D-RNN#2 RNN Conv 3×3×512 Stride: 1×1  Depth to Space  D-RNN#3 RNN Conv 3×3×256 Stride: 1×1  Depth to Space  D-RNN#4 RNN Conv 3×3×128 Stride: 1×1  Depth to Space  D-Conv#2 Conv 1×1×3 Stride: 1×1  It (?oats)  Size H×W×3  Figure 1.',
 'A single iteration of our shared RNN architecture.',
 'states are computed as  [f,i, o, j, ri, ro]T =  [?, ?, ?, bnd, bnd, bnd]T(cid:0)(W xt + U ht?1) + b(cid:1),  ct = f (cid:12) ct?1 + ri (cid:12) i (cid:12) j, ht = o (cid:12) bnd(ro (cid:12) ct), ˜ht = (Re ht, Im ht).',
 '(7) (8) (9) (10)  The output of an Associative LSTM at iteration t is ˜ht.',
 'The input xt, the output ˜ht, and the gate values f, i, o are real- valued, but the rest of the quantities are complex-valued.',
 'The function bnd(z) for complex z is z if |z| ? 1 and is z/|z| otherwise.',
 'As in the case of non-associative LSTM, we use convolutional linear transformations W and U.  Experimentally, we determined that Associative LSTMs were effective only when used in the decoder.',
 'Thus, in all our experiments with Associative LSTMs, non-associative LSTMs were used in the encoder.',
 'Gated Recurrent Units: The last recurrent element we investigate is the Gated Recur- rent Unit [3] (GRU).',
 'The formulation for GRU, which has an input xt and a hidden state/output ht, is:  zt = ?(Wzxt + Uzht?1), rt = ?(Wrxt + Urht?1), ht = (1 ? zt) (cid:12) ht?1+  zt (cid:12) tanh(W xt + U (rt (cid:12) ht?1)).',
 '(11) (12)  (13)  As in the case of LSTM, we use convolutions instead of simple multiplications.',
 'Inspired by the core ideas from ResNet [8] and Highway Networks [16], we can think of GRU as a computation block and pass residual information around the block in order to speed up convergence.',
 'Since GRU can be seen as a doubly indexed block, with one index being iteration and the other being space, we can formulate a residual version of GRU which now has two residual con- nections.',
 'In the equations below, we use ho t to denote the output of our formulation, which will be distinct from the  hidden state ht: ht = (1 ? zt) (cid:12) ht?1+  zt (cid:12) tanh(W xt + U (rt (cid:12) ht?1)) + ?hWhht?1, ho t = ht + ?xWoxxt.',
 '(14) (15)  where we use ?x = ?h = 0.1 for all the experiments in this paper.',
 'This idea parallels the work done in Higher Order RNNs [15], where linear connections are added between iterations, but not between the input and the output of the RNN.',
 '2.2.',
 'Reconstruction Framework  In addition to using different types of recurrent units, we examine three different approaches to creating the ?nal image reconstruction from our decoder outputs.',
 'We describe those approaches in this subsection, along with the changes needed to the loss function.',
 'One-shot Reconstruction: As was done in [17], we pre- dict the full image after each iteration of the decoder (? = 0 in (1)).',
 'Each successive iteration has access to more bits generated by the encoder which allows for a better recon- struction.',
 'We call this approach “one-shot reconstruction”.',
 'Despite trying to reconstruct the original image at each iter- ation, we only pass the previous iteration’s residual to the next iteration.',
 'This reduces the number of weights, and ex- periments show that passing both the original image and the residual does not improve the reconstructions.',
 'Additive Reconstruction: In additive reconstruction, which is more widely used in traditional image coding, each iteration only tries to reconstruct the residual from the pre- vious iterations.',
 'The ?nal image reconstruction is then the sum of the outputs of all iterations (? = 1 in (1)).',
 'Residual Scaling: In both additive and “one shot” re- construction, the residual starts large, and we expect it to decrease with each iteration.',
 'However, it may be dif?cult for the encoder and the decoder to operate ef?ciently across a wide range of values.',
 'Furthermore, the rate at which the residual shrinks is content dependent.',
 'In some patches (e.g., uniform regions), the drop-off will be much more dramatic than in other patches (e.g., highly textured patches).',
 '÷  ˆrt?1  Gain  Estimator  gt  gt?1  rt?1  ×  scaled residual  Encode & Decode  ?  rt  Figure 2.',
 'Adding content-dependent, iteration-dependent residual scaling to the additive reconstruction framework.',
 'Residual images are of size H×W×3 with three color channels, while gains are of size 1 and the same gain factor is applied to all three channels per pixel.',
 'To accommodate these variations, we extend our additive reconstruction architecture to include a content-dependent, iteration-dependent gain factor.',
 'Figure 2 shows the extension that we used.',
 'Conceptually, we look at the reconstruction of the previous residual image, rt?1, and derive a gain mul- tiplier for each patch.',
 'We then multiply the target residual going into the current iteration by the gain that is given from processing the previous iteration’s output.',
 'Equation 1 becomes:  gt = G(ˆxt),  bt = B(Et(rt?1 (cid:12) ZOH(gt?1))),  g0 = 1,  ˆrt?1 = Dt(bt) (cid:11) ZOH(gt?1), rt = x ? ˆxt, ˆxt = ˆxt?1 + ˆrt?1, r0 = x.',
 '(16) (17) (18) (19) where (cid:11) is element-wise division and ZOH is spatial upsam- pling by zero-order hold.',
 'G(·) estimates the gain factor, gt, using a ?ve-layer feed-forward convolutional network, each layer with a stride of two.',
 'The ?rst four layers give an output depth of 32, using a 3×3 convolutional kernel with an ELU nonlinearity [4].',
 'The ?nal layer gives an output depth of 1, using a 2×2 convolutional kernel, with an ELU nonlinearity.',
 'Since ELU has a range of (?1,?) a constant of 2 is added to the output of this network to obtain gt in the range of (1,?).',
 '3.',
 'Entropy Coding  The entropy of the codes generated during inference are not maximal because the network is not explicitly designed to maximize entropy in its codes, and the model does not necessarily exploit visual redundancy over a large spatial ex- tent.',
 'Adding an entropy coding layer can further improve the compression ratio, as is commonly done in standard image compression codecs.',
 'In this section, the image encoder is a given and is only used as a binary code generator.',
 'The lossless entropy coding schemes considered here are fully convolutional, process binary codes in progressive order and for a given encoding iteration in raster-scan or- der.',
 'All of our image encoder architectures generate binary codes of the form c(y, x, d) of size H × W × D, where H and W are integer fractions of the image height and  width and D is m × the number of iterations.',
 'We con- sider a standard lossless encoding framework that combines a conditional probabilistic model of the current binary code c(y, x, d) with an arithmetic coder to do the actual com- pression.',
 'More formally, given a context T (y, x, d) which depends only on previous bits in stream order, we will es- timate P (c(y, x, d) | T (y, x, d)) so that the expected ideal encoded length of c(y, x, d) is the cross entropy between P (c | T ) and ˆP (c | T ).',
 'We do not consider the small penalty involved by using a practical arithmetic coder that requires a quantized version of ˆP (c | T ).',
 '3.1.',
 'Single Iteration Entropy Coder  We leverage the PixelRNN architecture [14] and use a similar architecture (BinaryRNN) for the compression of binary codes of a single layer.',
 'In this architecture (shown on Figure 3), the estimation of the conditional code probabili- ties for line y depends directly on some neighboring codes but also indirectly on the previously decoded binary codes through a line of states S of size 1 × W × k which captures both some short term and long term dependencies.',
 'The state line is a summary of all the previous lines.',
 'In practice, we use k = 64.',
 'The probabilities are estimated and the state is updated line by line using a 1×3 LSTM convolution.',
 'The end-to-end probability estimation includes 3 stages.',
 'First, the initial convolution is a 7×7 convolution used to increase the receptive ?eld of the LSTM state, the receptive ?eld being the set of codes c(i, j,·) which can in?uence the probability estimation of codes c(y, x,·).',
 'As in [14], this initial convolution is a masked convolution so as to avoid dependencies on future codes.',
 'In the second stage, the line LSTM takes as input the result z0 of this initial convolution and processes one scan line at a time.',
 'Since LSTM hidden states are produced by processing the previous scan lines, the line LSTM captures both short- and long-term dependencies.',
 'For the same reason, the input-to-state LSTM transform is also a masked convolution.',
 'Finally, two 1×1 convolu- tions are added to increase the capacity of the network to memorize more binary code patterns.',
 'Since we attempt to predict binary codes, the Bernoulli-distribution parameter can be directly estimated using a sigmoid activation in the  \x0cShort range features: 7×7 masked convolution  Raster order  c 0 0 0 0 0 0 0 0 0  0 0 0  0 0 0  0 0 0  0 0 0  0 0 0  Line LSTM  LSTM state  s  1×3 Conv  Update  O  neline  LSTM Logic  1×2 Conv z0  Input to state  1×1 Conv  1×1 Conv  ˆP (c | T )  Figure 3.',
 'Binary recurrent network (BinaryRNN) architecture for a single iteration.',
 'The gray area denotes the context that is available at decode time.',
 'last convolution.',
 'We want to minimize the number of bits used after entropy coding, which leads naturally to a cross-entropy loss.',
 'In case of {0, 1} binary codes, the cross-entropy loss can be written  as:(cid:88)  ?c log2( ˆP (c | T ))? (1? c) log2(1? ˆP (c | T )) (20)  y,x,d  3.2.',
 'Progressive Entropy Coding  When dealing with multiple iterations, a baseline entropy coder would be to duplicate the single iteration entropy coder as many times as there are iterations, each iteration having its own line LSTM.',
 'However, such an architecture would not capture the redundancy between the iterations.',
 'We can augment the data that is passed to the line LSTM of itera- tion #k with some information coming from the previous layers: the line LSTM in Figure 3 receives not just z0 like in the single iteration approach but also z1 estimated from the previous iteration using a recurrent network as shown on Figure 4.',
 'Computing z1 does not require any masked convolution since the codes of the previous layers are fully available.',
 '4.',
 'Results  Training Setup: In order to evaluate the recurrent mod- els we described, we used two sets of training data.',
 'The ?rst dataset is the “32×32” dataset gathered in [17].',
 'The sec- ond dataset takes a random sample of 6 million 1280×720 images on the web, decomposes the images into non- overlapping 32×32 tiles and samples 100 tiles that have the worst compression ratio when using the PNG compres- sion algorithm.',
 'By selecting the patches that compress the least under PNG, we intend to create a dataset with “hard- to-compress” data.',
 'The hypothesis is that training on such patches should yield a better compression model.',
 'We refer to this dataset as the “High Entropy (HE)” dataset.',
 'All network architectures were trained using the Tensor- ?ow [1] API, with the Adam [11] optimizer.',
 'Each network was trained using learning rates of [0.1, ..., 2].',
 'The L1 loss  ?1 where s (see Equation 3) was weighted by ? = (s × n) is equal to B × H × W × C where B = 32 is the batch size, H = 32 and W = 32 are the image height and width, and C = 3 is the number of color channels.',
 'n = 16 is the number of RNN unroll iterations.',
 'Evaluation Metrics: In order to assess the performance of our models, we use a perceptual, full-reference image metric for comparing original, uncompressed images to com- pressed, degraded ones.',
 'It is important to note that there is no consensus in the ?eld for which metric best represents human perception so the best we can do is sample from the available choices while acknowledging that each metric has its own strengths and weaknesses.',
 'We use Multi-Scale Struc- tural Similarity (MS-SSIM) [19], a well-established metric for comparing lossy image compression algorithms, and the more recent Peak Signal to Noise Ratio - Human Visual System (PSNR-HVS) [7].',
 'We apply MS-SSIM to each of the RGB channels independently and average the results, while PSNR-HVS already incorporates color information.',
 'MS-SSIM gives a score between 0 and 1, and PSNR-HVS is measured in decibels.',
 'In both cases, higher values imply a closer match between the test and reference images.',
 'Both metrics are computed for all models over the reconstructed images after each iteration.',
 'In order to rank models, we use an aggregate measure computed as the area under the rate-distortion curve (AUC).',
 'We collect these metrics on the widely used Kodak Photo CD dataset [12].',
 'The dataset consists of 24 768×512 PNG images (landscape/portrait) which were never compressed with a lossy algorithm.',
 'Architectures: We ran experiments consisting of {GRU, Residual GRU, LSTM, Associative LSTM} × {One Shot Reconstruction, Additive Reconstruction, Additive Rescaled Residual} and report the results for the best performing models after 1 million training steps.',
 'It is dif?cult to pick a “winning” architecture since the two metrics that we are using don’t always agree.',
 'To further complicate matters, some models may perform better at low bit rates, while others do better at high bit rates.',
 'In order to  \x0cCodes from the previous iteration  3×3 Conv  64  3×3 Conv  64  1×1  Conv LSTM  64  1×1 Conv  64  1×1 Conv  z1  Figure 4.',
 'Description of neural network used to compute additional line LSTM inputs for progressive entropy coder.',
 'This allows propagation of information from the previous iterations to the current.',
 'be as fair as possible, we picked those models which had the largest area under the curve, and plotted them in Figure 5 and Figure 6.',
 'The effect of the High Entropy training set can be seen in Table 1.',
 'In general models bene?ted from being trained on this dataset rather than on the 32×32 dataset, suggesting that it is important to train models using “hard” examples.',
 'For examples of compressed images from each method, we refer the reader to the supplemental materials.',
 'When using the 32×32 training data, GRU (One Shot) had the highest performance in both metrics.',
 'The LSTM model with Residual Scaling had the second highest MS- SSIM, while the Residual GRU had the second highest PSNR-HVS.',
 'When training on the High Entropy dataset, The One Shot version of LSTM had the highest MS-SSIM, but the worst PSNR-HVS.',
 'The GRU with “one shot” re- construction ranked 2nd highest in both metrics, while the Residual GRU with “one shot” reconstruction had the high- est PSNR-HVS.',
 'We depict the results of compressing image 5 from the Kodak dataset in Figure 7.',
 'We invite the reader to refer to the supplemental materials for more examples of compressed images from the Kodak dataset.',
 'Entropy Coding: The progressive entropy coder is trained for a speci?c image encoder, and we compare a sub- set of our models.',
 'For training, we use a set of 1280×720 images that are encoded using one of the previous image encoders (resulting in a 80×45×32 bitmap or 1/8 bits per pixel per RNN iteration).',
 'Figure 5 and Figure 6 show that all models bene?t from this additional entropy coding layer.',
 'Since the Kodak dataset has relatively low resolution images, the gains are not very signi?cant – for the best models we gained between 5% at 2 bpp, and 32% at 0.25 bpp.',
 'The bene?t of such a model is truly realized only on large images.',
 'We apply the entropy coding model to the Baseline LSTM model, and the bit-rate saving ranges from 25% at 2 bpp to 57% at 0.25 bpp.',
 '5.',
 'Discussion  We presented a general architecture for compressing with RNNs, content-based residual scaling, and a new variation of GRU, which provided the highest PSNR-HVS out of the models trained on the high entropy dataset.',
 'Because our class of networks produce image distortions that are not well captured by the existing perceptual metrics, it is dif?cult to declare a best model.',
 'However, we provided  a set of models which perform well according to these metrics, and on average we achieve better than JPEG per- formance on both MS-SSIM AUC and PSNR-HVS AUC, both with and without entropy coding.',
 'With that said, our models do bene?t from the additional step of entropy cod- ing due to the fact that in the early iterations the recurrent encoder models produce spatially correlated codes.',
 'Ad- ditionally, we are open sourcing our best Residual GRU model and our Entropy Coder training and evaluation in https://github.com/tensor?ow/models/tree/master/comp res- sion.',
 'The next challenge will be besting compression methods derived from video compression codecs, such as WebP (which was derived from VP8 video codec), on large images since they employ tricks such as reusing patches that were already decoded.',
 'Additionally training the entropy coder (BinaryRNN) and the patch-based encoder jointly and on larger patches should allow us to choose a trade-off between the ef?ciency of the patch-based encoder and the predictive power of the entropy coder.',
 'Lastly, it is important to emphasize that the domain of perceptual differences is in active development.',
 'None of the available perceptual metrics truly correlate with human vision very well, and if they do, they only correlate for particular types of distortions.',
 'If one such metric were capable of correlating with human raters for all types of distortions, we could incorporate it directly into our loss function, and optimize directly for it.',
 '6.',
 'Supplementary Materials  Supplementary Materials are available here: https://  storage.googleapis.com/compression-ml/ residual_gru_results/supplemental.pdf.',
 'References [1] M. Abadi, A. Agarwal, P. Barham, E. Brevdo, Z. Chen, C. Citro, G. S. Corrado, A. Davis, J.',
 'Dean, M. Devin, S. Ghe- mawat, I. Goodfellow, A. Harp, G. Irving, M. Isard, Y. Jia, R. Jozefowicz, L. Kaiser, M. Kudlur, J. Levenberg, D. Mané, R. Monga, S. Moore, D. Murray, C. Olah, M. Schuster, J. Shlens, B. Steiner, I. Sutskever, K. Talwar, P. Tucker, V. Van-  \x0cTable 1.',
 'Performance on the Kodak dataset measured as area under the curve (AUC) for the speci?ed metric, up to 2 bits per pixel.',
 'All models are trained up for approximately 1,000,000 training steps.',
 'No entropy coding was used.',
 'After entropy coding, the AUC will be higher for the network-based approaches.',
 'Trained on the 32×32 dataset.',
 'Model GRU (One Shot) LSTM (Residual Scaling) LSTM (One Shot) LSTM (Additive Reconstruction) Residual GRU (One Shot) Residual GRU (Residual Scaling) Associative LSTM (One Shot) GRU (Residual Scaling) Baseline [17]  Rank MS-SSIM AUC Rank 1 4 3 6 2 8 5 7  1.8098 1.8091 1.8062 1.8041 1.8030 1.7983 1.7980 1.7948 1.7225  1 2 3 4 5 6 7 8  Trained on the High Entropy dataset.',
 'LSTM (One Shot) GRU (One Shot) Residual GRU (One Shot) Residual GRU (Residual Scaling) LSTM (Residual Scaling) LSTM (Additive) Associative LSTM (One Shot) GRU (Residual Scaling) Baseline LSTM [17]  YCbCr 4:4:4 YCbCr 4:2:0  8 2 1 7 4 5 3 6  1 2 3 4 5 6 7 8  JPEG  1.8166 1.8139 1.8119 1.8076 1.8000 1.7953 1.7912 1.8065 1.7408  1.7748 1.7998  PSNR-HVS AUC 53.15 52.36 52.57 52.22 52.73 51.25 52.33 51.37 48.36  48.86 53.07 53.19 49.61 51.25 50.67 52.09 49.97 48.88  51.28 52.61  Figure 5.',
 'Rate distortion curve on the Kodak dataset given as MS-SSIM vs. bit per pixel (bpp).',
 'Dotted lines: before entropy coding, Plain lines: after entropy coding.',
 'Left: Two top performing models trained on the 32x32 dataset.',
 'Right: Two top performing models trained on the High Entropy dataset.',
 'houcke, V. Vasudevan, F. Viégas, O. Vinyals, P. Warden, M. Wattenberg, M. Wicke, Y. Yu, and X. Zheng.',
 'Tensor- Flow: Large-scale machine learning on heterogeneous sys- tems, 2015.',
 'Software available from tensor?ow.org.',
 '5  [2] J. Ballé, V. Laparra, and E. P. Simoncelli.',
 'End-to-end opti- mization of nonlinear transform codes for perceptual quality.',
 'In Picture Coding Symposium, 2016.',
 '1  [3] J. Chung, C. Gulcehre, K. Cho, and Y. Bengio.',
 'Empirical  \x0c[11] D. P. Kingma and J. Ba.',
 'Adam: A method for stochastic  optimization.',
 'CoRR, abs/1412.6980, 2014.',
 '5  [12] E. Kodak.',
 'Kodak lossless true color image suite (PhotoCD  PCD0992).',
 '5  [13] A. Krizhevsky and G. E. Hinton.',
 'Using very deep autoen- coders for content-based image retrieval.',
 'In European Sym- posium on Arti?cial Neural Networks, 2011.',
 '1  [14] A. v. d. Oord, N. Kalchbrenner, and K. Kavukcuoglu.',
 'Pixel recurrent neural networks.',
 'arXiv preprint arXiv:1601.06759, 2016.',
 '4  [15] R. Soltani and H. Jiang.',
 'Higher order recurrent neural net-  works.',
 'arXiv preprint arXiv:1605.00064, 2016.',
 '3  [16] R. K. Srivastava, K. Greff, and J. Schmidhuber.',
 'Highway networks.',
 'In International Conference on Machine Learning: Deep Learning Workshop, 2015.',
 '3  [17] G. Toderici, S. M. O’Malley, S. J. Hwang, D. Vincent, D. Min- nen, S. Baluja, M. Covell, and R. Sukthankar.',
 'Variable rate image compression with recurrent neural networks.',
 'ICLR 2016, 2016.',
 '1, 2, 3, 5, 7  [18] P. Vincent, H. Larochelle, Y. Bengio, and P.-A.',
 'Manzagol.',
 'Extracting and composing robust features with denoising autoencoders.',
 'Journal of Machine Learning Research, 2012.',
 '1  [19] Z. Wang, E. P. Simoncelli, and A. C. Bovik.',
 'Multiscale structural similarity for image quality assessment.',
 'In Signals, Systems and Computers, 2004.',
 'Conference Record of the Thirty-Seventh Asilomar Conference on, volume 2, pages 1398–1402.',
 'Ieee, 2003.',
 '1, 5  [20] W. Zaremba, I. Sutskever, and O. Vinyals.',
 'Recurrent neu- ral network regularization.',
 'arXiv preprint arXiv:1409.2329, 2014.',
 '2  Figure 6.',
 'Rate distortion curve on the Kodak dataset given as PSNR- HVS vs. bit per pixel (bpp).',
 'Dotted lines: before entropy coding, Plain lines: after entropy coding.',
 'Top: Two top performing models trained on the 32x32 dataset.',
 'Bottom: Two top performing models trained on the High Entropy dataset.',
 'evaluation of gated recurrent neural networks on sequence modeling.',
 'arXiv preprint arXiv:1412.3555, 2014.',
 '3  [4] D. Clevert, T. Unterthiner, and S. Hochreiter.',
 'Fast and accu- rate deep network learning by exponential linear units (elus).',
 'CoRR, abs/1511.07289, 2015.',
 '4  [5] I. Danihelka, G. Wayne, B. Uria, N. Kalchbrenner, and A. Graves.',
 'Associative long short-term memory.',
 'In ICML 201"
